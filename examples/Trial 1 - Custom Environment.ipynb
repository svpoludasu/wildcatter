{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811ed8a0-f8f4-4f04-9b87-6d18c9d550d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box\n",
    "from gym.spaces import Discrete\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76690cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679268d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Task List\n",
    "- ~~Drill multiple wells, one after the other and not to update the environment after every simulation.~~\n",
    "- ~~Make sure well/wells dont crash into each other/itself or any faults/artifacts~~\n",
    "- ~~Avoid 180 degree turns~~\n",
    "- Have a target zone where the well eventually want to make it to and get higher reward\n",
    "- Use a metric like MSE/UCS to get an estimate on the amount of energy required to drill and optimizing it to have lowest energy usage (also tie in the economic constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822d739",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Action Space\n",
    "- Surface Location ?? Pick it randomly or intentionally?\n",
    "- Number of wells to drill\n",
    "- Bit Movement\n",
    "    -  Up\n",
    "    -  Down\n",
    "    -  Left\n",
    "    -  Right\n",
    "    -  Angle ?? If the grid size is as much as a stand then the max angle should be around 3 degrees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e214e4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Observation Space\n",
    "\n",
    "Same shape [matrix] as the input. Ideally 30 ft by 30 ft to match with the drilling pipe (90 ft by 90 ft for stand). Bool with true for wherever well is located."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459fbd0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Possible Rewards\n",
    "- While Drilling\n",
    "    -  Proximity to Reservoir (based on the percentage of Normalized TOC?) - *Positive Reward*\n",
    "    -  Proximity to Fault - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to itself or other wells - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to the possible depletion zone of an existing well - *VERY HIGH Negative Reward*\n",
    "    -  Remaining oil in the zone of the well - *High Positive Reward*\n",
    "\n",
    "- After Drilling\n",
    "    -  Total UCS/MSE it was drilled through - *Negative Reward based on the UCS total, can also relate it to a USD amount*    \n",
    "    -  Total Well Length - *Negative Reward based on the pipe count, can also relate it to a USD amount* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b0949",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simple Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ace1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SimpleDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        self.model = np.loadtxt(\n",
    "            env_config[\"model_path\"],\n",
    "            delimiter=env_config[\"delim\"],\n",
    "        )\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=1, shape=(self.nrow, self.ncol), dtype=\"bool\"\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def step(  # noqa: C901\n",
    "        self, action: int\n",
    "    ) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        done = False\n",
    "        actions = {\n",
    "            0: [1, 0],  # down\n",
    "            1: [0, -1],  # left\n",
    "            2: [0, 1],  # right\n",
    "            3: [-1, 0],  # up\n",
    "        }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        else:\n",
    "            reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            reward = 0\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        info: dict[str, Any] = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"\n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "\n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.surface_hole_location = [1, random.randint(0, self.ncol - 1)]  # noqa: S311\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.bit_location = self.surface_hole_location\n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e44cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Multidriller Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae97ad3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MultiDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment for multiple wells\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         reward = 0\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        else:\n",
    "            self.reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "#             reward = 0\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "#                 done = True\n",
    "                self.reward = -100  \n",
    "#                 print('    Immediate 180 degree turn')\n",
    "    \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            self.multi_reward += self.reward \n",
    "            \n",
    "            if len(self.trajectory)>0:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "            self.reset_well()\n",
    "            \n",
    "            if self.wells_drilled < self.num_wells:\n",
    "                    done = False            \n",
    "                    \n",
    "            return self.state, self.multi_reward, done, info\n",
    "        else:\n",
    "            self.last_action = action\n",
    "#             print(f'Last action: {actions[self.last_action]}')\n",
    "            return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "\n",
    "        # Log the surface locations already used\n",
    "        self.surface_location.append(self.surface_hole_location[1])\n",
    "        \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fe87d",
   "metadata": {},
   "source": [
    "# Reward based on Proximity Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470bc23",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb22f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "            # Reward from the model\n",
    "            self.reward = (self.model[newrow, newcol] * 2)\n",
    "            \n",
    "            # Checking if the reward from the model is negative and stopping the well\n",
    "            if self.reward < 0:\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:                \n",
    "                # Giving a small reward to encourage the agent to use pipes     \n",
    "                self.reward += -self.pipe_used/10\n",
    "                                \n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        # Avoid going along the surface\n",
    "        if ((self.bit_location != self.surface_hole_location) &\n",
    "                (self.bit_location[0] == 0)):\n",
    "            self.reward = -10\n",
    "            done = True\n",
    "#             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -10  \n",
    "#                 done = True\n",
    "#                 print('    Immediate 180 degree turn')\n",
    "\n",
    "        if self.reward > 0:\n",
    "            self.multi_reward += self.reward   \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                self.reset_well()\n",
    "                \n",
    "                if len(self.multi_trajectory) < self.num_wells:\n",
    "#                     print(\"MULTIREWARD\")\n",
    "                    return self.state, self.multi_reward, done, info  \n",
    "                \n",
    "            else:\n",
    "                self.reset_well()\n",
    "                self.reward = - 10            \n",
    "            \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"MULTIREWARD\")\n",
    "                \n",
    "                return self.state, self.multi_reward, done, info\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "                \n",
    "#             return self.state, self.reward, done, info\n",
    "        \n",
    "        else:\n",
    "            self.last_action = action\n",
    "        \n",
    "#         print(\"REWARD\")\n",
    "            \n",
    "        return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715cecb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9ab6b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model between o-10\n",
    "        self.model = self.model*(100/self.model.max())\n",
    "\n",
    "        self.model[np.less(self.model,0)] = -100\n",
    "        self.model[self.model == 0] = 1\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:               \n",
    "                \n",
    "            # Incremental Reward from the model\n",
    "#             self.reward = sum([self.model[x,y]*2 for x,y in self.trajectory[1:]])\n",
    "            \n",
    "            model_reward = (self.model[newrow, newcol])\n",
    "            \n",
    "            # Checking if the incremental reward from the model is negative and stopping the well\n",
    "            if model_reward < 0:\n",
    "                done = True\n",
    "                self.reward = -100\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:\n",
    "                # Giving a small -ve reward to encourage the agent to use less pipes     \n",
    "                self.reward += (model_reward - self.pipe_used)\n",
    "#                 print(f'Model Reward: {self.reward}')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#         # Avoid going along the surface\n",
    "#         if ((self.bit_location != self.surface_hole_location) &\n",
    "#                 (self.bit_location[0] == 0)):\n",
    "#             self.reward += -100\n",
    "#             done = True\n",
    "# #             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -100\n",
    "                done = True\n",
    "#                 print('    Immediate 180 degree turn')  \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "#         print(done)\n",
    "        if done:\n",
    "            self.wells_drilled += 1  \n",
    "#             print('Well Done')\n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                \n",
    "                # Update state\n",
    "                self.update_state()\n",
    "                \n",
    "                if self.reward > 0:\n",
    "                    self.multi_reward += self.reward\n",
    "                else:\n",
    "                    self.multi_reward = -100\n",
    "                \n",
    "            else:\n",
    "                self.multi_reward = -100   \n",
    "                       \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"FINAL REWARD\")\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.multi_reward = -100                \n",
    "            \n",
    "            self.reset_well()\n",
    "            \n",
    "        else:\n",
    "            self.last_action = action\n",
    "            self.multi_reward += self.reward\n",
    "            \n",
    "#         print(self.reward)\n",
    "             \n",
    "        return self.state, self.multi_reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d51445",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb91f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model between o-10\n",
    "        self.model = self.model*(100/self.model.max())\n",
    "\n",
    "        self.model[np.less(self.model,0)] = -100\n",
    "        self.model[self.model == 0] = 1\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:               \n",
    "                \n",
    "            # Incremental Reward from the model\n",
    "#             self.reward = sum([self.model[x,y]*2 for x,y in self.trajectory[1:]])\n",
    "            \n",
    "            model_reward = (self.model[newrow, newcol])\n",
    "            \n",
    "            # Checking if the incremental reward from the model is negative and stopping the well\n",
    "            if model_reward < 0:\n",
    "                done = True\n",
    "                self.reward = -100\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:\n",
    "                # Giving a small -ve reward to encourage the agent to use less pipes     \n",
    "                self.reward += (model_reward - self.pipe_used)\n",
    "#                 print(f'Model Reward: {self.reward}')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#         # Avoid going along the surface\n",
    "#         if ((self.bit_location != self.surface_hole_location) &\n",
    "#                 (self.bit_location[0] == 0)):\n",
    "#             self.reward += -100\n",
    "#             done = True\n",
    "# #             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -100\n",
    "                done = True\n",
    "#                 print('    Immediate 180 degree turn')  \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "#         print(done)\n",
    "        if done:\n",
    "            self.wells_drilled += 1  \n",
    "#             print('Well Done')\n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                \n",
    "                # Update state\n",
    "                self.update_state()\n",
    "                \n",
    "                if self.reward > 0:\n",
    "                    self.multi_reward += self.reward\n",
    "                else:\n",
    "                    self.multi_reward = -100\n",
    "                \n",
    "            else:\n",
    "                self.multi_reward = -100   \n",
    "                       \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"FINAL REWARD\")\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.multi_reward = -100                \n",
    "            \n",
    "            self.reset_well()\n",
    "            \n",
    "        else:\n",
    "            self.last_action = action\n",
    "            self.multi_reward += self.reward\n",
    "            \n",
    "#         print(self.reward)\n",
    "             \n",
    "        return self.state, self.multi_reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf5147",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Horizontal well Driller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0bd45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Horizontal well driller with a specific start point\n",
    "\n",
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, PReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD , Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63eb89b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "model = np.loadtxt(env_config[\"model_path\"],\n",
    "                   delimiter=env_config[\"delim\"])\n",
    "\n",
    "model[np.less(model,0)] = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859cafe2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the bit will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55f091",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847f4d8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f856b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfcb79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8248fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(model)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd1e34",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b03a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55d2dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167252ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        \n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            \n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5f097",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52e20c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583d0f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b3820",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f65dfc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9890c57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d18b24",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdfadbe1",
   "metadata": {},
   "source": [
    "# Simple  Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8b73063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDriller:  # type: ignore\n",
    "    \"\"\"Driller environment for horizontal wells with self.rewards based on Q learning\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "\n",
    "        self.rewards = np.loadtxt(env_config[\"model_path\"],\n",
    "                                  delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "\n",
    "        # Normalizing the model\n",
    "        self.rewards = self.rewards * (100 / self.rewards.max())\n",
    "\n",
    "        self.rewards[np.less(self.rewards, 0)] = -100\n",
    "        self.rewards[self.rewards == 0] = -1\n",
    "\n",
    "        self.actions = ['up', 'right', 'down', 'left']\n",
    "\n",
    "        self.q_values = np.zeros((self.rewards.shape[0],\n",
    "                                  self.rewards.shape[1],\n",
    "                                  len(self.actions)))\n",
    "\n",
    "        self.trajectory = []        \n",
    "        self.end = 0\n",
    "        \n",
    "        self.action_cache = np.nan \n",
    "        \n",
    "        self.explored = np.zeros((self.rewards.shape[0],\n",
    "                                  self.rewards.shape[1]))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define a function that determines if the specified location is a terminal state\n",
    "    def is_terminal_state(self, current_row_index, current_column_index):\n",
    "        if ((len(self.trajectory) > 1) &\n",
    "                (self.rewards[current_row_index, current_column_index] == -100)):\n",
    "            self.end = 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    # define a function that will choose a random, non-terminal starting location\n",
    "    def get_starting_location(self):\n",
    "        # get a random column index\n",
    "        current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "        current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "        return current_row_index, current_column_index\n",
    "#         return 18, 18\n",
    "\n",
    "#     def get_unique_starting_location(self):\n",
    "#         # get a random column index\n",
    "#         current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "#         current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "#         return current_row_index, current_column_index\n",
    "# #         return 18, 0\n",
    "    \n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "    # define a function that will decide the valid actions to avoid crashing into itself\n",
    "    def get_valid_actions(self, current_row_index, current_column_index):\n",
    "        va = [0, 1, 2, 3]\n",
    "        try:\n",
    "            # Avoid turning back into itself\n",
    "            if [current_row_index - 1, current_column_index] in self.trajectory:\n",
    "                va.remove(0)\n",
    "            if [current_row_index, current_column_index + 1] in self.trajectory:\n",
    "                va.remove(1)\n",
    "            if [current_row_index + 1, current_column_index] in self.trajectory:\n",
    "                va.remove(2)\n",
    "            if [current_row_index, current_column_index - 1] in self.trajectory:\n",
    "                va.remove(3)\n",
    "\n",
    "            # Remove left move if it is the first column\n",
    "            if current_column_index == 0:\n",
    "                va.remove(3)\n",
    "\n",
    "#             # Remove up move if it is the first row\n",
    "#             if current_row_index == 0:\n",
    "#                 va.remove(0)\n",
    "                \n",
    "            # Force to move down when at surface\n",
    "            if current_row_index == 0:\n",
    "                return [2]\n",
    "\n",
    "\n",
    "            # Remove right move if it is the last column\n",
    "            if current_column_index == (self.rewards.shape[1]-1):\n",
    "                va.remove(1)\n",
    "\n",
    "            # Remove down move if it is the last row\n",
    "            if current_row_index == (self.rewards.shape[0]-1):\n",
    "                va.remove(2)\n",
    "                \n",
    "            # Avoid going up if is gonna hit the surface\n",
    "            if (current_row_index - 1) == 0:\n",
    "                va.remove(0)\n",
    "            \n",
    "#             # Avoid wellbore looping\n",
    "#             if self.action_cache.notna():\n",
    "#                 va.remove(self.action_cache)\n",
    "\n",
    "        except:\n",
    "#             self.end = 1\n",
    "            pass\n",
    "            \n",
    "        return va\n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "    def get_next_action(self, current_row_index, current_column_index, epsilon):\n",
    "        \n",
    "        valid_actions = self.get_valid_actions(current_row_index, current_column_index)\n",
    "        \n",
    "        if len(valid_actions) == 0:\n",
    "            self.end = 1\n",
    "            \n",
    "        if (len(valid_actions) != 0) & (np.random.random() < epsilon):\n",
    "            action = max(valid_actions,key = lambda i: self.q_values[current_row_index, current_column_index].tolist()[i])\n",
    "#             print(f'Valid Actions: {valid_actions}, Picked Action: {action}')\n",
    "            return action\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def get_next_action_train(self, current_row_index, current_column_index, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.argmax(self.q_values[current_row_index, current_column_index])\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define a function that will get the next location based on the chosen action\n",
    "    def get_next_location(self, current_row_index, current_column_index, action_index):\n",
    "\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        \n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.rewards.shape[1] - 1:\n",
    "            new_column_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.rewards.shape[0] - 1:\n",
    "            new_row_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "        else:\n",
    "            self.end = 1\n",
    "\n",
    "        return new_row_index, new_column_index\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "  \n",
    "    def get_rewards(self, row_index, column_index): \n",
    "        \n",
    "        # From Model\n",
    "        reward = self.rewards[row_index, column_index]*15\n",
    "\n",
    "        # To encourage to maintain the shortest path\n",
    "        reward += -len(self.trajectory)*25\n",
    "\n",
    "        # To ensure that a horizontal well is drilled\n",
    "        reward += abs(self.trajectory[-1][1] - self.trajectory[0][1])*10\n",
    "    #                     print(reward)\n",
    "\n",
    "        # To make sure max  amount of target pipes are used\n",
    "        reward += -(self.available_pipe -len(self.trajectory))*5\n",
    "\n",
    "        # Adding a -ve reward to encourage the agent to visit unique rows,columns\n",
    "        rows = [i[0] for i in self.trajectory]\n",
    "        columns = [i[1] for i in self.trajectory]\n",
    "\n",
    "        reward += -(len(rows) - len(set(rows)))*10\n",
    "        reward += -(len(columns) - len(set(columns)))*20\n",
    "\n",
    "    #                     # Add a -ve reward to identify simultaneous right/left turns in the to avoid wellbore tornado effect\n",
    "    #                     if (action_index == self.action_cache):\n",
    "    #                         reward += -100\n",
    "    \n",
    "        return reward \n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function to train and populate the q table\n",
    "    def populate_q_table(self, num_episodes, epsilon = 0.1, discount_factor = 0.9, learning_rate = 0.9):\n",
    "        print('Training Started!')\n",
    "        for episode in range(num_episodes):\n",
    "            self.reset()\n",
    "\n",
    "            # get the starting location for this episode\n",
    "            row_index, column_index = self.get_starting_location()\n",
    "#             print(row_index, column_index)\n",
    "\n",
    "            self.trajectory.append([row_index, column_index])\n",
    "#             print(self.trajectory)\n",
    "            \n",
    "#             print(self.rewards[row_index, column_index])\n",
    "            \n",
    "#             print(self.is_terminal_state(row_index, column_index))\n",
    "\n",
    "            # continue taking actions (i.e., moving) until we reach a terminal state\n",
    "            while not (self.is_terminal_state(row_index, column_index) | (self.end == 1)):\n",
    "\n",
    "                # choose which action to take (i.e., where to move next)\n",
    "                action_index = self.get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "                # perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "                old_row_index, old_column_index = row_index, column_index  # store the old row and column indexes\n",
    "                row_index, column_index = self.get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "                self.trajectory.append([row_index, column_index])\n",
    "                reward = self.get_rewards(row_index, column_index)                 \n",
    "\n",
    "                    \n",
    "                if (action_index == 1) | (action_index == 3):\n",
    "                    self.action_cache = action_index                    \n",
    "\n",
    "                old_q_value = self.q_values[old_row_index, old_column_index, action_index]\n",
    "\n",
    "                temporal_difference = reward + (\n",
    "                            discount_factor * np.max(self.q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "                # update the Q-value for the previous state and action pair\n",
    "                new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "#                 print(new_q_value)\n",
    "\n",
    "                self.q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "                self.explored[old_row_index, old_column_index] = 1\n",
    "    \n",
    "    \n",
    "            if (episode != 0) & ((episode + 1) % 100_000 == 0):\n",
    "                print(f'    {\"{:,}\".format(episode + 1)} episodes completed')\n",
    "\n",
    "#             print(self.trajectory)\n",
    "        \n",
    "        print('Training Complete!')\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function that will get the shortest path\n",
    "    def get_shortest_path(self, start_row_index, start_column_index):\n",
    "        self.reset()\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        self.trajectory.append([current_row_index, current_column_index])\n",
    "\n",
    "        pipes_used = 0\n",
    "#         print(self.is_terminal_state(current_row_index, current_column_index))\n",
    "        \n",
    "        while not (self.is_terminal_state(current_row_index, current_column_index) | (self.end == 1)):\n",
    "#             print(self.trajectory)\n",
    "            # get the best action to take\n",
    "            action_index = self.get_next_action(current_row_index, current_column_index, 1.)\n",
    "#             print(current_row_index, current_column_index)\n",
    "#             print(self.actions[action_index])\n",
    "            # move to the next location on the path, and add the new location to the list\n",
    "            current_row_index, current_column_index = self.get_next_location(current_row_index, current_column_index,\n",
    "                                                                        action_index)\n",
    "#             print(f'{current_row_index}, {current_column_index}\\n')\n",
    "\n",
    "            \n",
    "            pipes_used += 1\n",
    "\n",
    "            if (pipes_used == self.available_pipe):\n",
    "                self.end = 1\n",
    "                print('Pipes Over')\n",
    "                \n",
    "            if ([current_row_index, current_column_index] in self.trajectory):\n",
    "                self.end = 1\n",
    "                print(f'Index in trajectory - [{current_row_index},{current_column_index}]')\n",
    "                \n",
    "            else:\n",
    "                self.trajectory.append([current_row_index, current_column_index])\n",
    "#                 print(self.trajectory)\n",
    "\n",
    "        return self.trajectory\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function that will reset everything\n",
    "    def reset(self):\n",
    "        self.trajectory = []        \n",
    "        self.end = 0\n",
    "        self.action_cache = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bab4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 1, delim=\",\")\n",
    "env = QDriller(env_config)\n",
    "\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66edc82a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started!\n",
      "    100,000 episodes completed\n",
      "    200,000 episodes completed\n",
      "    300,000 episodes completed\n",
      "    400,000 episodes completed\n",
      "    500,000 episodes completed\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "env.populate_q_table(500_000)\n",
    "\n",
    "# plt.figure(figsize=(15, 7))\n",
    "# plt.imshow(env.explored, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fbae4d49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index in trajectory - [12,23]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_row = 0\n",
    "start_column = 29\n",
    "\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "\n",
    "trajectory = env.get_shortest_path(start_row, start_column)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.rewards, vmin=-100, vmax=100)\n",
    "\n",
    "for i in range(0,len(trajectory)):\n",
    "    traj_z, traj_x = np.asarray(trajectory).T\n",
    "    plt.plot(traj_x, traj_z, \"-\", linewidth=6, color = 'k')\n",
    "\n",
    "plt.xticks(np.arange(0, 80, 1.0))\n",
    "plt.yticks(np.arange(0, 40, 1.0))\n",
    "plt.xlim([-0.5, 79.5])\n",
    "plt.ylim([39.5, -0.5])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a604cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78987a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be3f6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c203c466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d696c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82244f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d970039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abcfcf4f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bef83",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 1\n",
    "\n",
    "actions = {\n",
    "           0: [1, 0],  # down\n",
    "           1: [0, -1],  # left\n",
    "           2: [0, 1],  # right\n",
    "           3: [-1, 0],  # up\n",
    "          }\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    \n",
    "    print(\"Beginning Drill Campaign:\", episode)\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "#         print(f\"    Action: {actions[action]}\")\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Total Reward: {reward}\")\n",
    "#         print(f\"    done: {done}\\n\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273352b",
   "metadata": {},
   "source": [
    "# Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb3de7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4e748",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "env = RewardDriller(env_config)\n",
    "# env = MultiDriller(env_config)\n",
    "\n",
    "\n",
    "ppo = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "ppo.learn(total_timesteps = 800_000, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ac943",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = MultiDriller(env_config)\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = ppo.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a2b3d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad433c1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c4fc7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "dqn = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "dqn.learn(total_timesteps=500_000, log_interval=1_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b03e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = dqn.predict(state, deterministic=True)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a182590",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31eaef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc33143",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68430c-913a-422d-a19b-8a284d7bc5f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=100, num_wells = 3, delim=\",\")\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "a2c = A2C(\"MlpPolicy\", env, verbose=3)\n",
    "a2c.learn(total_timesteps=500_000, log_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263efa6f",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 100\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = a2c.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4d162",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
