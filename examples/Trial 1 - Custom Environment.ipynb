{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "811ed8a0-f8f4-4f04-9b87-6d18c9d550d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box\n",
    "from gym.spaces import Discrete\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76690cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679268d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Task List\n",
    "- ~~Drill multiple wells, one after the other and not to update the environment after every simulation.~~\n",
    "- ~~Make sure well/wells dont crash into each other/itself or any faults/artifacts~~\n",
    "- ~~Avoid 180 degree turns~~\n",
    "- Have a target zone where the well eventually want to make it to and get higher reward\n",
    "- Use a metric like MSE/UCS to get an estimate on the amount of energy required to drill and optimizing it to have lowest energy usage (also tie in the economic constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822d739",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Action Space\n",
    "- Surface Location ?? Pick it randomly or intentionally?\n",
    "- Number of wells to drill\n",
    "- Bit Movement\n",
    "    -  Up\n",
    "    -  Down\n",
    "    -  Left\n",
    "    -  Right\n",
    "    -  Angle ?? If the grid size is as much as a stand then the max angle should be around 3 degrees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e214e4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Observation Space\n",
    "\n",
    "Same shape [matrix] as the input. Ideally 30 ft by 30 ft to match with the drilling pipe (90 ft by 90 ft for stand). Bool with true for wherever well is located."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459fbd0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Possible Rewards\n",
    "- While Drilling\n",
    "    -  Proximity to Reservoir (based on the percentage of Normalized TOC?) - *Positive Reward*\n",
    "    -  Proximity to Fault - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to itself or other wells - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to the possible depletion zone of an existing well - *VERY HIGH Negative Reward*\n",
    "    -  Remaining oil in the zone of the well - *High Positive Reward*\n",
    "\n",
    "- After Drilling\n",
    "    -  Total UCS/MSE it was drilled through - *Negative Reward based on the UCS total, can also relate it to a USD amount*    \n",
    "    -  Total Well Length - *Negative Reward based on the pipe count, can also relate it to a USD amount* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b0949",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simple Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ace1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SimpleDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        self.model = np.loadtxt(\n",
    "            env_config[\"model_path\"],\n",
    "            delimiter=env_config[\"delim\"],\n",
    "        )\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=1, shape=(self.nrow, self.ncol), dtype=\"bool\"\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def step(  # noqa: C901\n",
    "        self, action: int\n",
    "    ) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        done = False\n",
    "        actions = {\n",
    "            0: [1, 0],  # down\n",
    "            1: [0, -1],  # left\n",
    "            2: [0, 1],  # right\n",
    "            3: [-1, 0],  # up\n",
    "        }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        else:\n",
    "            reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            reward = 0\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        info: dict[str, Any] = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"\n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "\n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.surface_hole_location = [1, random.randint(0, self.ncol - 1)]  # noqa: S311\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.bit_location = self.surface_hole_location\n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e44cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Multidriller Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae97ad3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MultiDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment for multiple wells\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         reward = 0\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        else:\n",
    "            self.reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "#             reward = 0\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "#                 done = True\n",
    "                self.reward = -100  \n",
    "#                 print('    Immediate 180 degree turn')\n",
    "    \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            self.multi_reward += self.reward \n",
    "            \n",
    "            if len(self.trajectory)>0:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "            self.reset_well()\n",
    "            \n",
    "            if self.wells_drilled < self.num_wells:\n",
    "                    done = False            \n",
    "                    \n",
    "            return self.state, self.multi_reward, done, info\n",
    "        else:\n",
    "            self.last_action = action\n",
    "#             print(f'Last action: {actions[self.last_action]}')\n",
    "            return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "\n",
    "        # Log the surface locations already used\n",
    "        self.surface_location.append(self.surface_hole_location[1])\n",
    "        \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fe87d",
   "metadata": {},
   "source": [
    "# Reward based on Proximity Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470bc23",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb22f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "            # Reward from the model\n",
    "            self.reward = (self.model[newrow, newcol] * 2)\n",
    "            \n",
    "            # Checking if the reward from the model is negative and stopping the well\n",
    "            if self.reward < 0:\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:                \n",
    "                # Giving a small reward to encourage the agent to use pipes     \n",
    "                self.reward += -self.pipe_used/10\n",
    "                                \n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        # Avoid going along the surface\n",
    "        if ((self.bit_location != self.surface_hole_location) &\n",
    "                (self.bit_location[0] == 0)):\n",
    "            self.reward = -10\n",
    "            done = True\n",
    "#             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -10  \n",
    "#                 done = True\n",
    "#                 print('    Immediate 180 degree turn')\n",
    "\n",
    "        if self.reward > 0:\n",
    "            self.multi_reward += self.reward   \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                self.reset_well()\n",
    "                \n",
    "                if len(self.multi_trajectory) < self.num_wells:\n",
    "#                     print(\"MULTIREWARD\")\n",
    "                    return self.state, self.multi_reward, done, info  \n",
    "                \n",
    "            else:\n",
    "                self.reset_well()\n",
    "                self.reward = - 10            \n",
    "            \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"MULTIREWARD\")\n",
    "                \n",
    "                return self.state, self.multi_reward, done, info\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "                \n",
    "#             return self.state, self.reward, done, info\n",
    "        \n",
    "        else:\n",
    "            self.last_action = action\n",
    "        \n",
    "#         print(\"REWARD\")\n",
    "            \n",
    "        return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715cecb",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model between o-10\n",
    "        self.model = self.model*(100/self.model.max())\n",
    "\n",
    "        self.model[np.less(self.model,0)] = -100\n",
    "        self.model[self.model == 0] = 1\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:               \n",
    "                \n",
    "            # Incremental Reward from the model\n",
    "#             self.reward = sum([self.model[x,y]*2 for x,y in self.trajectory[1:]])\n",
    "            \n",
    "            model_reward = (self.model[newrow, newcol])\n",
    "            \n",
    "            # Checking if the incremental reward from the model is negative and stopping the well\n",
    "            if model_reward < 0:\n",
    "                done = True\n",
    "                self.reward = -100\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:\n",
    "                # Giving a small -ve reward to encourage the agent to use less pipes     \n",
    "                self.reward += (model_reward - self.pipe_used)\n",
    "#                 print(f'Model Reward: {self.reward}')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#         # Avoid going along the surface\n",
    "#         if ((self.bit_location != self.surface_hole_location) &\n",
    "#                 (self.bit_location[0] == 0)):\n",
    "#             self.reward += -100\n",
    "#             done = True\n",
    "# #             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -100\n",
    "                done = True\n",
    "#                 print('    Immediate 180 degree turn')  \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "#         print(done)\n",
    "        if done:\n",
    "            self.wells_drilled += 1  \n",
    "#             print('Well Done')\n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                \n",
    "                # Update state\n",
    "                self.update_state()\n",
    "                \n",
    "                if self.reward > 0:\n",
    "                    self.multi_reward += self.reward\n",
    "                else:\n",
    "                    self.multi_reward = -100\n",
    "                \n",
    "            else:\n",
    "                self.multi_reward = -100   \n",
    "                       \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"FINAL REWARD\")\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.multi_reward = -100                \n",
    "            \n",
    "            self.reset_well()\n",
    "            \n",
    "        else:\n",
    "            self.last_action = action\n",
    "            self.multi_reward += self.reward\n",
    "            \n",
    "#         print(self.reward)\n",
    "             \n",
    "        return self.state, self.multi_reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff83e5c",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model between o-10\n",
    "        self.model = self.model*(100/self.model.max())\n",
    "\n",
    "        self.model[np.less(self.model,0)] = -100\n",
    "        self.model[self.model == 0] = 1\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:               \n",
    "                \n",
    "            # Incremental Reward from the model\n",
    "#             self.reward = sum([self.model[x,y]*2 for x,y in self.trajectory[1:]])\n",
    "            \n",
    "            model_reward = (self.model[newrow, newcol])\n",
    "            \n",
    "            # Checking if the incremental reward from the model is negative and stopping the well\n",
    "            if model_reward < 0:\n",
    "                done = True\n",
    "                self.reward = -100\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:\n",
    "                # Giving a small -ve reward to encourage the agent to use less pipes     \n",
    "                self.reward += (model_reward - self.pipe_used)\n",
    "#                 print(f'Model Reward: {self.reward}')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#         # Avoid going along the surface\n",
    "#         if ((self.bit_location != self.surface_hole_location) &\n",
    "#                 (self.bit_location[0] == 0)):\n",
    "#             self.reward += -100\n",
    "#             done = True\n",
    "# #             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -100\n",
    "                done = True\n",
    "#                 print('    Immediate 180 degree turn')  \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "#         print(done)\n",
    "        if done:\n",
    "            self.wells_drilled += 1  \n",
    "#             print('Well Done')\n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                \n",
    "                # Update state\n",
    "                self.update_state()\n",
    "                \n",
    "                if self.reward > 0:\n",
    "                    self.multi_reward += self.reward\n",
    "                else:\n",
    "                    self.multi_reward = -100\n",
    "                \n",
    "            else:\n",
    "                self.multi_reward = -100   \n",
    "                       \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"FINAL REWARD\")\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.multi_reward = -100                \n",
    "            \n",
    "            self.reset_well()\n",
    "            \n",
    "        else:\n",
    "            self.last_action = action\n",
    "            self.multi_reward += self.reward\n",
    "            \n",
    "#         print(self.reward)\n",
    "             \n",
    "        return self.state, self.multi_reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf5147",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Horizontal well Driller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0bd45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Horizontal well driller with a specific start point\n",
    "\n",
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, PReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD , Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63eb89b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "model = np.loadtxt(env_config[\"model_path\"],\n",
    "                   delimiter=env_config[\"delim\"])\n",
    "\n",
    "model[np.less(model,0)] = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859cafe2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the bit will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55f091",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847f4d8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f856b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfcb79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8248fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(model)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd1e34",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b03a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55d2dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167252ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        \n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            \n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5f097",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52e20c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583d0f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b3820",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f65dfc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9890c57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d18b24",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdfadbe1",
   "metadata": {},
   "source": [
    "# Simple  Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8b73063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDriller:  # type: ignore\n",
    "    \"\"\"Driller environment for horizontal wells with self.rewards based on Q learning\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "\n",
    "        self.rewards = np.loadtxt(env_config[\"model_path\"],\n",
    "                                  delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "\n",
    "        # Normalizing the model\n",
    "        self.rewards = self.rewards * (100 / self.rewards.max())\n",
    "\n",
    "        self.rewards[np.less(self.rewards, 0)] = -100\n",
    "        self.rewards[self.rewards == 0] = -1\n",
    "\n",
    "        self.actions = ['up', 'right', 'down', 'left']\n",
    "\n",
    "        self.q_values = np.zeros((self.rewards.shape[0],\n",
    "                                  self.rewards.shape[1],\n",
    "                                  len(self.actions)))\n",
    "\n",
    "        self.trajectory = []        \n",
    "        self.end = 0\n",
    "        \n",
    "        self.action_cache = np.nan \n",
    "        \n",
    "        self.explored = np.zeros((self.rewards.shape[0],\n",
    "                                  self.rewards.shape[1]))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define a function that determines if the specified location is a terminal state\n",
    "    def is_terminal_state(self, current_row_index, current_column_index):\n",
    "        if ((len(self.trajectory) > 1) &\n",
    "                (self.rewards[current_row_index, current_column_index] == -100)):\n",
    "            self.end = 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    # define a function that will choose a random, non-terminal starting location\n",
    "    def get_starting_location(self):\n",
    "        # get a random column index\n",
    "        current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "        current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "        return current_row_index, current_column_index\n",
    "#         return 18, 18\n",
    "\n",
    "#     def get_unique_starting_location(self):\n",
    "#         # get a random column index\n",
    "#         current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "#         current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "#         return current_row_index, current_column_index\n",
    "# #         return 18, 0\n",
    "    \n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "    # define a function that will decide the valid actions to avoid crashing into itself\n",
    "    def get_valid_actions(self, current_row_index, current_column_index):\n",
    "        va = [0, 1, 2, 3]\n",
    "        try:\n",
    "            # Avoid turning back into itself\n",
    "            if [current_row_index - 1, current_column_index] in self.trajectory:\n",
    "                va.remove(0)\n",
    "            if [current_row_index, current_column_index + 1] in self.trajectory:\n",
    "                va.remove(1)\n",
    "            if [current_row_index + 1, current_column_index] in self.trajectory:\n",
    "                va.remove(2)\n",
    "            if [current_row_index, current_column_index - 1] in self.trajectory:\n",
    "                va.remove(3)\n",
    "\n",
    "            # Remove left move if it is the first column\n",
    "            if current_column_index == 0:\n",
    "                va.remove(3)\n",
    "\n",
    "#             # Remove up move if it is the first row\n",
    "#             if current_row_index == 0:\n",
    "#                 va.remove(0)\n",
    "                \n",
    "            # Force to move down when at surface\n",
    "            if current_row_index == 0:\n",
    "                return [2]\n",
    "\n",
    "\n",
    "            # Remove right move if it is the last column\n",
    "            if current_column_index == (self.rewards.shape[1]-1):\n",
    "                va.remove(1)\n",
    "\n",
    "            # Remove down move if it is the last row\n",
    "            if current_row_index == (self.rewards.shape[0]-1):\n",
    "                va.remove(2)\n",
    "                \n",
    "            # Avoid going up if is gonna hit the surface\n",
    "            if (current_row_index - 1) == 0:\n",
    "                va.remove(0)\n",
    "            \n",
    "#             # Avoid wellbore looping\n",
    "#             if self.action_cache.notna():\n",
    "#                 va.remove(self.action_cache)\n",
    "\n",
    "        except:\n",
    "#             self.end = 1\n",
    "            pass\n",
    "            \n",
    "        return va\n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "    def get_next_action(self, current_row_index, current_column_index, epsilon):\n",
    "        \n",
    "        valid_actions = self.get_valid_actions(current_row_index, current_column_index)\n",
    "        \n",
    "        if len(valid_actions) == 0:\n",
    "            self.end = 1\n",
    "            \n",
    "        if (len(valid_actions) != 0) & (np.random.random() < epsilon):\n",
    "            action = max(valid_actions,key = lambda i: self.q_values[current_row_index, current_column_index].tolist()[i])\n",
    "#             print(f'Valid Actions: {valid_actions}, Picked Action: {action}')\n",
    "            return action\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def get_next_action_train(self, current_row_index, current_column_index, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.argmax(self.q_values[current_row_index, current_column_index])\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define a function that will get the next location based on the chosen action\n",
    "    def get_next_location(self, current_row_index, current_column_index, action_index):\n",
    "\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        \n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.rewards.shape[1] - 1:\n",
    "            new_column_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.rewards.shape[0] - 1:\n",
    "            new_row_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "        else:\n",
    "            self.end = 1\n",
    "\n",
    "        return new_row_index, new_column_index\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function to train and populate the q table\n",
    "    def populate_q_table(self, num_episodes, epsilon = 0.1, discount_factor = 0.9, learning_rate = 0.9):\n",
    "        print('Training Started!')\n",
    "        for episode in range(num_episodes):\n",
    "            self.reset()\n",
    "\n",
    "            # get the starting location for this episode\n",
    "            row_index, column_index = self.get_starting_location()\n",
    "#             print(row_index, column_index)\n",
    "\n",
    "            self.trajectory.append([row_index, column_index])\n",
    "#             print(self.trajectory)\n",
    "            \n",
    "#             print(self.rewards[row_index, column_index])\n",
    "            \n",
    "#             print(self.is_terminal_state(row_index, column_index))\n",
    "\n",
    "            # continue taking actions (i.e., moving) until we reach a terminal state\n",
    "            while not (self.is_terminal_state(row_index, column_index) | (self.end == 1)):\n",
    "\n",
    "                # choose which action to take (i.e., where to move next)\n",
    "                action_index = self.get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "                # perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "                old_row_index, old_column_index = row_index, column_index  # store the old row and column indexes\n",
    "                row_index, column_index = self.get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "                # receive the reward for moving to the new state\n",
    "#                 if ([row_index, column_index] in self.trajectory):\n",
    "#                     reward = -100\n",
    "#                 elif (row_index == 0):\n",
    "#                     reward = -100\n",
    "#                 else:\n",
    "#                     reward = self.rewards[row_index, column_index] - len(self.trajectory)\n",
    "#                 #         print(reward)\n",
    "\n",
    "                if [row_index, column_index] in self.trajectory:\n",
    "                    self.end == 1\n",
    "                    reward += -100\n",
    "                    break\n",
    "                else:\n",
    "                    self.trajectory.append([row_index, column_index])\n",
    "                    # From Model\n",
    "                    reward = self.rewards[row_index, column_index]*5\n",
    "\n",
    "                    # To encourage to maintain the shortest path\n",
    "                    reward += -len(self.trajectory)\n",
    "\n",
    "                    # To ensure that a horizontal well is drilled\n",
    "                    reward += abs(self.trajectory[-1][1] - self.trajectory[0][1])\n",
    "#                     print(reward)\n",
    "\n",
    "#                     # To make sure target pipes are used\n",
    "#                     reward += (self.available_pipe -len(self.trajectory))*2\n",
    "                    \n",
    "                    # Adding a -ve reward to encourage the agent to visit unique rows,columns\n",
    "                    rows = [i[0] for i in self.trajectory]\n",
    "                    columns = [i[1] for i in self.trajectory]\n",
    "\n",
    "                    reward += -(len(rows) - len(set(rows)))*1\n",
    "                    reward += -(len(columns) - len(set(columns)))*20\n",
    "                    \n",
    "#                     # Add a -ve reward to identify simultaneous right/left turns in the to avoid wellbore tornado effect\n",
    "#                     if (action_index == self.action_cache):\n",
    "#                         reward += -100\n",
    "                    \n",
    "                if (action_index == 1) | (action_index == 3):\n",
    "                    self.action_cache = action_index                    \n",
    "\n",
    "                old_q_value = self.q_values[old_row_index, old_column_index, action_index]\n",
    "\n",
    "                temporal_difference = reward + (\n",
    "                            discount_factor * np.max(self.q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "                # update the Q-value for the previous state and action pair\n",
    "                new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "#                 print(new_q_value)\n",
    "\n",
    "                self.q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "                self.explored[old_row_index, old_column_index] = 1\n",
    "    \n",
    "    \n",
    "            if (episode != 0) & ((episode + 1) % 100_000 == 0):\n",
    "                print(f'    {\"{:,}\".format(episode + 1)} episodes completed')\n",
    "\n",
    "#             print(self.trajectory)\n",
    "        \n",
    "        print('Training Complete!')\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function that will get the shortest path\n",
    "    def get_shortest_path(self, start_row_index, start_column_index):\n",
    "        self.reset()\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        self.trajectory.append([current_row_index, current_column_index])\n",
    "\n",
    "        pipes_used = 0\n",
    "#         print(self.is_terminal_state(current_row_index, current_column_index))\n",
    "        \n",
    "        while not (self.is_terminal_state(current_row_index, current_column_index) | (self.end == 1)):\n",
    "#             print(self.trajectory)\n",
    "            # get the best action to take\n",
    "            action_index = self.get_next_action(current_row_index, current_column_index, 1.)\n",
    "#             print(current_row_index, current_column_index)\n",
    "#             print(self.actions[action_index])\n",
    "            # move to the next location on the path, and add the new location to the list\n",
    "            current_row_index, current_column_index = self.get_next_location(current_row_index, current_column_index,\n",
    "                                                                        action_index)\n",
    "#             print(f'{current_row_index}, {current_column_index}\\n')\n",
    "\n",
    "            \n",
    "            pipes_used += 1\n",
    "\n",
    "            if (pipes_used == self.available_pipe):\n",
    "                self.end = 1\n",
    "                print('Pipes Over')\n",
    "                \n",
    "            if ([current_row_index, current_column_index] in self.trajectory):\n",
    "                self.end = 1\n",
    "                print(f'Index in trajectory - [{current_row_index},{current_column_index}]')\n",
    "                \n",
    "            else:\n",
    "                self.trajectory.append([current_row_index, current_column_index])\n",
    "#                 print(self.trajectory)\n",
    "\n",
    "        return self.trajectory\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function that will reset everything\n",
    "    def reset(self):\n",
    "        self.trajectory = []        \n",
    "        self.end = 0\n",
    "        self.action_cache = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bab4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=100, num_wells = 1, delim=\",\")\n",
    "env = QDriller(env_config)\n",
    "\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66edc82a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started!\n",
      "    100,000 episodes completed\n",
      "    200,000 episodes completed\n",
      "    300,000 episodes completed\n",
      "    400,000 episodes completed\n",
      "    500,000 episodes completed\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "env.populate_q_table(500_000)\n",
    "\n",
    "# plt.figure(figsize=(15, 7))\n",
    "# plt.imshow(env.explored, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbae4d49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipes Over\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_row = 0\n",
    "start_column = 50\n",
    "\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "\n",
    "trajectory = env.get_shortest_path(start_row, start_column)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.rewards, vmin=-100, vmax=100)\n",
    "\n",
    "for i in range(0,len(trajectory)):\n",
    "    traj_z, traj_x = np.asarray(trajectory).T\n",
    "    plt.plot(traj_x, traj_z, \"-\", linewidth=6, color = 'k')\n",
    "\n",
    "plt.xticks(np.arange(0, 80, 1.0))\n",
    "plt.yticks(np.arange(0, 40, 1.0))\n",
    "plt.xlim([-0.5, 79.5])\n",
    "plt.ylim([39.5, -0.5])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a604cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78987a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be3f6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c203c466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d696c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82244f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d970039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abcfcf4f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bef83",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 1\n",
    "\n",
    "actions = {\n",
    "           0: [1, 0],  # down\n",
    "           1: [0, -1],  # left\n",
    "           2: [0, 1],  # right\n",
    "           3: [-1, 0],  # up\n",
    "          }\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    \n",
    "    print(\"Beginning Drill Campaign:\", episode)\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "#         print(f\"    Action: {actions[action]}\")\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Total Reward: {reward}\")\n",
    "#         print(f\"    done: {done}\\n\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273352b",
   "metadata": {},
   "source": [
    "# Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb3de7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4e748",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "env = RewardDriller(env_config)\n",
    "# env = MultiDriller(env_config)\n",
    "\n",
    "\n",
    "ppo = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "ppo.learn(total_timesteps = 800_000, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ac943",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = MultiDriller(env_config)\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = ppo.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a2b3d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad433c1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c4fc7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "dqn = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "dqn.learn(total_timesteps=500_000, log_interval=1_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b03e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = dqn.predict(state, deterministic=True)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a182590",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31eaef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc33143",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68430c-913a-422d-a19b-8a284d7bc5f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=100, num_wells = 3, delim=\",\")\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "a2c = A2C(\"MlpPolicy\", env, verbose=3)\n",
    "a2c.learn(total_timesteps=500_000, log_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263efa6f",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 100\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = a2c.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4d162",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
