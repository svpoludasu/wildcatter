{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811ed8a0-f8f4-4f04-9b87-6d18c9d550d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box\n",
    "from gym.spaces import Discrete\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76690cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679268d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Task List\n",
    "- ~~Drill multiple wells, one after the other and not to update the environment after every simulation.~~\n",
    "- ~~Make sure well/wells dont crash into each other/itself or any faults/artifacts~~\n",
    "- ~~Avoid 180 degree turns~~\n",
    "- Have a target zone where the well eventually want to make it to and get higher reward\n",
    "- Use a metric like MSE/UCS to get an estimate on the amount of energy required to drill and optimizing it to have lowest energy usage (also tie in the economic constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822d739",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Action Space\n",
    "- Surface Location ?? Pick it randomly or intentionally?\n",
    "- Number of wells to drill\n",
    "- Bit Movement\n",
    "    -  Up\n",
    "    -  Down\n",
    "    -  Left\n",
    "    -  Right\n",
    "    -  Angle ?? If the grid size is as much as a stand then the max angle should be around 3 degrees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e214e4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Observation Space\n",
    "\n",
    "Same shape [matrix] as the input. Ideally 30 ft by 30 ft to match with the drilling pipe (90 ft by 90 ft for stand). Bool with true for wherever well is located."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459fbd0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Possible Rewards\n",
    "- While Drilling\n",
    "    -  Proximity to Reservoir (based on the percentage of Normalized TOC?) - *Positive Reward*\n",
    "    -  Proximity to Fault - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to itself or other wells - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to the possible depletion zone of an existing well - *VERY HIGH Negative Reward*\n",
    "    -  Remaining oil in the zone of the well - *High Positive Reward*\n",
    "\n",
    "- After Drilling\n",
    "    -  Total UCS/MSE it was drilled through - *Negative Reward based on the UCS total, can also relate it to a USD amount*    \n",
    "    -  Total Well Length - *Negative Reward based on the pipe count, can also relate it to a USD amount* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b0949",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simple Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ace1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SimpleDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        self.model = np.loadtxt(\n",
    "            env_config[\"model_path\"],\n",
    "            delimiter=env_config[\"delim\"],\n",
    "        )\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=1, shape=(self.nrow, self.ncol), dtype=\"bool\"\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def step(  # noqa: C901\n",
    "        self, action: int\n",
    "    ) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        done = False\n",
    "        actions = {\n",
    "            0: [1, 0],  # down\n",
    "            1: [0, -1],  # left\n",
    "            2: [0, 1],  # right\n",
    "            3: [-1, 0],  # up\n",
    "        }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        else:\n",
    "            reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            reward = 0\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        info: dict[str, Any] = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"\n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "\n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.surface_hole_location = [1, random.randint(0, self.ncol - 1)]  # noqa: S311\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.bit_location = self.surface_hole_location\n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e44cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Multidriller Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae97ad3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MultiDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment for multiple wells\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         reward = 0\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        else:\n",
    "            self.reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "#             reward = 0\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "#                 done = True\n",
    "                self.reward = -100  \n",
    "#                 print('    Immediate 180 degree turn')\n",
    "    \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            self.multi_reward += self.reward \n",
    "            \n",
    "            if len(self.trajectory)>0:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "            self.reset_well()\n",
    "            \n",
    "            if self.wells_drilled < self.num_wells:\n",
    "                    done = False            \n",
    "                    \n",
    "            return self.state, self.multi_reward, done, info\n",
    "        else:\n",
    "            self.last_action = action\n",
    "#             print(f'Last action: {actions[self.last_action]}')\n",
    "            return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "\n",
    "        # Log the surface locations already used\n",
    "        self.surface_location.append(self.surface_hole_location[1])\n",
    "        \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fe87d",
   "metadata": {},
   "source": [
    "# Reward based on Proximity Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470bc23",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb22f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "            # Reward from the model\n",
    "            self.reward = (self.model[newrow, newcol] * 2)\n",
    "            \n",
    "            # Checking if the reward from the model is negative and stopping the well\n",
    "            if self.reward < 0:\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:                \n",
    "                # Giving a small reward to encourage the agent to use pipes     \n",
    "                self.reward += -self.pipe_used/10\n",
    "                                \n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        # Avoid going along the surface\n",
    "        if ((self.bit_location != self.surface_hole_location) &\n",
    "                (self.bit_location[0] == 0)):\n",
    "            self.reward = -10\n",
    "            done = True\n",
    "#             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -10  \n",
    "#                 done = True\n",
    "#                 print('    Immediate 180 degree turn')\n",
    "\n",
    "        if self.reward > 0:\n",
    "            self.multi_reward += self.reward   \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                self.reset_well()\n",
    "                \n",
    "                if len(self.multi_trajectory) < self.num_wells:\n",
    "#                     print(\"MULTIREWARD\")\n",
    "                    return self.state, self.multi_reward, done, info  \n",
    "                \n",
    "            else:\n",
    "                self.reset_well()\n",
    "                self.reward = - 10            \n",
    "            \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"MULTIREWARD\")\n",
    "                \n",
    "                return self.state, self.multi_reward, done, info\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "                \n",
    "#             return self.state, self.reward, done, info\n",
    "        \n",
    "        else:\n",
    "            self.last_action = action\n",
    "        \n",
    "#         print(\"REWARD\")\n",
    "            \n",
    "        return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715cecb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9ab6b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model between o-10\n",
    "        self.model = self.model*(100/self.model.max())\n",
    "\n",
    "        self.model[np.less(self.model,0)] = -100\n",
    "        self.model[self.model == 0] = 1\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:               \n",
    "                \n",
    "            # Incremental Reward from the model\n",
    "#             self.reward = sum([self.model[x,y]*2 for x,y in self.trajectory[1:]])\n",
    "            \n",
    "            model_reward = (self.model[newrow, newcol])\n",
    "            \n",
    "            # Checking if the incremental reward from the model is negative and stopping the well\n",
    "            if model_reward < 0:\n",
    "                done = True\n",
    "                self.reward = -100\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:\n",
    "                # Giving a small -ve reward to encourage the agent to use less pipes     \n",
    "                self.reward += (model_reward - self.pipe_used)\n",
    "#                 print(f'Model Reward: {self.reward}')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#         # Avoid going along the surface\n",
    "#         if ((self.bit_location != self.surface_hole_location) &\n",
    "#                 (self.bit_location[0] == 0)):\n",
    "#             self.reward += -100\n",
    "#             done = True\n",
    "# #             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -100\n",
    "                done = True\n",
    "#                 print('    Immediate 180 degree turn')  \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "#         print(done)\n",
    "        if done:\n",
    "            self.wells_drilled += 1  \n",
    "#             print('Well Done')\n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                \n",
    "                # Update state\n",
    "                self.update_state()\n",
    "                \n",
    "                if self.reward > 0:\n",
    "                    self.multi_reward += self.reward\n",
    "                else:\n",
    "                    self.multi_reward = -100\n",
    "                \n",
    "            else:\n",
    "                self.multi_reward = -100   \n",
    "                       \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"FINAL REWARD\")\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.multi_reward = -100                \n",
    "            \n",
    "            self.reset_well()\n",
    "            \n",
    "        else:\n",
    "            self.last_action = action\n",
    "            self.multi_reward += self.reward\n",
    "            \n",
    "#         print(self.reward)\n",
    "             \n",
    "        return self.state, self.multi_reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d51445",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb91f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model between o-10\n",
    "        self.model = self.model*(100/self.model.max())\n",
    "\n",
    "        self.model[np.less(self.model,0)] = -100\n",
    "        self.model[self.model == 0] = 1\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:               \n",
    "                \n",
    "            # Incremental Reward from the model\n",
    "#             self.reward = sum([self.model[x,y]*2 for x,y in self.trajectory[1:]])\n",
    "            \n",
    "            model_reward = (self.model[newrow, newcol])\n",
    "            \n",
    "            # Checking if the incremental reward from the model is negative and stopping the well\n",
    "            if model_reward < 0:\n",
    "                done = True\n",
    "                self.reward = -100\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:\n",
    "                # Giving a small -ve reward to encourage the agent to use less pipes     \n",
    "                self.reward += (model_reward - self.pipe_used)\n",
    "#                 print(f'Model Reward: {self.reward}')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#         # Avoid going along the surface\n",
    "#         if ((self.bit_location != self.surface_hole_location) &\n",
    "#                 (self.bit_location[0] == 0)):\n",
    "#             self.reward += -100\n",
    "#             done = True\n",
    "# #             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -100\n",
    "                done = True\n",
    "#                 print('    Immediate 180 degree turn')  \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "#         print(done)\n",
    "        if done:\n",
    "            self.wells_drilled += 1  \n",
    "#             print('Well Done')\n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                \n",
    "                # Update state\n",
    "                self.update_state()\n",
    "                \n",
    "                if self.reward > 0:\n",
    "                    self.multi_reward += self.reward\n",
    "                else:\n",
    "                    self.multi_reward = -100\n",
    "                \n",
    "            else:\n",
    "                self.multi_reward = -100   \n",
    "                       \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"FINAL REWARD\")\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.multi_reward = -100                \n",
    "            \n",
    "            self.reset_well()\n",
    "            \n",
    "        else:\n",
    "            self.last_action = action\n",
    "            self.multi_reward += self.reward\n",
    "            \n",
    "#         print(self.reward)\n",
    "             \n",
    "        return self.state, self.multi_reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf5147",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Horizontal well Driller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0bd45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Horizontal well driller with a specific start point\n",
    "\n",
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, PReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD , Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63eb89b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "model = np.loadtxt(env_config[\"model_path\"],\n",
    "                   delimiter=env_config[\"delim\"])\n",
    "\n",
    "model[np.less(model,0)] = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859cafe2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the bit will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55f091",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847f4d8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f856b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfcb79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8248fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(model)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd1e34",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b03a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55d2dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167252ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        \n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            \n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5f097",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52e20c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583d0f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b3820",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f65dfc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9890c57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d18b24",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdfadbe1",
   "metadata": {},
   "source": [
    "# Simple  Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8b73063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDriller:  # type: ignore\n",
    "    \"\"\"Driller environment for horizontal wells with self.rewards based on Q learning\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "\n",
    "        self.rewards = np.loadtxt(env_config[\"model_path\"],\n",
    "                                  delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "\n",
    "        # Normalizing the model\n",
    "        self.rewards = self.rewards * (100 / self.rewards.max())\n",
    "\n",
    "        self.rewards[np.less(self.rewards, 0)] = -100\n",
    "        self.rewards[self.rewards == 0] = -1\n",
    "\n",
    "        self.actions = ['up', 'right', 'down', 'left']\n",
    "\n",
    "        self.q_values = np.zeros((self.rewards.shape[0],\n",
    "                                  self.rewards.shape[1],\n",
    "                                  len(self.actions)))\n",
    "\n",
    "        self.trajectory = []        \n",
    "        self.end = 0\n",
    "        \n",
    "        self.action_cache = np.nan \n",
    "        \n",
    "        self.explored = np.zeros((self.rewards.shape[0],\n",
    "                                  self.rewards.shape[1]))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define a function that determines if the specified location is a terminal state\n",
    "    def is_terminal_state(self, current_row_index, current_column_index):\n",
    "        if ((len(self.trajectory) > 1) &\n",
    "                (self.rewards[current_row_index, current_column_index] == -100)):\n",
    "            self.end = 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    # define a function that will choose a random, non-terminal starting location\n",
    "    def get_starting_location(self):\n",
    "        # get a random column index\n",
    "        current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "        current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "        return current_row_index, current_column_index\n",
    "#         return 18, 18\n",
    "\n",
    "#     def get_unique_starting_location(self):\n",
    "#         # get a random column index\n",
    "#         current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "#         current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "#         return current_row_index, current_column_index\n",
    "# #         return 18, 0\n",
    "    \n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "    # define a function that will decide the valid actions to avoid crashing into itself\n",
    "    def get_valid_actions(self, current_row_index, current_column_index):\n",
    "        va = [0, 1, 2, 3]\n",
    "        try:\n",
    "            # Avoid turning back into itself\n",
    "            if [current_row_index - 1, current_column_index] in self.trajectory:\n",
    "                va.remove(0)\n",
    "            if [current_row_index, current_column_index + 1] in self.trajectory:\n",
    "                va.remove(1)\n",
    "            if [current_row_index + 1, current_column_index] in self.trajectory:\n",
    "                va.remove(2)\n",
    "            if [current_row_index, current_column_index - 1] in self.trajectory:\n",
    "                va.remove(3)\n",
    "\n",
    "            # Remove left move if it is the first column\n",
    "            if current_column_index == 0:\n",
    "                va.remove(3)\n",
    "\n",
    "#             # Remove up move if it is the first row\n",
    "#             if current_row_index == 0:\n",
    "#                 va.remove(0)\n",
    "                \n",
    "            # Force to move down when at surface\n",
    "            if current_row_index == 0:\n",
    "                return [2]\n",
    "\n",
    "\n",
    "            # Remove right move if it is the last column\n",
    "            if current_column_index == (self.rewards.shape[1]-1):\n",
    "                va.remove(1)\n",
    "\n",
    "            # Remove down move if it is the last row\n",
    "            if current_row_index == (self.rewards.shape[0]-1):\n",
    "                va.remove(2)\n",
    "                \n",
    "            # Avoid going up if is gonna hit the surface\n",
    "            if (current_row_index - 1) == 0:\n",
    "                va.remove(0)\n",
    "            \n",
    "#             # Avoid wellbore looping\n",
    "#             if self.action_cache.notna():\n",
    "#                 va.remove(self.action_cache)\n",
    "\n",
    "        except:\n",
    "#             self.end = 1\n",
    "            pass\n",
    "            \n",
    "        return va\n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "    def get_next_action(self, current_row_index, current_column_index, epsilon):\n",
    "        \n",
    "        valid_actions = self.get_valid_actions(current_row_index, current_column_index)\n",
    "        \n",
    "        if len(valid_actions) == 0:\n",
    "            self.end = 1\n",
    "            \n",
    "        if (len(valid_actions) != 0) & (np.random.random() < epsilon):\n",
    "            action = max(valid_actions,key = lambda i: self.q_values[current_row_index, current_column_index].tolist()[i])\n",
    "#             print(f'Valid Actions: {valid_actions}, Picked Action: {action}')\n",
    "            return action\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def get_next_action_train(self, current_row_index, current_column_index, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.argmax(self.q_values[current_row_index, current_column_index])\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define a function that will get the next location based on the chosen action\n",
    "    def get_next_location(self, current_row_index, current_column_index, action_index):\n",
    "\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        \n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.rewards.shape[1] - 1:\n",
    "            new_column_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.rewards.shape[0] - 1:\n",
    "            new_row_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "        else:\n",
    "            self.end = 1\n",
    "\n",
    "        return new_row_index, new_column_index\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "  \n",
    "    def get_rewards(self, row_index, column_index): \n",
    "        \n",
    "        # From Model\n",
    "        reward = self.rewards[row_index, column_index]*15\n",
    "\n",
    "        # To encourage to maintain the shortest path\n",
    "        reward += -len(self.trajectory)*25\n",
    "\n",
    "        # To ensure that a horizontal well is drilled\n",
    "        reward += abs(self.trajectory[-1][1] - self.trajectory[0][1])*10\n",
    "    #                     print(reward)\n",
    "\n",
    "        # To make sure max  amount of target pipes are used\n",
    "        reward += -(self.available_pipe -len(self.trajectory))*5\n",
    "\n",
    "        # Adding a -ve reward to encourage the agent to visit unique rows,columns\n",
    "        rows = [i[0] for i in self.trajectory]\n",
    "        columns = [i[1] for i in self.trajectory]\n",
    "\n",
    "        reward += -(len(rows) - len(set(rows)))*10\n",
    "        reward += -(len(columns) - len(set(columns)))*20\n",
    "\n",
    "    #                     # Add a -ve reward to identify simultaneous right/left turns in the to avoid wellbore tornado effect\n",
    "    #                     if (action_index == self.action_cache):\n",
    "    #                         reward += -100\n",
    "    \n",
    "        return reward \n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function to train and populate the q table\n",
    "    def populate_q_table(self, num_episodes, epsilon = 0.1, discount_factor = 0.9, learning_rate = 0.9):\n",
    "        print('Training Started!')\n",
    "        for episode in range(num_episodes):\n",
    "            self.reset()\n",
    "\n",
    "            # get the starting location for this episode\n",
    "            row_index, column_index = self.get_starting_location()\n",
    "#             print(row_index, column_index)\n",
    "\n",
    "            self.trajectory.append([row_index, column_index])\n",
    "#             print(self.trajectory)\n",
    "            \n",
    "#             print(self.rewards[row_index, column_index])\n",
    "            \n",
    "#             print(self.is_terminal_state(row_index, column_index))\n",
    "\n",
    "            # continue taking actions (i.e., moving) until we reach a terminal state\n",
    "            while not (self.is_terminal_state(row_index, column_index) | (self.end == 1)):\n",
    "\n",
    "                # choose which action to take (i.e., where to move next)\n",
    "                action_index = self.get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "                # perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "                old_row_index, old_column_index = row_index, column_index  # store the old row and column indexes\n",
    "                row_index, column_index = self.get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "                self.trajectory.append([row_index, column_index])\n",
    "                reward = self.get_rewards(row_index, column_index)                 \n",
    "\n",
    "                    \n",
    "                if (action_index == 1) | (action_index == 3):\n",
    "                    self.action_cache = action_index                    \n",
    "\n",
    "                old_q_value = self.q_values[old_row_index, old_column_index, action_index]\n",
    "\n",
    "                temporal_difference = reward + (\n",
    "                            discount_factor * np.max(self.q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "                # update the Q-value for the previous state and action pair\n",
    "                new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "#                 print(new_q_value)\n",
    "\n",
    "                self.q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "                self.explored[old_row_index, old_column_index] = 1\n",
    "    \n",
    "    \n",
    "            if (episode != 0) & ((episode + 1) % 100_000 == 0):\n",
    "                print(f'    {\"{:,}\".format(episode + 1)} episodes completed')\n",
    "\n",
    "#             print(self.trajectory)\n",
    "        \n",
    "        print('Training Complete!')\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function that will get the shortest path\n",
    "    def get_shortest_path(self, start_row_index, start_column_index):\n",
    "        self.reset()\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        self.trajectory.append([current_row_index, current_column_index])\n",
    "\n",
    "        pipes_used = 0\n",
    "#         print(self.is_terminal_state(current_row_index, current_column_index))\n",
    "        \n",
    "        while not (self.is_terminal_state(current_row_index, current_column_index) | (self.end == 1)):\n",
    "#             print(self.trajectory)\n",
    "            # get the best action to take\n",
    "            action_index = self.get_next_action(current_row_index, current_column_index, 1.)\n",
    "#             print(current_row_index, current_column_index)\n",
    "#             print(self.actions[action_index])\n",
    "            # move to the next location on the path, and add the new location to the list\n",
    "            current_row_index, current_column_index = self.get_next_location(current_row_index, current_column_index,\n",
    "                                                                        action_index)\n",
    "#             print(f'{current_row_index}, {current_column_index}\\n')\n",
    "\n",
    "            \n",
    "            pipes_used += 1\n",
    "\n",
    "            if (pipes_used == self.available_pipe):\n",
    "                self.end = 1\n",
    "                print('Pipes Over')\n",
    "                \n",
    "            if ([current_row_index, current_column_index] in self.trajectory):\n",
    "                self.end = 1\n",
    "                print(f'Index in trajectory - [{current_row_index},{current_column_index}]')\n",
    "                \n",
    "            else:\n",
    "                self.trajectory.append([current_row_index, current_column_index])\n",
    "#                 print(self.trajectory)\n",
    "\n",
    "        return self.trajectory\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function that will reset everything\n",
    "    def reset(self):\n",
    "        self.trajectory = []        \n",
    "        self.end = 0\n",
    "        self.action_cache = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bab4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 1, delim=\",\")\n",
    "env = QDriller(env_config)\n",
    "\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66edc82a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started!\n",
      "    100,000 episodes completed\n",
      "    200,000 episodes completed\n",
      "    300,000 episodes completed\n",
      "    400,000 episodes completed\n",
      "    500,000 episodes completed\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "env.populate_q_table(500_000)\n",
    "\n",
    "# plt.figure(figsize=(15, 7))\n",
    "# plt.imshow(env.explored, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbae4d49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index in trajectory - [17,9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_row = 0\n",
    "start_column = 1\n",
    "\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "\n",
    "trajectory = env.get_shortest_path(start_row, start_column)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.rewards, vmin=-100, vmax=100)\n",
    "\n",
    "for i in range(0,len(trajectory)):\n",
    "    traj_z, traj_x = np.asarray(trajectory).T\n",
    "    plt.plot(traj_x, traj_z, \"-\", linewidth=6, color = 'k')\n",
    "\n",
    "plt.xticks(np.arange(0, 80, 1.0))\n",
    "plt.yticks(np.arange(0, 40, 1.0))\n",
    "plt.xlim([-0.5, 79.5])\n",
    "plt.ylim([39.5, -0.5])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a604cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78987a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be3f6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c203c466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d696c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82244f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d970039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abcfcf4f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bef83",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 1\n",
    "\n",
    "actions = {\n",
    "           0: [1, 0],  # down\n",
    "           1: [0, -1],  # left\n",
    "           2: [0, 1],  # right\n",
    "           3: [-1, 0],  # up\n",
    "          }\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    \n",
    "    print(\"Beginning Drill Campaign:\", episode)\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "#         print(f\"    Action: {actions[action]}\")\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Total Reward: {reward}\")\n",
    "#         print(f\"    done: {done}\\n\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273352b",
   "metadata": {},
   "source": [
    "# Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb3de7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4e748",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "env = RewardDriller(env_config)\n",
    "# env = MultiDriller(env_config)\n",
    "\n",
    "\n",
    "ppo = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "ppo.learn(total_timesteps = 800_000, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ac943",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = MultiDriller(env_config)\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = ppo.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a2b3d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad433c1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c4fc7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "dqn = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "dqn.learn(total_timesteps=500_000, log_interval=1_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b03e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = dqn.predict(state, deterministic=True)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a182590",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31eaef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc33143",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68430c-913a-422d-a19b-8a284d7bc5f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=100, num_wells = 3, delim=\",\")\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "a2c = A2C(\"MlpPolicy\", env, verbose=3)\n",
    "a2c.learn(total_timesteps=500_000, log_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263efa6f",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 100\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = a2c.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4d162",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
