{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "811ed8a0-f8f4-4f04-9b87-6d18c9d550d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box\n",
    "from gym.spaces import Discrete\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76690cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679268d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Task List\n",
    "- ~~Drill multiple wells, one after the other and not to update the environment after every simulation.~~\n",
    "- ~~Make sure well/wells dont crash into each other/itself or any faults/artifacts~~\n",
    "- ~~Avoid 180 degree turns~~\n",
    "- Have a target zone where the well eventually want to make it to and get higher reward\n",
    "- Use a metric like MSE/UCS to get an estimate on the amount of energy required to drill and optimizing it to have lowest energy usage (also tie in the economic constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822d739",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Action Space\n",
    "- Surface Location ?? Pick it randomly or intentionally?\n",
    "- Number of wells to drill\n",
    "- Bit Movement\n",
    "    -  Up\n",
    "    -  Down\n",
    "    -  Left\n",
    "    -  Right\n",
    "    -  Angle ?? If the grid size is as much as a stand then the max angle should be around 3 degrees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e214e4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Observation Space\n",
    "\n",
    "Same shape [matrix] as the input. Ideally 30 ft by 30 ft to match with the drilling pipe (90 ft by 90 ft for stand). Bool with true for wherever well is located."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459fbd0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Possible Rewards\n",
    "- While Drilling\n",
    "    -  Proximity to Reservoir (based on the percentage of Normalized TOC?) - *Positive Reward*\n",
    "    -  Proximity to Fault - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to itself or other wells - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to the possible depletion zone of an existing well - *VERY HIGH Negative Reward*\n",
    "    -  Remaining oil in the zone of the well - *High Positive Reward*\n",
    "\n",
    "- After Drilling\n",
    "    -  Total UCS/MSE it was drilled through - *Negative Reward based on the UCS total, can also relate it to a USD amount*    \n",
    "    -  Total Well Length - *Negative Reward based on the pipe count, can also relate it to a USD amount* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b0949",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simple Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ace1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SimpleDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        self.model = np.loadtxt(\n",
    "            env_config[\"model_path\"],\n",
    "            delimiter=env_config[\"delim\"],\n",
    "        )\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=1, shape=(self.nrow, self.ncol), dtype=\"bool\"\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def step(  # noqa: C901\n",
    "        self, action: int\n",
    "    ) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        done = False\n",
    "        actions = {\n",
    "            0: [1, 0],  # down\n",
    "            1: [0, -1],  # left\n",
    "            2: [0, 1],  # right\n",
    "            3: [-1, 0],  # up\n",
    "        }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        else:\n",
    "            reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            reward = 0\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        info: dict[str, Any] = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"\n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "\n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.surface_hole_location = [1, random.randint(0, self.ncol - 1)]  # noqa: S311\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.bit_location = self.surface_hole_location\n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e44cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Multidriller Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae97ad3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MultiDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment for multiple wells\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         reward = 0\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        else:\n",
    "            self.reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "#             reward = 0\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "#                 done = True\n",
    "                self.reward = -100  \n",
    "#                 print('    Immediate 180 degree turn')\n",
    "    \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            self.multi_reward += self.reward \n",
    "            \n",
    "            if len(self.trajectory)>0:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "            self.reset_well()\n",
    "            \n",
    "            if self.wells_drilled < self.num_wells:\n",
    "                    done = False            \n",
    "                    \n",
    "            return self.state, self.multi_reward, done, info\n",
    "        else:\n",
    "            self.last_action = action\n",
    "#             print(f'Last action: {actions[self.last_action]}')\n",
    "            return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "\n",
    "        # Log the surface locations already used\n",
    "        self.surface_location.append(self.surface_hole_location[1])\n",
    "        \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fe87d",
   "metadata": {},
   "source": [
    "# Reward based on Proximity Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470bc23",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb22f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "            # Reward from the model\n",
    "            self.reward = (self.model[newrow, newcol] * 2)\n",
    "            \n",
    "            # Checking if the reward from the model is negative and stopping the well\n",
    "            if self.reward < 0:\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:                \n",
    "                # Giving a small reward to encourage the agent to use pipes     \n",
    "                self.reward += -self.pipe_used/10\n",
    "                                \n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        # Avoid going along the surface\n",
    "        if ((self.bit_location != self.surface_hole_location) &\n",
    "                (self.bit_location[0] == 0)):\n",
    "            self.reward = -10\n",
    "            done = True\n",
    "#             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -10  \n",
    "#                 done = True\n",
    "#                 print('    Immediate 180 degree turn')\n",
    "\n",
    "        if self.reward > 0:\n",
    "            self.multi_reward += self.reward   \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                self.reset_well()\n",
    "                \n",
    "                if len(self.multi_trajectory) < self.num_wells:\n",
    "#                     print(\"MULTIREWARD\")\n",
    "                    return self.state, self.multi_reward, done, info  \n",
    "                \n",
    "            else:\n",
    "                self.reset_well()\n",
    "                self.reward = - 10            \n",
    "            \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"MULTIREWARD\")\n",
    "                \n",
    "                return self.state, self.multi_reward, done, info\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "                \n",
    "#             return self.state, self.reward, done, info\n",
    "        \n",
    "        else:\n",
    "            self.last_action = action\n",
    "        \n",
    "#         print(\"REWARD\")\n",
    "            \n",
    "        return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715cecb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9ab6b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model between o-10\n",
    "        self.model = self.model*(100/self.model.max())\n",
    "\n",
    "        self.model[np.less(self.model,0)] = -100\n",
    "        self.model[self.model == 0] = 1\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:               \n",
    "                \n",
    "            # Incremental Reward from the model\n",
    "#             self.reward = sum([self.model[x,y]*2 for x,y in self.trajectory[1:]])\n",
    "            \n",
    "            model_reward = (self.model[newrow, newcol])\n",
    "            \n",
    "            # Checking if the incremental reward from the model is negative and stopping the well\n",
    "            if model_reward < 0:\n",
    "                done = True\n",
    "                self.reward = -100\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:\n",
    "                # Giving a small -ve reward to encourage the agent to use less pipes     \n",
    "                self.reward += (model_reward - self.pipe_used)\n",
    "#                 print(f'Model Reward: {self.reward}')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#         # Avoid going along the surface\n",
    "#         if ((self.bit_location != self.surface_hole_location) &\n",
    "#                 (self.bit_location[0] == 0)):\n",
    "#             self.reward += -100\n",
    "#             done = True\n",
    "# #             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -100\n",
    "                done = True\n",
    "#                 print('    Immediate 180 degree turn')  \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "#         print(done)\n",
    "        if done:\n",
    "            self.wells_drilled += 1  \n",
    "#             print('Well Done')\n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                \n",
    "                # Update state\n",
    "                self.update_state()\n",
    "                \n",
    "                if self.reward > 0:\n",
    "                    self.multi_reward += self.reward\n",
    "                else:\n",
    "                    self.multi_reward = -100\n",
    "                \n",
    "            else:\n",
    "                self.multi_reward = -100   \n",
    "                       \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"FINAL REWARD\")\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.multi_reward = -100                \n",
    "            \n",
    "            self.reset_well()\n",
    "            \n",
    "        else:\n",
    "            self.last_action = action\n",
    "            self.multi_reward += self.reward\n",
    "            \n",
    "#         print(self.reward)\n",
    "             \n",
    "        return self.state, self.multi_reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d51445",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb91f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model between o-10\n",
    "        self.model = self.model*(100/self.model.max())\n",
    "\n",
    "        self.model[np.less(self.model,0)] = -100\n",
    "        self.model[self.model == 0] = 1\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:               \n",
    "                \n",
    "            # Incremental Reward from the model\n",
    "#             self.reward = sum([self.model[x,y]*2 for x,y in self.trajectory[1:]])\n",
    "            \n",
    "            model_reward = (self.model[newrow, newcol])\n",
    "            \n",
    "            # Checking if the incremental reward from the model is negative and stopping the well\n",
    "            if model_reward < 0:\n",
    "                done = True\n",
    "                self.reward = -100\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:\n",
    "                # Giving a small -ve reward to encourage the agent to use less pipes     \n",
    "                self.reward += (model_reward - self.pipe_used)\n",
    "#                 print(f'Model Reward: {self.reward}')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#         # Avoid going along the surface\n",
    "#         if ((self.bit_location != self.surface_hole_location) &\n",
    "#                 (self.bit_location[0] == 0)):\n",
    "#             self.reward += -100\n",
    "#             done = True\n",
    "# #             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -100\n",
    "                done = True\n",
    "#                 print('    Immediate 180 degree turn')  \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "#         print(done)\n",
    "        if done:\n",
    "            self.wells_drilled += 1  \n",
    "#             print('Well Done')\n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                \n",
    "                # Update state\n",
    "                self.update_state()\n",
    "                \n",
    "                if self.reward > 0:\n",
    "                    self.multi_reward += self.reward\n",
    "                else:\n",
    "                    self.multi_reward = -100\n",
    "                \n",
    "            else:\n",
    "                self.multi_reward = -100   \n",
    "                       \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"FINAL REWARD\")\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.multi_reward = -100                \n",
    "            \n",
    "            self.reset_well()\n",
    "            \n",
    "        else:\n",
    "            self.last_action = action\n",
    "            self.multi_reward += self.reward\n",
    "            \n",
    "#         print(self.reward)\n",
    "             \n",
    "        return self.state, self.multi_reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf5147",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Horizontal well Driller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0bd45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Horizontal well driller with a specific start point\n",
    "\n",
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, PReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD , Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63eb89b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "model = np.loadtxt(env_config[\"model_path\"],\n",
    "                   delimiter=env_config[\"delim\"])\n",
    "\n",
    "model[np.less(model,0)] = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859cafe2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the bit will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55f091",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847f4d8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f856b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfcb79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8248fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(model)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd1e34",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b03a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55d2dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167252ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        \n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            \n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5f097",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52e20c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583d0f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b3820",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f65dfc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9890c57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d18b24",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdfadbe1",
   "metadata": {},
   "source": [
    "# Simple  Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b73063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDriller:  # type: ignore\n",
    "    \"\"\"Driller environment for horizontal wells with self.rewards based on Q learning\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "\n",
    "        self.rewards = np.loadtxt(env_config[\"model_path\"],\n",
    "                                  delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "\n",
    "        # Normalizing the model\n",
    "        self.rewards = self.rewards * (100 / self.rewards.max())\n",
    "\n",
    "        self.rewards[np.less(self.rewards, 0)] = -100\n",
    "        self.rewards[self.rewards == 0] = -1\n",
    "\n",
    "        self.actions = ['up', 'right', 'down', 'left']\n",
    "\n",
    "        self.q_values = np.zeros((self.rewards.shape[0],\n",
    "                                  self.rewards.shape[1],\n",
    "                                  len(self.actions)))\n",
    "\n",
    "        self.trajectory = []        \n",
    "        self.end = 0\n",
    "        \n",
    "        self.action_cache = np.nan \n",
    "        \n",
    "        self.explored = np.zeros((self.rewards.shape[0],\n",
    "                                  self.rewards.shape[1]))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define a function that determines if the specified location is a terminal state\n",
    "    def is_terminal_state(self, current_row_index, current_column_index):\n",
    "        if ((len(self.trajectory) > 1) &\n",
    "                (self.rewards[current_row_index, current_column_index] == -100)):\n",
    "            self.end = 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    # define a function that will choose a random, non-terminal starting location\n",
    "    def get_starting_location(self):\n",
    "        # get a random column index\n",
    "        current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "        current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "        return current_row_index, current_column_index\n",
    "#         return 18, 18\n",
    "\n",
    "#     def get_unique_starting_location(self):\n",
    "#         # get a random column index\n",
    "#         current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "#         current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "#         return current_row_index, current_column_index\n",
    "# #         return 18, 0\n",
    "    \n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "    # define a function that will decide the valid actions to avoid crashing into itself\n",
    "    def get_valid_actions(self, current_row_index, current_column_index):\n",
    "        va = [0, 1, 2, 3]\n",
    "        try:\n",
    "            # Avoid turning back into itself\n",
    "            if [current_row_index - 1, current_column_index] in self.trajectory:\n",
    "                va.remove(0)\n",
    "            if [current_row_index, current_column_index + 1] in self.trajectory:\n",
    "                va.remove(1)\n",
    "            if [current_row_index + 1, current_column_index] in self.trajectory:\n",
    "                va.remove(2)\n",
    "            if [current_row_index, current_column_index - 1] in self.trajectory:\n",
    "                va.remove(3)\n",
    "\n",
    "            # Remove left move if it is the first column\n",
    "            if current_column_index == 0:\n",
    "                va.remove(3)\n",
    "\n",
    "#             # Remove up move if it is the first row\n",
    "#             if current_row_index == 0:\n",
    "#                 va.remove(0)\n",
    "                \n",
    "            # Force to move down when at surface\n",
    "            if current_row_index == 0:\n",
    "                return [2]\n",
    "\n",
    "\n",
    "            # Remove right move if it is the last column\n",
    "            if current_column_index == (self.rewards.shape[1]-1):\n",
    "                va.remove(1)\n",
    "\n",
    "            # Remove down move if it is the last row\n",
    "            if current_row_index == (self.rewards.shape[0]-1):\n",
    "                va.remove(2)\n",
    "                \n",
    "            # Avoid going up if is gonna hit the surface\n",
    "            if (current_row_index - 1) == 0:\n",
    "                va.remove(0)\n",
    "            \n",
    "#             # Avoid wellbore looping\n",
    "#             if self.action_cache.notna():\n",
    "#                 va.remove(self.action_cache)\n",
    "\n",
    "        except:\n",
    "#             self.end = 1\n",
    "            pass\n",
    "            \n",
    "        return va\n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "    def get_next_action(self, current_row_index, current_column_index, epsilon):\n",
    "        \n",
    "        valid_actions = self.get_valid_actions(current_row_index, current_column_index)\n",
    "        \n",
    "        if len(valid_actions) == 0:\n",
    "            self.end = 1\n",
    "            \n",
    "        if (len(valid_actions) != 0) & (np.random.random() < epsilon):\n",
    "            action = max(valid_actions,key = lambda i: self.q_values[current_row_index, current_column_index].tolist()[i])\n",
    "#             print(f'Valid Actions: {valid_actions}, Picked Action: {action}')\n",
    "            return action\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def get_next_action_train(self, current_row_index, current_column_index, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.argmax(self.q_values[current_row_index, current_column_index])\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define a function that will get the next location based on the chosen action\n",
    "    def get_next_location(self, current_row_index, current_column_index, action_index):\n",
    "\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        \n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.rewards.shape[1] - 1:\n",
    "            new_column_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.rewards.shape[0] - 1:\n",
    "            new_row_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "        else:\n",
    "            self.end = 1\n",
    "\n",
    "        return new_row_index, new_column_index\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "  \n",
    "    def get_rewards(self, row_index, column_index): \n",
    "        \n",
    "        # From Model\n",
    "        reward = self.rewards[row_index, column_index]*15\n",
    "\n",
    "        # To encourage to maintain the shortest path\n",
    "        reward += -len(self.trajectory)*25\n",
    "\n",
    "        # To ensure that a horizontal well is drilled\n",
    "        reward += abs(self.trajectory[-1][1] - self.trajectory[0][1])*10\n",
    "    #                     print(reward)\n",
    "\n",
    "        # To make sure max  amount of target pipes are used\n",
    "        reward += -(self.available_pipe -len(self.trajectory))*5\n",
    "\n",
    "        # Adding a -ve reward to encourage the agent to visit unique rows,columns\n",
    "        rows = [i[0] for i in self.trajectory]\n",
    "        columns = [i[1] for i in self.trajectory]\n",
    "\n",
    "        reward += -(len(rows) - len(set(rows)))*10\n",
    "        reward += -(len(columns) - len(set(columns)))*20\n",
    "\n",
    "    #                     # Add a -ve reward to identify simultaneous right/left turns in the to avoid wellbore tornado effect\n",
    "    #                     if (action_index == self.action_cache):\n",
    "    #                         reward += -100\n",
    "    \n",
    "        return reward \n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function to train and populate the q table\n",
    "    def populate_q_table(self, num_episodes, epsilon = 0.1, discount_factor = 0.9, learning_rate = 0.9):\n",
    "        print('Training Started!')\n",
    "        for episode in range(num_episodes):\n",
    "            self.reset()\n",
    "\n",
    "            # get the starting location for this episode\n",
    "            row_index, column_index = self.get_starting_location()\n",
    "#             print(row_index, column_index)\n",
    "\n",
    "            self.trajectory.append([row_index, column_index])\n",
    "#             print(self.trajectory)\n",
    "            \n",
    "#             print(self.rewards[row_index, column_index])\n",
    "            \n",
    "#             print(self.is_terminal_state(row_index, column_index))\n",
    "\n",
    "            # continue taking actions (i.e., moving) until we reach a terminal state\n",
    "            while not (self.is_terminal_state(row_index, column_index) | (self.end == 1)):\n",
    "\n",
    "                # choose which action to take (i.e., where to move next)\n",
    "                action_index = self.get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "                # perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "                old_row_index, old_column_index = row_index, column_index  # store the old row and column indexes\n",
    "                row_index, column_index = self.get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "                self.trajectory.append([row_index, column_index])\n",
    "                reward = self.get_rewards(row_index, column_index)                 \n",
    "\n",
    "                    \n",
    "                if (action_index == 1) | (action_index == 3):\n",
    "                    self.action_cache = action_index                    \n",
    "\n",
    "                old_q_value = self.q_values[old_row_index, old_column_index, action_index]\n",
    "\n",
    "                temporal_difference = reward + (\n",
    "                            discount_factor * np.max(self.q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "                # update the Q-value for the previous state and action pair\n",
    "                new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "#                 print(new_q_value)\n",
    "\n",
    "                self.q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "                self.explored[old_row_index, old_column_index] = 1\n",
    "    \n",
    "    \n",
    "            if (episode != 0) & ((episode + 1) % 100_000 == 0):\n",
    "                print(f'    {\"{:,}\".format(episode + 1)} episodes completed')\n",
    "\n",
    "#             print(self.trajectory)\n",
    "        \n",
    "        print('Training Complete!')\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function that will get the shortest path\n",
    "    def get_shortest_path(self, start_row_index, start_column_index):\n",
    "        self.reset()\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        self.trajectory.append([current_row_index, current_column_index])\n",
    "\n",
    "        pipes_used = 0\n",
    "#         print(self.is_terminal_state(current_row_index, current_column_index))\n",
    "        \n",
    "        while not (self.is_terminal_state(current_row_index, current_column_index) | (self.end == 1)):\n",
    "#             print(self.trajectory)\n",
    "            # get the best action to take\n",
    "            action_index = self.get_next_action(current_row_index, current_column_index, 1.)\n",
    "#             print(current_row_index, current_column_index)\n",
    "#             print(self.actions[action_index])\n",
    "            # move to the next location on the path, and add the new location to the list\n",
    "            current_row_index, current_column_index = self.get_next_location(current_row_index, current_column_index,\n",
    "                                                                        action_index)\n",
    "#             print(f'{current_row_index}, {current_column_index}\\n')\n",
    "\n",
    "            \n",
    "            pipes_used += 1\n",
    "\n",
    "            if (pipes_used == self.available_pipe):\n",
    "                self.end = 1\n",
    "                print('Pipes Over')\n",
    "                \n",
    "            if ([current_row_index, current_column_index] in self.trajectory):\n",
    "                self.end = 1\n",
    "                print(f'Index in trajectory - [{current_row_index},{current_column_index}]')\n",
    "                \n",
    "            else:\n",
    "                self.trajectory.append([current_row_index, current_column_index])\n",
    "#                 print(self.trajectory)\n",
    "\n",
    "        return self.trajectory\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function that will reset everything\n",
    "    def reset(self):\n",
    "        self.trajectory = []        \n",
    "        self.end = 0\n",
    "        self.action_cache = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bab4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 1, delim=\",\")\n",
    "env = QDriller(env_config)\n",
    "\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66edc82a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started!\n",
      "    100,000 episodes completed\n",
      "    200,000 episodes completed\n",
      "    300,000 episodes completed\n",
      "    400,000 episodes completed\n",
      "    500,000 episodes completed\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "env.populate_q_table(500_000)\n",
    "\n",
    "# plt.figure(figsize=(15, 7))\n",
    "# plt.imshow(env.explored, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbae4d49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipes Over\n",
      "Index in trajectory - [29,19]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_row = 27\n",
    "start_column = 1\n",
    "\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "\n",
    "trajectory = env.get_shortest_path(start_row, start_column)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.rewards, vmin=-100, vmax=100)\n",
    "\n",
    "for i in range(0,len(trajectory)):\n",
    "    traj_z, traj_x = np.asarray(trajectory).T\n",
    "    plt.plot(traj_x, traj_z, \"-\", linewidth=6, color = 'k')\n",
    "\n",
    "plt.xticks(np.arange(0, 80, 1.0))\n",
    "plt.yticks(np.arange(0, 40, 1.0))\n",
    "plt.xlim([-0.5, 79.5])\n",
    "plt.ylim([39.5, -0.5])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a604cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78987a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be3f6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c203c466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d696c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82244f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d970039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abcfcf4f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bef83",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 1\n",
    "\n",
    "actions = {\n",
    "           0: [1, 0],  # down\n",
    "           1: [0, -1],  # left\n",
    "           2: [0, 1],  # right\n",
    "           3: [-1, 0],  # up\n",
    "          }\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    \n",
    "    print(\"Beginning Drill Campaign:\", episode)\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "#         print(f\"    Action: {actions[action]}\")\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Total Reward: {reward}\")\n",
    "#         print(f\"    done: {done}\\n\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273352b",
   "metadata": {},
   "source": [
    "# Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb3de7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4e748",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "env = RewardDriller(env_config)\n",
    "# env = MultiDriller(env_config)\n",
    "\n",
    "\n",
    "ppo = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "ppo.learn(total_timesteps = 800_000, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ac943",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = MultiDriller(env_config)\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = ppo.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a2b3d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad433c1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c4fc7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "dqn = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "dqn.learn(total_timesteps=500_000, log_interval=1_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b03e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = dqn.predict(state, deterministic=True)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a182590",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31eaef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc33143",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68430c-913a-422d-a19b-8a284d7bc5f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=100, num_wells = 3, delim=\",\")\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "a2c = A2C(\"MlpPolicy\", env, verbose=3)\n",
    "a2c.learn(total_timesteps=500_000, log_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263efa6f",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 100\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = a2c.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4d162",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
