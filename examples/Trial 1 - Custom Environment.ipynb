{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "811ed8a0-f8f4-4f04-9b87-6d18c9d550d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box\n",
    "from gym.spaces import Discrete\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76690cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679268d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Task List\n",
    "- ~~Drill multiple wells, one after the other and not to update the environment after every simulation.~~\n",
    "- ~~Make sure well/wells dont crash into each other/itself or any faults/artifacts~~\n",
    "- ~~Avoid 180 degree turns~~\n",
    "- Have a target zone where the well eventually want to make it to and get higher reward\n",
    "- Use a metric like MSE/UCS to get an estimate on the amount of energy required to drill and optimizing it to have lowest energy usage (also tie in the economic constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822d739",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Action Space\n",
    "- Surface Location ?? Pick it randomly or intentionally?\n",
    "- Number of wells to drill\n",
    "- Bit Movement\n",
    "    -  Up\n",
    "    -  Down\n",
    "    -  Left\n",
    "    -  Right\n",
    "    -  Angle ?? If the grid size is as much as a stand then the max angle should be around 3 degrees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e214e4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Observation Space\n",
    "\n",
    "Same shape [matrix] as the input. Ideally 30 ft by 30 ft to match with the drilling pipe (90 ft by 90 ft for stand). Bool with true for wherever well is located."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459fbd0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Possible Rewards\n",
    "- While Drilling\n",
    "    -  Proximity to Reservoir (based on the percentage of Normalized TOC?) - *Positive Reward*\n",
    "    -  Proximity to Fault - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to itself or other wells - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to the possible depletion zone of an existing well - *VERY HIGH Negative Reward*\n",
    "    -  Remaining oil in the zone of the well - *High Positive Reward*\n",
    "\n",
    "- After Drilling\n",
    "    -  Total UCS/MSE it was drilled through - *Negative Reward based on the UCS total, can also relate it to a USD amount*    \n",
    "    -  Total Well Length - *Negative Reward based on the pipe count, can also relate it to a USD amount* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b0949",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simple Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ace1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SimpleDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        self.model = np.loadtxt(\n",
    "            env_config[\"model_path\"],\n",
    "            delimiter=env_config[\"delim\"],\n",
    "        )\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=1, shape=(self.nrow, self.ncol), dtype=\"bool\"\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def step(  # noqa: C901\n",
    "        self, action: int\n",
    "    ) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        done = False\n",
    "        actions = {\n",
    "            0: [1, 0],  # down\n",
    "            1: [0, -1],  # left\n",
    "            2: [0, 1],  # right\n",
    "            3: [-1, 0],  # up\n",
    "        }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        else:\n",
    "            reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            reward = 0\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        info: dict[str, Any] = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"\n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "\n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.surface_hole_location = [1, random.randint(0, self.ncol - 1)]  # noqa: S311\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.bit_location = self.surface_hole_location\n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e44cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Multidriller Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae97ad3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MultiDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment for multiple wells\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         reward = 0\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        else:\n",
    "            self.reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "#             reward = 0\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "#                 done = True\n",
    "                self.reward = -100  \n",
    "#                 print('    Immediate 180 degree turn')\n",
    "    \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            self.multi_reward += self.reward \n",
    "            \n",
    "            if len(self.trajectory)>0:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "            self.reset_well()\n",
    "            \n",
    "            if self.wells_drilled < self.num_wells:\n",
    "                    done = False            \n",
    "                    \n",
    "            return self.state, self.multi_reward, done, info\n",
    "        else:\n",
    "            self.last_action = action\n",
    "#             print(f'Last action: {actions[self.last_action]}')\n",
    "            return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "\n",
    "        # Log the surface locations already used\n",
    "        self.surface_location.append(self.surface_hole_location[1])\n",
    "        \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fe87d",
   "metadata": {},
   "source": [
    "# Reward based on Proximity Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e482042",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb22f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "            # Reward from the model\n",
    "            self.reward = (self.model[newrow, newcol] * 2)\n",
    "            \n",
    "            # Checking if the reward from the model is negative and stopping the well\n",
    "            if self.reward < 0:\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:                \n",
    "                # Giving a small reward to encourage the agent to use pipes     \n",
    "                self.reward += -self.pipe_used/10\n",
    "                                \n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        # Avoid going along the surface\n",
    "        if ((self.bit_location != self.surface_hole_location) &\n",
    "                (self.bit_location[0] == 0)):\n",
    "            self.reward = -10\n",
    "            done = True\n",
    "#             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -10  \n",
    "#                 done = True\n",
    "#                 print('    Immediate 180 degree turn')\n",
    "\n",
    "        if self.reward > 0:\n",
    "            self.multi_reward += self.reward   \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                self.reset_well()\n",
    "                \n",
    "                if len(self.multi_trajectory) < self.num_wells:\n",
    "#                     print(\"MULTIREWARD\")\n",
    "                    return self.state, self.multi_reward, done, info  \n",
    "                \n",
    "            else:\n",
    "                self.reset_well()\n",
    "                self.reward = - 10            \n",
    "            \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"MULTIREWARD\")\n",
    "                \n",
    "                return self.state, self.multi_reward, done, info\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "                \n",
    "#             return self.state, self.reward, done, info\n",
    "        \n",
    "        else:\n",
    "            self.last_action = action\n",
    "        \n",
    "#         print(\"REWARD\")\n",
    "            \n",
    "        return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6fa716",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc772cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model between o-10\n",
    "        self.model = self.model*(100/self.model.max())\n",
    "\n",
    "        self.model[np.less(self.model,0)] = -100\n",
    "        self.model[self.model == 0] = 1\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:               \n",
    "                \n",
    "            # Incremental Reward from the model\n",
    "#             self.reward = sum([self.model[x,y]*2 for x,y in self.trajectory[1:]])\n",
    "            \n",
    "            model_reward = (self.model[newrow, newcol])\n",
    "            \n",
    "            # Checking if the incremental reward from the model is negative and stopping the well\n",
    "            if model_reward < 0:\n",
    "                done = True\n",
    "                self.reward = -100\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:\n",
    "                # Giving a small -ve reward to encourage the agent to use less pipes     \n",
    "                self.reward += (model_reward - self.pipe_used)\n",
    "#                 print(f'Model Reward: {self.reward}')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#         # Avoid going along the surface\n",
    "#         if ((self.bit_location != self.surface_hole_location) &\n",
    "#                 (self.bit_location[0] == 0)):\n",
    "#             self.reward += -100\n",
    "#             done = True\n",
    "# #             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -100\n",
    "                done = True\n",
    "#                 print('    Immediate 180 degree turn')  \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "#         print(done)\n",
    "        if done:\n",
    "            self.wells_drilled += 1  \n",
    "#             print('Well Done')\n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                \n",
    "                # Update state\n",
    "                self.update_state()\n",
    "                \n",
    "                if self.reward > 0:\n",
    "                    self.multi_reward += self.reward\n",
    "                else:\n",
    "                    self.multi_reward = -100\n",
    "                \n",
    "            else:\n",
    "                self.multi_reward = -100   \n",
    "                       \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"FINAL REWARD\")\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.multi_reward = -100                \n",
    "            \n",
    "            self.reset_well()\n",
    "            \n",
    "        else:\n",
    "            self.last_action = action\n",
    "            self.multi_reward += self.reward\n",
    "            \n",
    "#         print(self.reward)\n",
    "             \n",
    "        return self.state, self.multi_reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf5147",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Horizontal well Driller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0bd45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Horizontal well driller with a specific start point\n",
    "\n",
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, PReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD , Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa9d95",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "model = np.loadtxt(env_config[\"model_path\"],\n",
    "                   delimiter=env_config[\"delim\"])\n",
    "\n",
    "model[np.less(model,0)] = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df397d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the bit will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55f091",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847f4d8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f856b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfcb79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8248fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(model)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd1e34",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b03a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55d2dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167252ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        \n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            \n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5f097",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52e20c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583d0f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b3820",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f65dfc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d98b531",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13303d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df60bc59",
   "metadata": {},
   "source": [
    "# Q table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd4c316",
   "metadata": {},
   "source": [
    "## Class Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49c4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 1, delim=\",\")\n",
    "# env = RewardDriller(env_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6766c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDriller:  # type: ignore\n",
    "    \"\"\"Driller environment for horizontal wells with self.rewards based on Q learning\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "\n",
    "        self.rewards = np.loadtxt(env_config[\"model_path\"],\n",
    "                                  delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model\n",
    "        self.rewards = self.rewards * (100 / self.rewards.max())\n",
    "\n",
    "        self.rewards[np.less(self.rewards, 0)] = -100\n",
    "        self.rewards[self.rewards == 0] = -1\n",
    "\n",
    "        self.actions = ['up', 'right', 'down', 'left']\n",
    "\n",
    "        self.q_values = np.zeros((self.rewards.shape[0],\n",
    "                                  self.rewards.shape[1],\n",
    "                                  len(self.actions)))\n",
    "\n",
    "        self.trajectory = []\n",
    "\n",
    "    # define a function that determines if the specified location is a terminal state\n",
    "    def is_terminal_state(self, current_row_index, current_column_index):\n",
    "        if ((len(self.trajectory) > 1) &\n",
    "                (self.rewards[current_row_index, current_column_index] == -100)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    # define a function that will choose a random, non-terminal starting location\n",
    "    def get_starting_location(self):\n",
    "        # get a random column index\n",
    "        current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "        return 0, current_column_index\n",
    "\n",
    "    # define a function that will decide the valid actions to avoid crashing into itself\n",
    "    def get_valid_actions(self, current_row_index, current_column_index):\n",
    "        va = [0, 1, 2, 3]\n",
    "        if [current_row_index - 1, current_column_index] in self.trajectory:\n",
    "            va.remove(0)\n",
    "        if [current_row_index, current_column_index + 1] in self.trajectory:\n",
    "            va.remove(1)\n",
    "        if [current_row_index + 1, current_column_index] in self.trajectory:\n",
    "            va.remove(2)\n",
    "        if [current_row_index, current_column_index - 1] in self.trajectory:\n",
    "            va.remove(3)\n",
    "        return va\n",
    "\n",
    "    # define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "    def get_next_action(self, current_row_index, current_column_index, epsilon):\n",
    "        valid_actions = self.get_valid_actions(current_row_index, current_column_index)\n",
    "        max_q = np.max(self.q_values[current_row_index, current_column_index][valid_actions])\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = (np.where(self.q_values[current_row_index, current_column_index] == max_q)[0][0])\n",
    "#             print(f'Valid Actions: {valid_actions}, Picked Action: {action}')\n",
    "            return action\n",
    "\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "    def get_next_action_train(self, current_row_index, current_column_index, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.argmax(self.q_values[current_row_index, current_column_index])\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "\n",
    "    # define a function that will get the next location based on the chosen action\n",
    "    def get_next_location(self, current_row_index, current_column_index, action_index):\n",
    "\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.rewards.shape[1] - 1:\n",
    "            new_column_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.rewards.shape[0] - 1:\n",
    "            new_row_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "\n",
    "        return new_row_index, new_column_index\n",
    "\n",
    "    # Define a function to train and populate the q table\n",
    "    def populate_q_table(self, num_episodes, epsilon = 0.1, discount_factor = 0.9, learning_rate = 0.9):\n",
    "        for episode in range(num_episodes):\n",
    "            self.trajectory = []\n",
    "\n",
    "            # get the starting location for this episode\n",
    "            row_index, column_index = self.get_starting_location()\n",
    "#             print(row_index, column_index)\n",
    "\n",
    "            self.trajectory.append([row_index, column_index])\n",
    "#             print(self.trajectory)\n",
    "            \n",
    "#             print(self.rewards[row_index, column_index])\n",
    "            \n",
    "            # continue taking actions (i.e., moving) until we reach a terminal state\n",
    "#             print(self.is_terminal_state(row_index, column_index))\n",
    "            \n",
    "            while not self.is_terminal_state(row_index, column_index):\n",
    "\n",
    "                # choose which action to take (i.e., where to move next)\n",
    "                action_index = self.get_next_action_train(row_index, column_index, epsilon)\n",
    "\n",
    "                # perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "                old_row_index, old_column_index = row_index, column_index  # store the old row and column indexes\n",
    "                row_index, column_index = self.get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "                # receive the reward for moving to the new state\n",
    "                if ([row_index, column_index] in self.trajectory):\n",
    "                    reward = -100\n",
    "                elif (row_index == 0):\n",
    "                    reward = -100\n",
    "                else:\n",
    "                    reward = self.rewards[row_index, column_index] - len(self.trajectory)\n",
    "                #         print(reward)\n",
    "\n",
    "                self.trajectory.append([row_index, column_index])\n",
    "\n",
    "                old_q_value = self.q_values[old_row_index, old_column_index, action_index]\n",
    "\n",
    "                temporal_difference = reward + (\n",
    "                            discount_factor * np.max(self.q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "                # update the Q-value for the previous state and action pair\n",
    "                new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "#                 print(new_q_value)\n",
    "\n",
    "                self.q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "        #     print(trajectory)\n",
    "        self.trajectory = []\n",
    "        print('Training complete!')\n",
    "\n",
    "    # Define a function that will get the shortest path\n",
    "    def get_shortest_path(self, start_row_index, start_column_index, available_pipe):\n",
    "        self.trajectory = []\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        self.trajectory.append([current_row_index, current_column_index])\n",
    "\n",
    "        pipes_used = 0\n",
    "        while not self.is_terminal_state(current_row_index, current_column_index):\n",
    "\n",
    "            # get the best action to take\n",
    "            action_index = self.get_next_action(current_row_index, current_column_index, 1.)\n",
    "\n",
    "            # move to the next location on the path, and add the new location to the list\n",
    "            current_row_index, current_column_index = self.get_next_location(current_row_index, current_column_index,\n",
    "                                                                        action_index)\n",
    "\n",
    "            #         if ([current_row_index, current_column_index] in trajectory)|((current_row_index == 0)):\n",
    "            #             break\n",
    "\n",
    "            self.trajectory.append([current_row_index, current_column_index])\n",
    "            pipes_used += 1\n",
    "\n",
    "            if pipes_used == available_pipe:\n",
    "                break\n",
    "\n",
    "        return self.trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7acaa88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 1, delim=\",\")\n",
    "env = QDriller(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.populate_q_table(500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb93f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_row = 0\n",
    "start_column = 23\n",
    "available_pipe = 100\n",
    "\n",
    "trajectory = env.get_shortest_path(start_row, start_column,available_pipe)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.rewards, vmin=-100, vmax=100)\n",
    "\n",
    "for i in range(0,len(trajectory)):\n",
    "    traj_z, traj_x = np.asarray(trajectory).T\n",
    "    plt.plot(traj_x, traj_z, \"-\", linewidth=6, color = 'k')\n",
    "\n",
    "plt.xticks(np.arange(0, 80, 1.0))\n",
    "plt.yticks(np.arange(0, 40, 1.0))\n",
    "plt.xlim([-0.5, 79.5])\n",
    "plt.ylim([39.5, -0.5])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426a852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fa95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76eeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25369f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3e302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1510448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee29aa0e",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4336f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", delim=\",\", available_pipe=70, num_wells = 1, start_column = 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50daf17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcef6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.loadtxt(env_config[\"model_path\"],\n",
    "                   delimiter=env_config[\"delim\"])\n",
    "\n",
    "# Normalizing the model\n",
    "rewards = rewards*(100/rewards.max())\n",
    "\n",
    "rewards[np.less(rewards,0)] = -100\n",
    "rewards[rewards == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = {\n",
    "#            0: [1, 0],  # down\n",
    "#            1: [0, -1],  # left\n",
    "#            2: [0, 1],  # right\n",
    "#            3: [-1, 0],  # up\n",
    "#           }\n",
    "\n",
    "#define actions\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = np.zeros((rewards.shape[0], rewards.shape[1], len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47503f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that determines if the specified location is a terminal state\n",
    "\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "\n",
    "    if ((len(trajectory) > 1) & \n",
    "        (rewards[current_row_index, current_column_index] == -100)):\n",
    "        return True\n",
    "    \n",
    "#     elif ((len(trajectory) > 1) & \n",
    "#         ([current_row_index, current_column_index] in trajectory)):\n",
    "#         return True\n",
    "    \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#     if ((rewards[current_row_index, current_column_index] <= 0) | \n",
    "#         (self.bit_location in self.trajectory[:-1]) | \n",
    "#         (self.pipe_used == self.available_pipe)):\n",
    "        \n",
    "#         return True\n",
    "\n",
    "#     elif (self.last_action != None):\n",
    "#         if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "#             return True\n",
    "\n",
    "#     else:\n",
    "#         return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that will choose a random, non-terminal starting location\n",
    "\n",
    "def get_starting_location():        \n",
    "#     current_column_index = env_config[\"start_column\"]\n",
    "    \n",
    "    #get a random row and column index\n",
    "    current_column_index = np.random.randint(rewards.shape[1])\n",
    "    \n",
    "#     while current_column_index in starting_location_cache:\n",
    "#         current_column_index = np.random.randint(rewards.shape[1])\n",
    "        \n",
    "    return 0, current_column_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced57302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(current_row_index, current_column_index):\n",
    "    va = [0, 1, 2, 3]\n",
    "\n",
    "    if [current_row_index - 1, current_column_index] in trajectory:\n",
    "        va.remove(0)\n",
    "\n",
    "    if [current_row_index, current_column_index + 1] in trajectory:\n",
    "        va.remove(1)  \n",
    "        \n",
    "    if [current_row_index + 1, current_column_index] in trajectory:\n",
    "        va.remove(2) \n",
    "\n",
    "    if [current_row_index, current_column_index - 1] in trajectory:\n",
    "        va.remove(3) \n",
    "    \n",
    "    return va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f788bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next_action(0, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "    valid_actions = get_valid_actions(current_row_index, current_column_index)\n",
    "    max_q = np.max(q_values[current_row_index, current_column_index][valid_actions])\n",
    "    \n",
    "    if np.random.random() < epsilon:\n",
    "        action = (np.where(q_values[current_row_index, current_column_index] == max_q)[0][0])\n",
    "        print(f'Valid Actions: {valid_actions}, Picked Action: {action}')\n",
    "        return action\n",
    "    \n",
    "    else: \n",
    "        return np.random.randint(len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that will get the next location based on the chosen action\n",
    "\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "    \n",
    "    new_row_index = current_row_index\n",
    "    new_column_index = current_column_index\n",
    "    if actions[action_index] == 'up' and current_row_index > 0:\n",
    "        new_row_index -= 1\n",
    "        \n",
    "    elif actions[action_index] == 'right' and current_column_index < rewards.shape[1] - 1:\n",
    "        new_column_index += 1\n",
    "        \n",
    "    elif actions[action_index] == 'down' and current_row_index < rewards.shape[0] - 1:\n",
    "        new_row_index += 1\n",
    "        \n",
    "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "        new_column_index -= 1\n",
    "        \n",
    "    return new_row_index, new_column_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe7418a",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee8bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training parameters\n",
    "epsilon = 0.01 #the percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the AI agent should learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521324d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run through training episodes\n",
    "\n",
    "for episode in range(10_000):\n",
    "    trajectory = []\n",
    "    \n",
    "    #get the starting location for this episode\n",
    "    row_index, column_index = get_starting_location()\n",
    "\n",
    "    trajectory.append([row_index, column_index])\n",
    "    \n",
    "    #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "    while not is_terminal_state(row_index, column_index):\n",
    "        \n",
    "        #choose which action to take (i.e., where to move next)\n",
    "        action_index = get_next_action(row_index, column_index, epsilon)\n",
    "        \n",
    "        #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "        old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "        row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "        \n",
    "        #receive the reward for moving to the new state\n",
    "        if ([row_index, column_index] in trajectory):\n",
    "            reward = -100\n",
    "        elif (row_index == 0):\n",
    "            reward = -100\n",
    "        else:\n",
    "            reward = rewards[row_index, column_index] - len(trajectory)/5\n",
    "#         print(reward)\n",
    "\n",
    "        trajectory.append([row_index, column_index])\n",
    "    \n",
    "        old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "        temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "        #update the Q-value for the previous state and action pair\n",
    "        new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "#         print(new_q_value)\n",
    "        q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "        \n",
    "#     print(trajectory)\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1396cd7",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1617f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that will get the shortest path\n",
    "\n",
    "def get_shortest_path(start_row_index, start_column_index, available_pipe):\n",
    "    trajectory = []\n",
    "    current_row_index, current_column_index = start_row_index, start_column_index\n",
    "    trajectory.append([current_row_index, current_column_index])\n",
    "    \n",
    "    pipes_used = 0\n",
    "    while not is_terminal_state(current_row_index, current_column_index):\n",
    "        \n",
    "        #get the best action to take\n",
    "        action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "                \n",
    "        #move to the next location on the path, and add the new location to the list\n",
    "        current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "        \n",
    "#         if ([current_row_index, current_column_index] in trajectory)|((current_row_index == 0)):\n",
    "#             break\n",
    "            \n",
    "        trajectory.append([current_row_index, current_column_index])\n",
    "        pipes_used += 1\n",
    "        \n",
    "        if pipes_used == available_pipe:\n",
    "            break\n",
    "        \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c805c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_row = 0\n",
    "start_column = 23\n",
    "available_pipe = 25\n",
    "\n",
    "trajectory = []\n",
    "trajectory = get_shortest_path(start_row, start_column,available_pipe)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(rewards, vmin=-100, vmax=100)\n",
    "\n",
    "for i in range(0,len(trajectory)):\n",
    "    traj_z, traj_x = np.asarray(trajectory).T\n",
    "    plt.plot(traj_x, traj_z, \"-\", linewidth=6, color = 'k')\n",
    "\n",
    "plt.xticks(np.arange(0, 80, 1.0))\n",
    "plt.yticks(np.arange(0, 40, 1.0))\n",
    "plt.xlim([-0.5, 79.5])\n",
    "plt.ylim([39.5, -0.5])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780fc770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "\n",
    "actions = ['up', 'right', 'down', 'left']get_valid_actions(2, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values[2, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef821c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c28637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abcfcf4f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bef83",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 1\n",
    "\n",
    "actions = {\n",
    "           0: [1, 0],  # down\n",
    "           1: [0, -1],  # left\n",
    "           2: [0, 1],  # right\n",
    "           3: [-1, 0],  # up\n",
    "          }\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    \n",
    "    print(\"Beginning Drill Campaign:\", episode)\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "#         print(f\"    Action: {actions[action]}\")\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Total Reward: {reward}\")\n",
    "#         print(f\"    done: {done}\\n\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273352b",
   "metadata": {},
   "source": [
    "# Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb3de7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4e748",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "env = RewardDriller(env_config)\n",
    "# env = MultiDriller(env_config)\n",
    "\n",
    "\n",
    "ppo = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "ppo.learn(total_timesteps = 800_000, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ac943",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = MultiDriller(env_config)\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = ppo.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ca4c3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad433c1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c4fc7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "dqn = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "dqn.learn(total_timesteps=500_000, log_interval=1_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b03e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = dqn.predict(state, deterministic=True)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a182590",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31eaef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc33143",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68430c-913a-422d-a19b-8a284d7bc5f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=100, num_wells = 3, delim=\",\")\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "a2c = A2C(\"MlpPolicy\", env, verbose=3)\n",
    "a2c.learn(total_timesteps=500_000, log_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263efa6f",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 100\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = a2c.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4d162",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
