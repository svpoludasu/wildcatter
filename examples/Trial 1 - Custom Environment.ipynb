{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "811ed8a0-f8f4-4f04-9b87-6d18c9d550d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box\n",
    "from gym.spaces import Discrete\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76690cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679268d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Task List\n",
    "- ~~Drill multiple wells, one after the other and not to update the environment after every simulation.~~\n",
    "- ~~Make sure well/wells dont crash into each other/itself or any faults/artifacts~~\n",
    "- ~~Avoid 180 degree turns~~\n",
    "- Have a target zone where the well eventually want to make it to and get higher reward\n",
    "- Use a metric like MSE/UCS to get an estimate on the amount of energy required to drill and optimizing it to have lowest energy usage (also tie in the economic constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822d739",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Action Space\n",
    "- Surface Location ?? Pick it randomly or intentionally?\n",
    "- Number of wells to drill\n",
    "- Bit Movement\n",
    "    -  Up\n",
    "    -  Down\n",
    "    -  Left\n",
    "    -  Right\n",
    "    -  Angle ?? If the grid size is as much as a stand then the max angle should be around 3 degrees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e214e4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Observation Space\n",
    "\n",
    "Same shape [matrix] as the input. Ideally 30 ft by 30 ft to match with the drilling pipe (90 ft by 90 ft for stand). Bool with true for wherever well is located."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459fbd0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Possible Rewards\n",
    "- While Drilling\n",
    "    -  Proximity to Reservoir (based on the percentage of Normalized TOC?) - *Positive Reward*\n",
    "    -  Proximity to Fault - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to itself or other wells - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to the possible depletion zone of an existing well - *VERY HIGH Negative Reward*\n",
    "    -  Remaining oil in the zone of the well - *High Positive Reward*\n",
    "\n",
    "- After Drilling\n",
    "    -  Total UCS/MSE it was drilled through - *Negative Reward based on the UCS total, can also relate it to a USD amount*    \n",
    "    -  Total Well Length - *Negative Reward based on the pipe count, can also relate it to a USD amount* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b0949",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simple Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ace1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SimpleDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        self.model = np.loadtxt(\n",
    "            env_config[\"model_path\"],\n",
    "            delimiter=env_config[\"delim\"],\n",
    "        )\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=1, shape=(self.nrow, self.ncol), dtype=\"bool\"\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def step(  # noqa: C901\n",
    "        self, action: int\n",
    "    ) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        done = False\n",
    "        actions = {\n",
    "            0: [1, 0],  # down\n",
    "            1: [0, -1],  # left\n",
    "            2: [0, 1],  # right\n",
    "            3: [-1, 0],  # up\n",
    "        }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        else:\n",
    "            reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            reward = 0\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        info: dict[str, Any] = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"\n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "\n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.surface_hole_location = [1, random.randint(0, self.ncol - 1)]  # noqa: S311\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.bit_location = self.surface_hole_location\n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e44cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Multidriller Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae97ad3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MultiDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment for multiple wells\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         reward = 0\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        else:\n",
    "            self.reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "#             reward = 0\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "#                 done = True\n",
    "                self.reward = -100  \n",
    "#                 print('    Immediate 180 degree turn')\n",
    "    \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            self.multi_reward += self.reward \n",
    "            \n",
    "            if len(self.trajectory)>0:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "            self.reset_well()\n",
    "            \n",
    "            if self.wells_drilled < self.num_wells:\n",
    "                    done = False            \n",
    "                    \n",
    "            return self.state, self.multi_reward, done, info\n",
    "        else:\n",
    "            self.last_action = action\n",
    "#             print(f'Last action: {actions[self.last_action]}')\n",
    "            return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "\n",
    "        # Log the surface locations already used\n",
    "        self.surface_location.append(self.surface_hole_location[1])\n",
    "        \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fe87d",
   "metadata": {},
   "source": [
    "# Reward based on Proximity Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470bc23",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb22f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "            # Reward from the model\n",
    "            self.reward = (self.model[newrow, newcol] * 2)\n",
    "            \n",
    "            # Checking if the reward from the model is negative and stopping the well\n",
    "            if self.reward < 0:\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:                \n",
    "                # Giving a small reward to encourage the agent to use pipes     \n",
    "                self.reward += -self.pipe_used/10\n",
    "                                \n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        # Avoid going along the surface\n",
    "        if ((self.bit_location != self.surface_hole_location) &\n",
    "                (self.bit_location[0] == 0)):\n",
    "            self.reward = -10\n",
    "            done = True\n",
    "#             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -10  \n",
    "#                 done = True\n",
    "#                 print('    Immediate 180 degree turn')\n",
    "\n",
    "        if self.reward > 0:\n",
    "            self.multi_reward += self.reward   \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                self.reset_well()\n",
    "                \n",
    "                if len(self.multi_trajectory) < self.num_wells:\n",
    "#                     print(\"MULTIREWARD\")\n",
    "                    return self.state, self.multi_reward, done, info  \n",
    "                \n",
    "            else:\n",
    "                self.reset_well()\n",
    "                self.reward = - 10            \n",
    "            \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"MULTIREWARD\")\n",
    "                \n",
    "                return self.state, self.multi_reward, done, info\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "                \n",
    "#             return self.state, self.reward, done, info\n",
    "        \n",
    "        else:\n",
    "            self.last_action = action\n",
    "        \n",
    "#         print(\"REWARD\")\n",
    "            \n",
    "        return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715cecb",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model between o-10\n",
    "        self.model = self.model*(100/self.model.max())\n",
    "\n",
    "        self.model[np.less(self.model,0)] = -100\n",
    "        self.model[self.model == 0] = 1\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:               \n",
    "                \n",
    "            # Incremental Reward from the model\n",
    "#             self.reward = sum([self.model[x,y]*2 for x,y in self.trajectory[1:]])\n",
    "            \n",
    "            model_reward = (self.model[newrow, newcol])\n",
    "            \n",
    "            # Checking if the incremental reward from the model is negative and stopping the well\n",
    "            if model_reward < 0:\n",
    "                done = True\n",
    "                self.reward = -100\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:\n",
    "                # Giving a small -ve reward to encourage the agent to use less pipes     \n",
    "                self.reward += (model_reward - self.pipe_used)\n",
    "#                 print(f'Model Reward: {self.reward}')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#         # Avoid going along the surface\n",
    "#         if ((self.bit_location != self.surface_hole_location) &\n",
    "#                 (self.bit_location[0] == 0)):\n",
    "#             self.reward += -100\n",
    "#             done = True\n",
    "# #             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -100\n",
    "                done = True\n",
    "#                 print('    Immediate 180 degree turn')  \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "#         print(done)\n",
    "        if done:\n",
    "            self.wells_drilled += 1  \n",
    "#             print('Well Done')\n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                \n",
    "                # Update state\n",
    "                self.update_state()\n",
    "                \n",
    "                if self.reward > 0:\n",
    "                    self.multi_reward += self.reward\n",
    "                else:\n",
    "                    self.multi_reward = -100\n",
    "                \n",
    "            else:\n",
    "                self.multi_reward = -100   \n",
    "                       \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"FINAL REWARD\")\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.multi_reward = -100                \n",
    "            \n",
    "            self.reset_well()\n",
    "            \n",
    "        else:\n",
    "            self.last_action = action\n",
    "            self.multi_reward += self.reward\n",
    "            \n",
    "#         print(self.reward)\n",
    "             \n",
    "        return self.state, self.multi_reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d51445",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb91f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model between o-10\n",
    "        self.model = self.model*(100/self.model.max())\n",
    "\n",
    "        self.model[np.less(self.model,0)] = -100\n",
    "        self.model[self.model == 0] = 1\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:               \n",
    "                \n",
    "            # Incremental Reward from the model\n",
    "#             self.reward = sum([self.model[x,y]*2 for x,y in self.trajectory[1:]])\n",
    "            \n",
    "            model_reward = (self.model[newrow, newcol])\n",
    "            \n",
    "            # Checking if the incremental reward from the model is negative and stopping the well\n",
    "            if model_reward < 0:\n",
    "                done = True\n",
    "                self.reward = -100\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:\n",
    "                # Giving a small -ve reward to encourage the agent to use less pipes     \n",
    "                self.reward += (model_reward - self.pipe_used)\n",
    "#                 print(f'Model Reward: {self.reward}')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#         # Avoid going along the surface\n",
    "#         if ((self.bit_location != self.surface_hole_location) &\n",
    "#                 (self.bit_location[0] == 0)):\n",
    "#             self.reward += -100\n",
    "#             done = True\n",
    "# #             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -100\n",
    "                done = True\n",
    "#                 print('    Immediate 180 degree turn')  \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "#         print(done)\n",
    "        if done:\n",
    "            self.wells_drilled += 1  \n",
    "#             print('Well Done')\n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                \n",
    "                # Update state\n",
    "                self.update_state()\n",
    "                \n",
    "                if self.reward > 0:\n",
    "                    self.multi_reward += self.reward\n",
    "                else:\n",
    "                    self.multi_reward = -100\n",
    "                \n",
    "            else:\n",
    "                self.multi_reward = -100   \n",
    "                       \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"FINAL REWARD\")\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.multi_reward = -100                \n",
    "            \n",
    "            self.reset_well()\n",
    "            \n",
    "        else:\n",
    "            self.last_action = action\n",
    "            self.multi_reward += self.reward\n",
    "            \n",
    "#         print(self.reward)\n",
    "             \n",
    "        return self.state, self.multi_reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf5147",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Horizontal well Driller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0bd45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Horizontal well driller with a specific start point\n",
    "\n",
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, PReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD , Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63eb89b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "model = np.loadtxt(env_config[\"model_path\"],\n",
    "                   delimiter=env_config[\"delim\"])\n",
    "\n",
    "model[np.less(model,0)] = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859cafe2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the bit will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55f091",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847f4d8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f856b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfcb79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8248fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(model)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd1e34",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b03a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55d2dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167252ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        \n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            \n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5f097",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52e20c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583d0f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b3820",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f65dfc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9890c57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d18b24",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdfadbe1",
   "metadata": {},
   "source": [
    "# Simple  Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f8b73063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDriller:  # type: ignore\n",
    "    \"\"\"Driller environment for horizontal wells with self.rewards based on Q learning\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "\n",
    "        self.rewards = np.loadtxt(env_config[\"model_path\"],\n",
    "                                  delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "\n",
    "        # Normalizing the model\n",
    "        self.rewards = self.rewards * (100 / self.rewards.max())\n",
    "\n",
    "        self.rewards[np.less(self.rewards, 0)] = -100\n",
    "        self.rewards[self.rewards == 0] = -1\n",
    "\n",
    "        self.actions = ['up', 'right', 'down', 'left']\n",
    "\n",
    "        self.q_values = np.zeros((self.rewards.shape[0],\n",
    "                                  self.rewards.shape[1],\n",
    "                                  len(self.actions)))\n",
    "\n",
    "        self.trajectory = []        \n",
    "        self.end = 0\n",
    "        \n",
    "        self.action_cache = np.nan \n",
    "        \n",
    "        self.explored = np.zeros((self.rewards.shape[0],\n",
    "                                  self.rewards.shape[1]))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define a function that determines if the specified location is a terminal state\n",
    "    def is_terminal_state(self, current_row_index, current_column_index):\n",
    "        if ((len(self.trajectory) > 1) &\n",
    "                (self.rewards[current_row_index, current_column_index] == -100)):\n",
    "            self.end = 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    # define a function that will choose a random, non-terminal starting location\n",
    "    def get_starting_location(self):\n",
    "        # get a random column index\n",
    "        current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "        current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "        return current_row_index, current_column_index\n",
    "#         return 18, 18\n",
    "\n",
    "#     def get_unique_starting_location(self):\n",
    "#         # get a random column index\n",
    "#         current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "#         current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "#         return current_row_index, current_column_index\n",
    "# #         return 18, 0\n",
    "    \n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "    # define a function that will decide the valid actions to avoid crashing into itself\n",
    "    def get_valid_actions(self, current_row_index, current_column_index):\n",
    "        va = [0, 1, 2, 3]\n",
    "        try:\n",
    "            # Avoid turning back into itself\n",
    "            if [current_row_index - 1, current_column_index] in self.trajectory:\n",
    "                va.remove(0)\n",
    "            if [current_row_index, current_column_index + 1] in self.trajectory:\n",
    "                va.remove(1)\n",
    "            if [current_row_index + 1, current_column_index] in self.trajectory:\n",
    "                va.remove(2)\n",
    "            if [current_row_index, current_column_index - 1] in self.trajectory:\n",
    "                va.remove(3)\n",
    "\n",
    "            # Remove left move if it is the first column\n",
    "            if current_column_index == 0:\n",
    "                va.remove(3)\n",
    "\n",
    "#             # Remove up move if it is the first row\n",
    "#             if current_row_index == 0:\n",
    "#                 va.remove(0)\n",
    "                \n",
    "            # Force to move down when at surface\n",
    "            if current_row_index == 0:\n",
    "                return [2]\n",
    "\n",
    "\n",
    "            # Remove right move if it is the last column\n",
    "            if current_column_index == (self.rewards.shape[1]-1):\n",
    "                va.remove(1)\n",
    "\n",
    "            # Remove down move if it is the last row\n",
    "            if current_row_index == (self.rewards.shape[0]-1):\n",
    "                va.remove(2)\n",
    "                \n",
    "            # Avoid going up if is gonna hit the surface\n",
    "            if (current_row_index - 1) == 0:\n",
    "                va.remove(0)\n",
    "            \n",
    "#             # Avoid wellbore looping\n",
    "#             if self.action_cache.notna():\n",
    "#                 va.remove(self.action_cache)\n",
    "\n",
    "        except:\n",
    "#             self.end = 1\n",
    "            pass\n",
    "            \n",
    "        return va\n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "    def get_next_action(self, current_row_index, current_column_index, epsilon):\n",
    "        \n",
    "        valid_actions = self.get_valid_actions(current_row_index, current_column_index)\n",
    "        \n",
    "        if len(valid_actions) == 0:\n",
    "            self.end = 1\n",
    "            \n",
    "        if (len(valid_actions) != 0) & (np.random.random() < epsilon):\n",
    "            action = max(valid_actions,key = lambda i: self.q_values[current_row_index, current_column_index].tolist()[i])\n",
    "#             print(f'Valid Actions: {valid_actions}, Picked Action: {action}')\n",
    "            return action\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def get_next_action_train(self, current_row_index, current_column_index, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.argmax(self.q_values[current_row_index, current_column_index])\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define a function that will get the next location based on the chosen action\n",
    "    def get_next_location(self, current_row_index, current_column_index, action_index):\n",
    "\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        \n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.rewards.shape[1] - 1:\n",
    "            new_column_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.rewards.shape[0] - 1:\n",
    "            new_row_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "        else:\n",
    "            self.end = 1\n",
    "\n",
    "        return new_row_index, new_column_index\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function to train and populate the q table\n",
    "    def populate_q_table(self, num_episodes, epsilon = 0.1, discount_factor = 0.9, learning_rate = 0.9):\n",
    "        print('Training Started!')\n",
    "        for episode in range(num_episodes):\n",
    "            self.reset()\n",
    "\n",
    "            # get the starting location for this episode\n",
    "            row_index, column_index = self.get_starting_location()\n",
    "#             print(row_index, column_index)\n",
    "\n",
    "            self.trajectory.append([row_index, column_index])\n",
    "#             print(self.trajectory)\n",
    "            \n",
    "#             print(self.rewards[row_index, column_index])\n",
    "            \n",
    "#             print(self.is_terminal_state(row_index, column_index))\n",
    "\n",
    "            # continue taking actions (i.e., moving) until we reach a terminal state\n",
    "            while not (self.is_terminal_state(row_index, column_index) | (self.end == 1)):\n",
    "\n",
    "                # choose which action to take (i.e., where to move next)\n",
    "                action_index = self.get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "                # perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "                old_row_index, old_column_index = row_index, column_index  # store the old row and column indexes\n",
    "                row_index, column_index = self.get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "                # receive the reward for moving to the new state\n",
    "#                 if ([row_index, column_index] in self.trajectory):\n",
    "#                     reward = -100\n",
    "#                 elif (row_index == 0):\n",
    "#                     reward = -100\n",
    "#                 else:\n",
    "#                     reward = self.rewards[row_index, column_index] - len(self.trajectory)\n",
    "#                 #         print(reward)\n",
    "\n",
    "                if [row_index, column_index] in self.trajectory:\n",
    "                    self.end == 1\n",
    "                    reward += -100\n",
    "                    break\n",
    "                else:\n",
    "                    self.trajectory.append([row_index, column_index])\n",
    "                    # From Model\n",
    "                    reward = self.rewards[row_index, column_index]*15\n",
    "\n",
    "                    # To encourage to maintain the shortest path\n",
    "                    reward += -len(self.trajectory)*25\n",
    "\n",
    "                    # To ensure that a horizontal well is drilled\n",
    "                    reward += abs(self.trajectory[-1][1] - self.trajectory[0][1])*10\n",
    "#                     print(reward)\n",
    "\n",
    "#                     # To make sure target pipes are used\n",
    "#                     reward += (self.available_pipe -len(self.trajectory))*2\n",
    "                    \n",
    "                    # Adding a -ve reward to encourage the agent to visit unique rows,columns\n",
    "                    rows = [i[0] for i in self.trajectory]\n",
    "                    columns = [i[1] for i in self.trajectory]\n",
    "\n",
    "                    reward += -(len(rows) - len(set(rows)))*10\n",
    "                    reward += -(len(columns) - len(set(columns)))*20\n",
    "                    \n",
    "#                     # Add a -ve reward to identify simultaneous right/left turns in the to avoid wellbore tornado effect\n",
    "#                     if (action_index == self.action_cache):\n",
    "#                         reward += -100\n",
    "                    \n",
    "                if (action_index == 1) | (action_index == 3):\n",
    "                    self.action_cache = action_index                    \n",
    "\n",
    "                old_q_value = self.q_values[old_row_index, old_column_index, action_index]\n",
    "\n",
    "                temporal_difference = reward + (\n",
    "                            discount_factor * np.max(self.q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "                # update the Q-value for the previous state and action pair\n",
    "                new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "#                 print(new_q_value)\n",
    "\n",
    "                self.q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "                self.explored[old_row_index, old_column_index] = 1\n",
    "    \n",
    "    \n",
    "            if (episode != 0) & ((episode + 1) % 100_000 == 0):\n",
    "                print(f'    {\"{:,}\".format(episode + 1)} episodes completed')\n",
    "\n",
    "#             print(self.trajectory)\n",
    "        \n",
    "        print('Training Complete!')\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function that will get the shortest path\n",
    "    def get_shortest_path(self, start_row_index, start_column_index):\n",
    "        self.reset()\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        self.trajectory.append([current_row_index, current_column_index])\n",
    "\n",
    "        pipes_used = 0\n",
    "#         print(self.is_terminal_state(current_row_index, current_column_index))\n",
    "        \n",
    "        while not (self.is_terminal_state(current_row_index, current_column_index) | (self.end == 1)):\n",
    "#             print(self.trajectory)\n",
    "            # get the best action to take\n",
    "            action_index = self.get_next_action(current_row_index, current_column_index, 1.)\n",
    "#             print(current_row_index, current_column_index)\n",
    "#             print(self.actions[action_index])\n",
    "            # move to the next location on the path, and add the new location to the list\n",
    "            current_row_index, current_column_index = self.get_next_location(current_row_index, current_column_index,\n",
    "                                                                        action_index)\n",
    "#             print(f'{current_row_index}, {current_column_index}\\n')\n",
    "\n",
    "            \n",
    "            pipes_used += 1\n",
    "\n",
    "            if (pipes_used == self.available_pipe):\n",
    "                self.end = 1\n",
    "                print('Pipes Over')\n",
    "                \n",
    "            if ([current_row_index, current_column_index] in self.trajectory):\n",
    "                self.end = 1\n",
    "                print(f'Index in trajectory - [{current_row_index},{current_column_index}]')\n",
    "                \n",
    "            else:\n",
    "                self.trajectory.append([current_row_index, current_column_index])\n",
    "#                 print(self.trajectory)\n",
    "\n",
    "        return self.trajectory\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function that will reset everything\n",
    "    def reset(self):\n",
    "        self.trajectory = []        \n",
    "        self.end = 0\n",
    "        self.action_cache = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3bab4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 1, delim=\",\")\n",
    "env = QDriller(env_config)\n",
    "\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "66edc82a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started!\n",
      "    100,000 episodes completed\n",
      "    200,000 episodes completed\n",
      "    300,000 episodes completed\n",
      "    400,000 episodes completed\n",
      "    500,000 episodes completed\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "env.populate_q_table(500_000)\n",
    "\n",
    "# plt.figure(figsize=(15, 7))\n",
    "# plt.imshow(env.explored, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fbae4d49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index in trajectory - [16,58]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_row = 0\n",
    "start_column = 50\n",
    "\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "\n",
    "trajectory = env.get_shortest_path(start_row, start_column)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.rewards, vmin=-100, vmax=100)\n",
    "\n",
    "for i in range(0,len(trajectory)):\n",
    "    traj_z, traj_x = np.asarray(trajectory).T\n",
    "    plt.plot(traj_x, traj_z, \"-\", linewidth=6, color = 'k')\n",
    "\n",
    "plt.xticks(np.arange(0, 80, 1.0))\n",
    "plt.yticks(np.arange(0, 40, 1.0))\n",
    "plt.xlim([-0.5, 79.5])\n",
    "plt.ylim([39.5, -0.5])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a604cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78987a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be3f6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c203c466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d696c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82244f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d970039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abcfcf4f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bef83",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 1\n",
    "\n",
    "actions = {\n",
    "           0: [1, 0],  # down\n",
    "           1: [0, -1],  # left\n",
    "           2: [0, 1],  # right\n",
    "           3: [-1, 0],  # up\n",
    "          }\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    \n",
    "    print(\"Beginning Drill Campaign:\", episode)\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "#         print(f\"    Action: {actions[action]}\")\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Total Reward: {reward}\")\n",
    "#         print(f\"    done: {done}\\n\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273352b",
   "metadata": {},
   "source": [
    "# Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb3de7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4e748",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "env = RewardDriller(env_config)\n",
    "# env = MultiDriller(env_config)\n",
    "\n",
    "\n",
    "ppo = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "ppo.learn(total_timesteps = 800_000, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ac943",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = MultiDriller(env_config)\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = ppo.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a2b3d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad433c1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c4fc7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "dqn = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "dqn.learn(total_timesteps=500_000, log_interval=1_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b03e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = dqn.predict(state, deterministic=True)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a182590",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31eaef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc33143",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68430c-913a-422d-a19b-8a284d7bc5f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=100, num_wells = 3, delim=\",\")\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "a2c = A2C(\"MlpPolicy\", env, verbose=3)\n",
    "a2c.learn(total_timesteps=500_000, log_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263efa6f",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 100\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = a2c.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4d162",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
