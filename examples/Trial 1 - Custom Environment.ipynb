{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811ed8a0-f8f4-4f04-9b87-6d18c9d550d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box\n",
    "from gym.spaces import Discrete\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76690cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679268d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Task List\n",
    "- ~~Drill multiple wells, one after the other and not to update the environment after every simulation.~~\n",
    "- ~~Make sure well/wells dont crash into each other/itself or any faults/artifacts~~\n",
    "- ~~Avoid 180 degree turns~~\n",
    "- Have a target zone where the well eventually want to make it to and get higher reward\n",
    "- Use a metric like MSE/UCS to get an estimate on the amount of energy required to drill and optimizing it to have lowest energy usage (also tie in the economic constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822d739",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Action Space\n",
    "- Surface Location ?? Pick it randomly or intentionally?\n",
    "- Number of wells to drill\n",
    "- Bit Movement\n",
    "    -  Up\n",
    "    -  Down\n",
    "    -  Left\n",
    "    -  Right\n",
    "    -  Angle ?? If the grid size is as much as a stand then the max angle should be around 3 degrees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e214e4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Observation Space\n",
    "\n",
    "Same shape [matrix] as the input. Ideally 30 ft by 30 ft to match with the drilling pipe (90 ft by 90 ft for stand). Bool with true for wherever well is located."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459fbd0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Possible Rewards\n",
    "- While Drilling\n",
    "    -  Proximity to Reservoir (based on the percentage of Normalized TOC?) - *Positive Reward*\n",
    "    -  Proximity to Fault - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to itself or other wells - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to the possible depletion zone of an existing well - *VERY HIGH Negative Reward*\n",
    "    -  Remaining oil in the zone of the well - *High Positive Reward*\n",
    "\n",
    "- After Drilling\n",
    "    -  Total UCS/MSE it was drilled through - *Negative Reward based on the UCS total, can also relate it to a USD amount*    \n",
    "    -  Total Well Length - *Negative Reward based on the pipe count, can also relate it to a USD amount* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b0949",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simple Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ace1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SimpleDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        self.model = np.loadtxt(\n",
    "            env_config[\"model_path\"],\n",
    "            delimiter=env_config[\"delim\"],\n",
    "        )\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=1, shape=(self.nrow, self.ncol), dtype=\"bool\"\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def step(  # noqa: C901\n",
    "        self, action: int\n",
    "    ) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        done = False\n",
    "        actions = {\n",
    "            0: [1, 0],  # down\n",
    "            1: [0, -1],  # left\n",
    "            2: [0, 1],  # right\n",
    "            3: [-1, 0],  # up\n",
    "        }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        else:\n",
    "            reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            reward = 0\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        info: dict[str, Any] = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"\n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "\n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.surface_hole_location = [1, random.randint(0, self.ncol - 1)]  # noqa: S311\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.bit_location = self.surface_hole_location\n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e44cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Multidriller Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae97ad3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MultiDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment for multiple wells\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         reward = 0\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        else:\n",
    "            self.reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "#             reward = 0\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "#                 done = True\n",
    "                self.reward = -100  \n",
    "#                 print('    Immediate 180 degree turn')\n",
    "    \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            self.multi_reward += self.reward \n",
    "            \n",
    "            if len(self.trajectory)>0:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "            self.reset_well()\n",
    "            \n",
    "            if self.wells_drilled < self.num_wells:\n",
    "                    done = False            \n",
    "                    \n",
    "            return self.state, self.multi_reward, done, info\n",
    "        else:\n",
    "            self.last_action = action\n",
    "#             print(f'Last action: {actions[self.last_action]}')\n",
    "            return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "\n",
    "        # Log the surface locations already used\n",
    "        self.surface_location.append(self.surface_hole_location[1])\n",
    "        \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fe87d",
   "metadata": {},
   "source": [
    "# Reward based on Proximity Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470bc23",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb22f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "            # Reward from the model\n",
    "            self.reward = (self.model[newrow, newcol] * 2)\n",
    "            \n",
    "            # Checking if the reward from the model is negative and stopping the well\n",
    "            if self.reward < 0:\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:                \n",
    "                # Giving a small reward to encourage the agent to use pipes     \n",
    "                self.reward += -self.pipe_used/10\n",
    "                                \n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        # Avoid going along the surface\n",
    "        if ((self.bit_location != self.surface_hole_location) &\n",
    "                (self.bit_location[0] == 0)):\n",
    "            self.reward = -10\n",
    "            done = True\n",
    "#             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -10  \n",
    "#                 done = True\n",
    "#                 print('    Immediate 180 degree turn')\n",
    "\n",
    "        if self.reward > 0:\n",
    "            self.multi_reward += self.reward   \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                self.reset_well()\n",
    "                \n",
    "                if len(self.multi_trajectory) < self.num_wells:\n",
    "#                     print(\"MULTIREWARD\")\n",
    "                    return self.state, self.multi_reward, done, info  \n",
    "                \n",
    "            else:\n",
    "                self.reset_well()\n",
    "                self.reward = - 10            \n",
    "            \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"MULTIREWARD\")\n",
    "                \n",
    "                return self.state, self.multi_reward, done, info\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "                \n",
    "#             return self.state, self.reward, done, info\n",
    "        \n",
    "        else:\n",
    "            self.last_action = action\n",
    "        \n",
    "#         print(\"REWARD\")\n",
    "            \n",
    "        return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715cecb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9ab6b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model between o-10\n",
    "        self.model = self.model*(100/self.model.max())\n",
    "\n",
    "        self.model[np.less(self.model,0)] = -100\n",
    "        self.model[self.model == 0] = 1\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:               \n",
    "                \n",
    "            # Incremental Reward from the model\n",
    "#             self.reward = sum([self.model[x,y]*2 for x,y in self.trajectory[1:]])\n",
    "            \n",
    "            model_reward = (self.model[newrow, newcol])\n",
    "            \n",
    "            # Checking if the incremental reward from the model is negative and stopping the well\n",
    "            if model_reward < 0:\n",
    "                done = True\n",
    "                self.reward = -100\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:\n",
    "                # Giving a small -ve reward to encourage the agent to use less pipes     \n",
    "                self.reward += (model_reward - self.pipe_used)\n",
    "#                 print(f'Model Reward: {self.reward}')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#         # Avoid going along the surface\n",
    "#         if ((self.bit_location != self.surface_hole_location) &\n",
    "#                 (self.bit_location[0] == 0)):\n",
    "#             self.reward += -100\n",
    "#             done = True\n",
    "# #             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -100\n",
    "                done = True\n",
    "#                 print('    Immediate 180 degree turn')  \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "#         print(done)\n",
    "        if done:\n",
    "            self.wells_drilled += 1  \n",
    "#             print('Well Done')\n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                \n",
    "                # Update state\n",
    "                self.update_state()\n",
    "                \n",
    "                if self.reward > 0:\n",
    "                    self.multi_reward += self.reward\n",
    "                else:\n",
    "                    self.multi_reward = -100\n",
    "                \n",
    "            else:\n",
    "                self.multi_reward = -100   \n",
    "                       \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"FINAL REWARD\")\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.multi_reward = -100                \n",
    "            \n",
    "            self.reset_well()\n",
    "            \n",
    "        else:\n",
    "            self.last_action = action\n",
    "            self.multi_reward += self.reward\n",
    "            \n",
    "#         print(self.reward)\n",
    "             \n",
    "        return self.state, self.multi_reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf5147",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Horizontal well Driller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0bd45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Horizontal well driller with a specific start point\n",
    "\n",
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, PReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD , Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63eb89b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "model = np.loadtxt(env_config[\"model_path\"],\n",
    "                   delimiter=env_config[\"delim\"])\n",
    "\n",
    "model[np.less(model,0)] = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859cafe2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the bit will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55f091",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847f4d8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f856b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfcb79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8248fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(model)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd1e34",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b03a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55d2dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167252ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        \n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            \n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5f097",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52e20c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583d0f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b3820",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f65dfc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9890c57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d18b24",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdfadbe1",
   "metadata": {},
   "source": [
    "# Simple  Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc9e25",
   "metadata": {},
   "source": [
    "## Class Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b73063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDriller:  # type: ignore\n",
    "    \"\"\"Driller environment for horizontal wells with self.rewards based on Q learning\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "\n",
    "        self.rewards = np.loadtxt(env_config[\"model_path\"],\n",
    "                                  delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model\n",
    "        self.rewards = self.rewards * (100 / self.rewards.max())\n",
    "\n",
    "        self.rewards[np.less(self.rewards, 0)] = -100\n",
    "        self.rewards[self.rewards == 0] = -1\n",
    "\n",
    "        self.actions = ['up', 'right', 'down', 'left']\n",
    "\n",
    "        self.q_values = np.zeros((self.rewards.shape[0],\n",
    "                                  self.rewards.shape[1],\n",
    "                                  len(self.actions)))\n",
    "\n",
    "        self.trajectory = []        \n",
    "        self.end = 0\n",
    "        \n",
    "        self.explored = np.zeros((self.rewards.shape[0],\n",
    "                                  self.rewards.shape[1]))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define a function that determines if the specified location is a terminal state\n",
    "    def is_terminal_state(self, current_row_index, current_column_index):\n",
    "        if ((len(self.trajectory) > 1) &\n",
    "                (self.rewards[current_row_index, current_column_index] == -100)):\n",
    "            self.end = 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    # define a function that will choose a random, non-terminal starting location\n",
    "    def get_starting_location(self):\n",
    "        # get a random column index\n",
    "        current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "        current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "        return current_row_index, current_column_index\n",
    "#         return 18, 18\n",
    "\n",
    "#     def get_unique_starting_location(self):\n",
    "#         # get a random column index\n",
    "#         current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "#         current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "#         return current_row_index, current_column_index\n",
    "# #         return 18, 0\n",
    "    \n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "    # define a function that will decide the valid actions to avoid crashing into itself\n",
    "    def get_valid_actions(self, current_row_index, current_column_index):\n",
    "        va = [0, 1, 2, 3]\n",
    "        try:\n",
    "            \n",
    "            if [current_row_index - 1, current_column_index] in self.trajectory:\n",
    "                va.remove(0)\n",
    "            if [current_row_index, current_column_index + 1] in self.trajectory:\n",
    "                va.remove(1)\n",
    "            if [current_row_index + 1, current_column_index] in self.trajectory:\n",
    "                va.remove(2)\n",
    "            if [current_row_index, current_column_index - 1] in self.trajectory:\n",
    "                va.remove(3)\n",
    "\n",
    "            # Remove left move if it is the first column\n",
    "            if current_column_index == 0:\n",
    "                va.remove(3)\n",
    "\n",
    "            # Remove up move if it is the first row\n",
    "            if current_row_index == 0:\n",
    "                va.remove(0)\n",
    "                \n",
    "            # Force to move down when at surface\n",
    "            if current_row_index == 0:\n",
    "                return [2]\n",
    "\n",
    "\n",
    "            # Remove right move if it is the last column\n",
    "            if current_column_index == (self.rewards.shape[0]-1):\n",
    "                va.remove(1)\n",
    "\n",
    "            # Remove down move if it is the last row\n",
    "            if current_row_index == (self.rewards.shape[1]-1):\n",
    "                va.remove(2)\n",
    "                \n",
    "            # Avoid going up if is gonna hit the surface\n",
    "            if (current_row_index - 1) == 0:\n",
    "                va.remove(0)                \n",
    "\n",
    "        except:\n",
    "#             self.end = 1\n",
    "            pass\n",
    "            \n",
    "        return va\n",
    "    \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "    def get_next_action(self, current_row_index, current_column_index, epsilon):\n",
    "        \n",
    "        valid_actions = self.get_valid_actions(current_row_index, current_column_index)\n",
    "        \n",
    "        if len(valid_actions) == 0:\n",
    "            self.end = 1\n",
    "            \n",
    "        if (len(valid_actions) != 0) & (np.random.random() < epsilon):\n",
    "            action = max(valid_actions,key = lambda i: self.q_values[current_row_index, current_column_index].tolist()[i])\n",
    "#             print(f'Valid Actions: {valid_actions}, Picked Action: {action}')\n",
    "            return action\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def get_next_action_train(self, current_row_index, current_column_index, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.argmax(self.q_values[current_row_index, current_column_index])\n",
    "        else:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # define a function that will get the next location based on the chosen action\n",
    "    def get_next_location(self, current_row_index, current_column_index, action_index):\n",
    "\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.rewards.shape[1] - 1:\n",
    "            new_column_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.rewards.shape[0] - 1:\n",
    "            new_row_index += 1\n",
    "\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "\n",
    "        return new_row_index, new_column_index\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function to train and populate the q table\n",
    "    def populate_q_table(self, num_episodes, epsilon = 0.1, discount_factor = 0.9, learning_rate = 0.9):\n",
    "        print('Training Started!')\n",
    "        for episode in range(num_episodes):\n",
    "            self.reset()\n",
    "\n",
    "            # get the starting location for this episode\n",
    "            row_index, column_index = self.get_starting_location()\n",
    "#             print(row_index, column_index)\n",
    "\n",
    "            self.trajectory.append([row_index, column_index])\n",
    "#             print(self.trajectory)\n",
    "            \n",
    "#             print(self.rewards[row_index, column_index])\n",
    "            \n",
    "#             print(self.is_terminal_state(row_index, column_index, self.trajectory))\n",
    "\n",
    "            # continue taking actions (i.e., moving) until we reach a terminal state\n",
    "            while not (self.is_terminal_state(row_index, column_index) | (self.end == 1)):\n",
    "\n",
    "                # choose which action to take (i.e., where to move next)\n",
    "                action_index = self.get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "                # perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "                old_row_index, old_column_index = row_index, column_index  # store the old row and column indexes\n",
    "                row_index, column_index = self.get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "                # receive the reward for moving to the new state\n",
    "                if ([row_index, column_index] in self.trajectory):\n",
    "                    reward = -100\n",
    "                elif (row_index == 0):\n",
    "                    reward = -100\n",
    "                else:\n",
    "                    reward = self.rewards[row_index, column_index] - len(self.trajectory)\n",
    "                #         print(reward)\n",
    "\n",
    "                self.trajectory.append([row_index, column_index])\n",
    "\n",
    "                old_q_value = self.q_values[old_row_index, old_column_index, action_index]\n",
    "\n",
    "                temporal_difference = reward + (\n",
    "                            discount_factor * np.max(self.q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "                # update the Q-value for the previous state and action pair\n",
    "                new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "#                 print(new_q_value)\n",
    "\n",
    "                self.q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "                self.explored[old_row_index, old_column_index] = 1\n",
    "    \n",
    "    \n",
    "            if (episode != 0) & ((episode + 1) % 100_000 == 0):\n",
    "                print(f'    {\"{:,}\".format(episode + 1)} episodes completed')\n",
    "\n",
    "#             print(self.trajectory)\n",
    "        \n",
    "        self.end = 0\n",
    "        print('Training Complete!')\n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function that will get the shortest path\n",
    "    def get_shortest_path(self, start_row_index, start_column_index, available_pipe):\n",
    "        self.reset()\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        self.trajectory.append([current_row_index, current_column_index])\n",
    "\n",
    "        pipes_used = 0\n",
    "        \n",
    "        while not (self.is_terminal_state(current_row_index, current_column_index) | (self.end == 1)):\n",
    "#             print(self.trajectory)\n",
    "            # get the best action to take\n",
    "            action_index = self.get_next_action(current_row_index, current_column_index, 1.)\n",
    "\n",
    "            # move to the next location on the path, and add the new location to the list\n",
    "            current_row_index, current_column_index = self.get_next_location(current_row_index, current_column_index,\n",
    "                                                                        action_index)\n",
    "\n",
    "            \n",
    "            pipes_used += 1\n",
    "\n",
    "            if pipes_used == available_pipe:\n",
    "                self.end = 1\n",
    "                print('Pipes Over')\n",
    "            else:\n",
    "                self.trajectory.append([current_row_index, current_column_index])\n",
    "#                 print(self.trajectory)\n",
    "\n",
    "        return self.trajectory\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Define a function that will reset everything\n",
    "    def reset(self):\n",
    "        self.trajectory = []        \n",
    "        self.end = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bab4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 1, delim=\",\")\n",
    "env = QDriller(env_config)\n",
    "\n",
    "\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66edc82a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started!\n",
      "    100,000 episodes completed\n",
      "    200,000 episodes completed\n",
      "    300,000 episodes completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.populate_q_table(1_000_000)\n",
    "\n",
    "# plt.figure(figsize=(15, 7))\n",
    "# plt.imshow(env.explored, vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbae4d49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_row = 0\n",
    "start_column = 10\n",
    "available_pipe = 100\n",
    "\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "\n",
    "trajectory = env.get_shortest_path(start_row, start_column,available_pipe)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.rewards, vmin=-100, vmax=100)\n",
    "\n",
    "for i in range(0,len(trajectory)):\n",
    "    traj_z, traj_x = np.asarray(trajectory).T\n",
    "    plt.plot(traj_x, traj_z, \"-\", linewidth=6, color = 'k')\n",
    "\n",
    "plt.xticks(np.arange(0, 80, 1.0))\n",
    "plt.yticks(np.arange(0, 40, 1.0))\n",
    "plt.xlim([-0.5, 79.5])\n",
    "plt.ylim([39.5, -0.5])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8388d530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f4e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015e8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d25cdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e81df90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7d43a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a7ed824",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd06b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", delim=\",\", available_pipe=70, num_wells = 1, start_column = 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa5bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.loadtxt(env_config[\"model_path\"],\n",
    "                   delimiter=env_config[\"delim\"])\n",
    "\n",
    "# Normalizing the model\n",
    "rewards = rewards*(100/rewards.max())\n",
    "\n",
    "rewards[np.less(rewards,0)] = -100\n",
    "rewards[rewards == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f29f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = {\n",
    "#            0: [1, 0],  # down\n",
    "#            1: [0, -1],  # left\n",
    "#            2: [0, 1],  # right\n",
    "#            3: [-1, 0],  # up\n",
    "#           }\n",
    "\n",
    "#define actions\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3652863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = np.zeros((rewards.shape[0], rewards.shape[1], len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that determines if the specified location is a terminal state\n",
    "\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "\n",
    "    if ((len(trajectory) > 1) & \n",
    "        (rewards[current_row_index, current_column_index] == -100)):\n",
    "        return True\n",
    "    \n",
    "#     elif ((len(trajectory) > 1) & \n",
    "#         ([current_row_index, current_column_index] in trajectory)):\n",
    "#         return True\n",
    "    \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#     if ((rewards[current_row_index, current_column_index] <= 0) | \n",
    "#         (self.bit_location in self.trajectory[:-1]) | \n",
    "#         (self.pipe_used == self.available_pipe)):\n",
    "        \n",
    "#         return True\n",
    "\n",
    "#     elif (self.last_action != None):\n",
    "#         if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "#             return True\n",
    "\n",
    "#     else:\n",
    "#         return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9be57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that will choose a random, non-terminal starting location\n",
    "\n",
    "def get_starting_location():        \n",
    "#     current_column_index = env_config[\"start_column\"]\n",
    "    \n",
    "    #get a random row and column index\n",
    "    current_column_index = np.random.randint(rewards.shape[1])\n",
    "    \n",
    "#     while current_column_index in starting_location_cache:\n",
    "#         current_column_index = np.random.randint(rewards.shape[1])\n",
    "        \n",
    "    return 0, current_column_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(current_row_index, current_column_index):\n",
    "    va = [0, 1, 2, 3]\n",
    "\n",
    "    if [current_row_index - 1, current_column_index] in trajectory:\n",
    "        va.remove(0)\n",
    "\n",
    "    if [current_row_index, current_column_index + 1] in trajectory:\n",
    "        va.remove(1)  \n",
    "        \n",
    "    if [current_row_index + 1, current_column_index] in trajectory:\n",
    "        va.remove(2) \n",
    "\n",
    "    if [current_row_index, current_column_index - 1] in trajectory:\n",
    "        va.remove(3) \n",
    "    \n",
    "    return va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6047613",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next_action(0, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "    valid_actions = get_valid_actions(current_row_index, current_column_index)\n",
    "    max_q = np.max(q_values[current_row_index, current_column_index][valid_actions])\n",
    "    \n",
    "    if np.random.random() < epsilon:\n",
    "        action = (np.where(q_values[current_row_index, current_column_index] == max_q)[0][0])\n",
    "        print(f'Valid Actions: {valid_actions}, Picked Action: {action}')\n",
    "        return action\n",
    "    \n",
    "    else: \n",
    "        return np.random.randint(len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that will get the next location based on the chosen action\n",
    "\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "    \n",
    "    new_row_index = current_row_index\n",
    "    new_column_index = current_column_index\n",
    "    if actions[action_index] == 'up' and current_row_index > 0:\n",
    "        new_row_index -= 1\n",
    "        \n",
    "    elif actions[action_index] == 'right' and current_column_index < rewards.shape[1] - 1:\n",
    "        new_column_index += 1\n",
    "        \n",
    "    elif actions[action_index] == 'down' and current_row_index < rewards.shape[0] - 1:\n",
    "        new_row_index += 1\n",
    "        \n",
    "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "        new_column_index -= 1\n",
    "        \n",
    "    return new_row_index, new_column_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa54731",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb572359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training parameters\n",
    "epsilon = 0.01 #the percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the AI agent should learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7bedbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run through training episodes\n",
    "\n",
    "for episode in range(10_000):\n",
    "    trajectory = []\n",
    "    \n",
    "    #get the starting location for this episode\n",
    "    row_index, column_index = get_starting_location()\n",
    "\n",
    "    trajectory.append([row_index, column_index])\n",
    "    \n",
    "    #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "    while not is_terminal_state(row_index, column_index):\n",
    "        \n",
    "        #choose which action to take (i.e., where to move next)\n",
    "        action_index = get_next_action(row_index, column_index, epsilon)\n",
    "        \n",
    "        #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "        old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "        row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "        \n",
    "        #receive the reward for moving to the new state\n",
    "        if ([row_index, column_index] in trajectory):\n",
    "            reward = -100\n",
    "        elif (row_index == 0):\n",
    "            reward = -100\n",
    "        else:\n",
    "            reward = rewards[row_index, column_index] - len(trajectory)/5\n",
    "#         print(reward)\n",
    "\n",
    "        trajectory.append([row_index, column_index])\n",
    "    \n",
    "        old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "        temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "        #update the Q-value for the previous state and action pair\n",
    "        new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "#         print(new_q_value)\n",
    "        q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "        \n",
    "#     print(trajectory)\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d57526a",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e596e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that will get the shortest path\n",
    "\n",
    "def get_shortest_path(start_row_index, start_column_index, available_pipe):\n",
    "    trajectory = []\n",
    "    current_row_index, current_column_index = start_row_index, start_column_index\n",
    "    trajectory.append([current_row_index, current_column_index])\n",
    "    \n",
    "    pipes_used = 0\n",
    "    while not is_terminal_state(current_row_index, current_column_index):\n",
    "        \n",
    "        #get the best action to take\n",
    "        action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "                \n",
    "        #move to the next location on the path, and add the new location to the list\n",
    "        current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "        \n",
    "#         if ([current_row_index, current_column_index] in trajectory)|((current_row_index == 0)):\n",
    "#             break\n",
    "            \n",
    "        trajectory.append([current_row_index, current_column_index])\n",
    "        pipes_used += 1\n",
    "        \n",
    "        if pipes_used == available_pipe:\n",
    "            break\n",
    "        \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f1d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_row = 0\n",
    "start_column = 23\n",
    "available_pipe = 25\n",
    "\n",
    "trajectory = []\n",
    "trajectory = get_shortest_path(start_row, start_column,available_pipe)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(rewards, vmin=-100, vmax=100)\n",
    "\n",
    "for i in range(0,len(trajectory)):\n",
    "    traj_z, traj_x = np.asarray(trajectory).T\n",
    "    plt.plot(traj_x, traj_z, \"-\", linewidth=6, color = 'k')\n",
    "\n",
    "plt.xticks(np.arange(0, 80, 1.0))\n",
    "plt.yticks(np.arange(0, 40, 1.0))\n",
    "plt.xlim([-0.5, 79.5])\n",
    "plt.ylim([39.5, -0.5])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f47c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "\n",
    "actions = ['up', 'right', 'down', 'left']get_valid_actions(2, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values[2, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82244f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d970039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abcfcf4f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bef83",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 1\n",
    "\n",
    "actions = {\n",
    "           0: [1, 0],  # down\n",
    "           1: [0, -1],  # left\n",
    "           2: [0, 1],  # right\n",
    "           3: [-1, 0],  # up\n",
    "          }\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    \n",
    "    print(\"Beginning Drill Campaign:\", episode)\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "#         print(f\"    Action: {actions[action]}\")\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Total Reward: {reward}\")\n",
    "#         print(f\"    done: {done}\\n\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273352b",
   "metadata": {},
   "source": [
    "# Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb3de7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4e748",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "env = RewardDriller(env_config)\n",
    "# env = MultiDriller(env_config)\n",
    "\n",
    "\n",
    "ppo = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "ppo.learn(total_timesteps = 800_000, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ac943",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = MultiDriller(env_config)\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = ppo.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a2b3d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad433c1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c4fc7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "dqn = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "dqn.learn(total_timesteps=500_000, log_interval=1_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b03e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = dqn.predict(state, deterministic=True)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a182590",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31eaef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc33143",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68430c-913a-422d-a19b-8a284d7bc5f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=100, num_wells = 3, delim=\",\")\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "a2c = A2C(\"MlpPolicy\", env, verbose=3)\n",
    "a2c.learn(total_timesteps=500_000, log_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263efa6f",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 100\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = a2c.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4d162",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
