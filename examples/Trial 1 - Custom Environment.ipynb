{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811ed8a0-f8f4-4f04-9b87-6d18c9d550d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from gym.spaces import Box\n",
    "from gym.spaces import Discrete\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76690cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679268d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Task List\n",
    "- ~~Drill multiple wells, one after the other and not to update the environment after every simulation.~~\n",
    "- ~~Make sure well/wells dont crash into each other/itself or any faults/artifacts~~\n",
    "- ~~Avoid 180 degree turns~~\n",
    "- Have a target zone where the well eventually want to make it to and get higher reward\n",
    "- Use a metric like MSE/UCS to get an estimate on the amount of energy required to drill and optimizing it to have lowest energy usage (also tie in the economic constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822d739",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Action Space\n",
    "- Surface Location ?? Pick it randomly or intentionally?\n",
    "- Number of wells to drill\n",
    "- Bit Movement\n",
    "    -  Up\n",
    "    -  Down\n",
    "    -  Left\n",
    "    -  Right\n",
    "    -  Angle ?? If the grid size is as much as a stand then the max angle should be around 3 degrees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e214e4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Observation Space\n",
    "\n",
    "Same shape [matrix] as the input. Ideally 30 ft by 30 ft to match with the drilling pipe (90 ft by 90 ft for stand). Bool with true for wherever well is located."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4459fbd0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Possible Rewards\n",
    "- While Drilling\n",
    "    -  Proximity to Reservoir (based on the percentage of Normalized TOC?) - *Positive Reward*\n",
    "    -  Proximity to Fault - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to itself or other wells - *VERY HIGH Negative Reward*\n",
    "    -  Proximity to the possible depletion zone of an existing well - *VERY HIGH Negative Reward*\n",
    "    -  Remaining oil in the zone of the well - *High Positive Reward*\n",
    "\n",
    "- After Drilling\n",
    "    -  Total UCS/MSE it was drilled through - *Negative Reward based on the UCS total, can also relate it to a USD amount*    \n",
    "    -  Total Well Length - *Negative Reward based on the pipe count, can also relate it to a USD amount* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b0949",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Simple Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6ace1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SimpleDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        self.model = np.loadtxt(\n",
    "            env_config[\"model_path\"],\n",
    "            delimiter=env_config[\"delim\"],\n",
    "        )\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.observation_space = Box(\n",
    "            low=0, high=1, shape=(self.nrow, self.ncol), dtype=\"bool\"\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def step(  # noqa: C901\n",
    "        self, action: int\n",
    "    ) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        done = False\n",
    "        actions = {\n",
    "            0: [1, 0],  # down\n",
    "            1: [0, -1],  # left\n",
    "            2: [0, 1],  # right\n",
    "            3: [-1, 0],  # up\n",
    "        }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        else:\n",
    "            reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            reward = 0\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            reward = -100\n",
    "\n",
    "        info: dict[str, Any] = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"\n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "\n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.surface_hole_location = [1, random.randint(0, self.ncol - 1)]  # noqa: S311\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.bit_location = self.surface_hole_location\n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e44cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Multidriller Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae97ad3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MultiDriller(Env):  # type: ignore\n",
    "    \"\"\"Simple driller environment for multiple wells\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         reward = 0\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "#             reward = 0\n",
    "\n",
    "        else:\n",
    "            self.reward = self.model[newrow, newcol] + self.pipe_used / 2\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "#             reward = 0\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "#                 done = True\n",
    "                self.reward = -100  \n",
    "#                 print('    Immediate 180 degree turn')\n",
    "    \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            self.multi_reward += self.reward \n",
    "            \n",
    "            if len(self.trajectory)>0:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "            self.reset_well()\n",
    "            \n",
    "            if self.wells_drilled < self.num_wells:\n",
    "                    done = False            \n",
    "                    \n",
    "            return self.state, self.multi_reward, done, info\n",
    "        else:\n",
    "            self.last_action = action\n",
    "#             print(f'Last action: {actions[self.last_action]}')\n",
    "            return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "\n",
    "        # Log the surface locations already used\n",
    "        self.surface_location.append(self.surface_hole_location[1])\n",
    "        \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fe87d",
   "metadata": {},
   "source": [
    "# Reward based on Proximity Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b5004",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "10fb22f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:\n",
    "            if len(self.trajectory)>0:\n",
    "                self.update_state()\n",
    "            # Reward from the model\n",
    "            self.reward = (self.model[newrow, newcol] * 2)\n",
    "            \n",
    "            # Checking if the reward from the model is negative and stopping the well\n",
    "            if self.reward < 0:\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:                \n",
    "                # Giving a small reward to encourage the agent to use pipes     \n",
    "                self.reward += -self.pipe_used/10\n",
    "                                \n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        # Avoid going along the surface\n",
    "        if ((self.bit_location != self.surface_hole_location) &\n",
    "                (self.bit_location[0] == 0)):\n",
    "            self.reward = -10\n",
    "            done = True\n",
    "#             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -10\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -10  \n",
    "#                 done = True\n",
    "#                 print('    Immediate 180 degree turn')\n",
    "\n",
    "        if self.reward > 0:\n",
    "            self.multi_reward += self.reward   \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "        \n",
    "        if done:\n",
    "            self.wells_drilled += 1            \n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                self.reset_well()\n",
    "                \n",
    "                if len(self.multi_trajectory) < self.num_wells:\n",
    "#                     print(\"MULTIREWARD\")\n",
    "                    return self.state, self.multi_reward, done, info  \n",
    "                \n",
    "            else:\n",
    "                self.reset_well()\n",
    "                self.reward = - 10            \n",
    "            \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"MULTIREWARD\")\n",
    "                \n",
    "                return self.state, self.multi_reward, done, info\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.reward = -10\n",
    "                \n",
    "#             return self.state, self.reward, done, info\n",
    "        \n",
    "        else:\n",
    "            self.last_action = action\n",
    "        \n",
    "#         print(\"REWARD\")\n",
    "            \n",
    "        return self.state, self.reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42937250",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "54ef086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for multiple wells with rewards based on proximity to reservoir\"\"\"\n",
    "\n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.model = np.loadtxt(env_config[\"model_path\"],\n",
    "                                delimiter=env_config[\"delim\"])\n",
    "\n",
    "        # Normalizing the model between o-10\n",
    "        self.model = self.model*(100/self.model.max())\n",
    "\n",
    "        self.model[np.less(self.model,0)] = -100\n",
    "        self.model[self.model == 0] = 1\n",
    "\n",
    "        self.nrow, self.ncol = self.model.shape\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        \n",
    "        self.available_pipe = env_config[\"available_pipe\"]\n",
    "            \n",
    "        self.num_wells = env_config[\"num_wells\"]\n",
    "                \n",
    "        \n",
    "        self.wells_drilled = 0 \n",
    "        self.reward = 0\n",
    "        self.multi_reward = 0\n",
    "\n",
    "        self.production = 0\n",
    "        self.pipe_used = 0\n",
    "        self.trajectory: list[list[int]] = []\n",
    "        self.bit_location: list[int] = []\n",
    "        self.surface_location = []\n",
    "        self.last_action = None\n",
    "            \n",
    "            \n",
    "        self.multi_trajectory: list[list[list[int]]] = []\n",
    "        self.action_space = Discrete(4)        \n",
    "\n",
    "        self.observation_space = Box(low=0, high=1, \n",
    "                                     shape=(self.nrow, self.ncol), \n",
    "                                     dtype=\"bool\")\n",
    "        self.reset_well()\n",
    "        self.reset()\n",
    "    \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def step(self, action: int) -> tuple[NDArray[np.bool_], int, bool, dict[str, Any]]:\n",
    "        \"\"\"Take step based on action.\"\"\"\n",
    "        \n",
    "        done = False\n",
    "#         self.reset_well()\n",
    "        \n",
    "        actions = {\n",
    "                   0: [1, 0],  # down\n",
    "                   1: [0, -1],  # left\n",
    "                   2: [0, 1],  # right\n",
    "                   3: [-1, 0],  # up\n",
    "                  }\n",
    "\n",
    "        dz_dx = actions[action]\n",
    "        new_location = [prev + now for prev, now in zip(self.bit_location, dz_dx)]\n",
    "\n",
    "        self.bit_location = new_location\n",
    "\n",
    "        self.trajectory.append(new_location)\n",
    "        newrow, newcol = new_location\n",
    "\n",
    "        self.pipe_used += 1\n",
    "\n",
    "        if newrow < 1 or newrow >= self.nrow:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Rows exceeded')\n",
    "\n",
    "        elif newcol < 0 or newcol >= self.ncol:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Number of Cols exceeded')\n",
    "\n",
    "        else:               \n",
    "                \n",
    "            # Incremental Reward from the model\n",
    "#             self.reward = sum([self.model[x,y]*2 for x,y in self.trajectory[1:]])\n",
    "            \n",
    "            model_reward = (self.model[newrow, newcol])\n",
    "            \n",
    "            # Checking if the incremental reward from the model is negative and stopping the well\n",
    "            if model_reward < 0:\n",
    "                done = True\n",
    "                self.reward = -100\n",
    "#                 print('    Negative reward from model')\n",
    "                \n",
    "            else:\n",
    "                # Giving a small -ve reward to encourage the agent to use less pipes     \n",
    "                self.reward += (model_reward - self.pipe_used)\n",
    "#                 print(f'Model Reward: {self.reward}')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "#         # Avoid going along the surface\n",
    "#         if ((self.bit_location != self.surface_hole_location) &\n",
    "#                 (self.bit_location[0] == 0)):\n",
    "#             self.reward += -100\n",
    "#             done = True\n",
    "# #             print('    Going along the surface horizontally')\n",
    "\n",
    "        if self.pipe_used == self.available_pipe:\n",
    "            done = True\n",
    "            self.reward = 0\n",
    "#             print('    Done with total pipes')\n",
    "\n",
    "        if self.bit_location in self.trajectory[:-1]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed onto itself')\n",
    "            \n",
    "        if self.bit_location in [item for sublist in self.multi_trajectory for item in sublist]:\n",
    "            done = True\n",
    "            self.reward = -100\n",
    "#             print('    Crashed into a different well')\n",
    "        \n",
    "        # Avoid immediate 180 degree turns\n",
    "        if (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                self.reward = -100\n",
    "                done = True\n",
    "#                 print('    Immediate 180 degree turn')  \n",
    "            \n",
    "        info: dict[str, Any] = {}\n",
    "#         print(done)\n",
    "        if done:\n",
    "            self.wells_drilled += 1  \n",
    "#             print('Well Done')\n",
    "            done = False\n",
    "            \n",
    "            # Minimum pipe length for wells\n",
    "            if len(self.trajectory) > 5:\n",
    "                self.multi_trajectory.append(self.trajectory)\n",
    "                \n",
    "                # Cache the surface locations already used\n",
    "                self.surface_location.append(self.surface_hole_location[1])\n",
    "                \n",
    "                # Update state\n",
    "                self.update_state()\n",
    "                \n",
    "                if self.reward > 0:\n",
    "                    self.multi_reward += self.reward\n",
    "                else:\n",
    "                    self.multi_reward = -100\n",
    "                \n",
    "            else:\n",
    "                self.multi_reward = -100   \n",
    "                       \n",
    "            if len(self.multi_trajectory) == self.num_wells:\n",
    "                done = True  \n",
    "#                 print(\"FINAL REWARD\")\n",
    "            \n",
    "            # Avoiding infinite loop\n",
    "            elif self.wells_drilled > 100:\n",
    "#                 print(\"INFINITE LOOP\")\n",
    "                done = True\n",
    "                self.multi_reward = -100                \n",
    "            \n",
    "            self.reset_well()\n",
    "            \n",
    "        else:\n",
    "            self.last_action = action\n",
    "            self.multi_reward += self.reward\n",
    "            \n",
    "#         print(self.reward)\n",
    "             \n",
    "        return self.state, self.multi_reward, done, info\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def update_state(self) -> None:\n",
    "        \"\"\"Update state method.\"\"\"        \n",
    "        traj_i, traj_j = np.asarray(self.trajectory).T\n",
    "        self.state[traj_i, traj_j] = 1\n",
    "            \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def render(self) -> None:\n",
    "        \"\"\"Gym environment rendering.\"\"\"\n",
    "        raise NotImplementedError(\"No renderer implemented yet.\")\n",
    "        \n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "\n",
    "    def reset_well(self) -> NDArray[np.bool_]:\n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        \n",
    "        # random surface location  that was not used before\n",
    "        self.surface_hole_location = [0, random.choice(list(set(range(0, self.ncol - 1))-set(self.surface_location)))] \n",
    "        self.bit_location = self.surface_hole_location            \n",
    "        self.trajectory = [self.surface_hole_location]\n",
    "        self.pipe_used = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------      \n",
    "    \n",
    "    def reset(self) -> NDArray[np.bool_]:\n",
    "        \n",
    "        \"\"\"Reset the status of the environment.\"\"\"\n",
    "        self.state = np.zeros((self.nrow, self.ncol), dtype=bool)\n",
    "        self.multi_trajectory = []\n",
    "        self.surface_location = []\n",
    "        self.multi_reward = 0 \n",
    "        self.wells_drilled = 0 \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf5147",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Horizontal well Driller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0b0bd45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Horizontal well driller with a specific start point\n",
    "\n",
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, PReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD , Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f902e541",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "model = np.loadtxt(env_config[\"model_path\"],\n",
    "                   delimiter=env_config[\"delim\"])\n",
    "\n",
    "model[np.less(model,0)] = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f3326",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the bit will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c55f091",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b847f4d8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "775f856b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3dfcb79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "ea8248fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'maze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-295-7baea48ff788>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# canvas, reward, game_over = qmaze.act(DOWN)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# print(\"reward=\", reward)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-1053026d1943>\u001b[0m in \u001b[0;36mshow\u001b[1;34m(qmaze)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqmaze\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'on'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mnrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqmaze\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaze\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'maze'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = Qmaze(model)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7cd1e34",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16eb5ecfd60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b97b03a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f55d2dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "167252ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        \n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            \n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97b5f097",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f52e20c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a583d0f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16eb5f21130>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e49b3820",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2f65dfc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/14999 | Loss: 0.0018 | Episodes: 104 | Win count: 0 | Win rate: 0.000 | time: 287.4 seconds\n",
      "Epoch: 001/14999 | Loss: 0.0811 | Episodes: 10 | Win count: 1 | Win rate: 0.000 | time: 320.8 seconds\n",
      "Epoch: 002/14999 | Loss: 0.0125 | Episodes: 102 | Win count: 1 | Win rate: 0.000 | time: 10.98 minutes\n",
      "Epoch: 003/14999 | Loss: 0.0031 | Episodes: 11 | Win count: 2 | Win rate: 0.000 | time: 11.56 minutes\n",
      "Epoch: 004/14999 | Loss: 0.0165 | Episodes: 22 | Win count: 3 | Win rate: 0.000 | time: 12.72 minutes\n",
      "Epoch: 005/14999 | Loss: 0.0162 | Episodes: 5 | Win count: 4 | Win rate: 0.000 | time: 12.99 minutes\n",
      "Epoch: 006/14999 | Loss: 0.0037 | Episodes: 3 | Win count: 5 | Win rate: 0.000 | time: 13.15 minutes\n",
      "Epoch: 007/14999 | Loss: 0.0020 | Episodes: 80 | Win count: 6 | Win rate: 0.000 | time: 17.38 minutes\n",
      "Epoch: 008/14999 | Loss: 0.0032 | Episodes: 101 | Win count: 6 | Win rate: 0.000 | time: 22.67 minutes\n",
      "Epoch: 009/14999 | Loss: 0.0079 | Episodes: 14 | Win count: 7 | Win rate: 0.000 | time: 23.39 minutes\n",
      "Epoch: 010/14999 | Loss: 0.0064 | Episodes: 106 | Win count: 7 | Win rate: 0.000 | time: 29.15 minutes\n",
      "Epoch: 011/14999 | Loss: 0.0008 | Episodes: 12 | Win count: 8 | Win rate: 0.000 | time: 29.81 minutes\n",
      "Epoch: 012/14999 | Loss: 0.0106 | Episodes: 20 | Win count: 9 | Win rate: 0.000 | time: 30.90 minutes\n",
      "Epoch: 013/14999 | Loss: 0.0028 | Episodes: 106 | Win count: 9 | Win rate: 0.000 | time: 36.55 minutes\n",
      "Epoch: 014/14999 | Loss: 0.0020 | Episodes: 106 | Win count: 9 | Win rate: 0.000 | time: 42.20 minutes\n",
      "Epoch: 015/14999 | Loss: 0.0223 | Episodes: 11 | Win count: 10 | Win rate: 0.000 | time: 42.80 minutes\n",
      "Epoch: 016/14999 | Loss: 0.0087 | Episodes: 5 | Win count: 11 | Win rate: 0.000 | time: 43.07 minutes\n",
      "Epoch: 017/14999 | Loss: 0.0036 | Episodes: 101 | Win count: 11 | Win rate: 0.000 | time: 48.58 minutes\n",
      "Epoch: 018/14999 | Loss: 0.0664 | Episodes: 7 | Win count: 12 | Win rate: 0.000 | time: 48.95 minutes\n",
      "Epoch: 019/14999 | Loss: 0.0027 | Episodes: 104 | Win count: 12 | Win rate: 0.000 | time: 54.36 minutes\n",
      "Epoch: 020/14999 | Loss: 0.0009 | Episodes: 10 | Win count: 13 | Win rate: 0.000 | time: 54.90 minutes\n",
      "Epoch: 021/14999 | Loss: 0.0591 | Episodes: 4 | Win count: 14 | Win rate: 0.000 | time: 55.12 minutes\n",
      "Epoch: 022/14999 | Loss: 0.0027 | Episodes: 104 | Win count: 14 | Win rate: 0.000 | time: 60.78 minutes\n",
      "Epoch: 023/14999 | Loss: 0.0036 | Episodes: 6 | Win count: 15 | Win rate: 0.000 | time: 61.11 minutes\n",
      "Epoch: 024/14999 | Loss: 0.0015 | Episodes: 104 | Win count: 15 | Win rate: 0.625 | time: 66.63 minutes\n",
      "Epoch: 025/14999 | Loss: 0.0430 | Episodes: 2 | Win count: 16 | Win rate: 0.625 | time: 1.11 hours\n",
      "Epoch: 026/14999 | Loss: 0.0489 | Episodes: 104 | Win count: 16 | Win rate: 0.625 | time: 1.20 hours\n",
      "Epoch: 027/14999 | Loss: 0.0009 | Episodes: 101 | Win count: 16 | Win rate: 0.583 | time: 1.30 hours\n",
      "Epoch: 028/14999 | Loss: 0.0073 | Episodes: 4 | Win count: 17 | Win rate: 0.583 | time: 1.30 hours\n",
      "Epoch: 029/14999 | Loss: 0.0025 | Episodes: 86 | Win count: 18 | Win rate: 0.583 | time: 1.37 hours\n",
      "Epoch: 030/14999 | Loss: 0.0041 | Episodes: 13 | Win count: 19 | Win rate: 0.583 | time: 1.39 hours\n",
      "Epoch: 031/14999 | Loss: 0.0030 | Episodes: 44 | Win count: 20 | Win rate: 0.583 | time: 1.43 hours\n",
      "Epoch: 032/14999 | Loss: 0.0338 | Episodes: 1 | Win count: 21 | Win rate: 0.625 | time: 1.43 hours\n",
      "Epoch: 033/14999 | Loss: 0.0045 | Episodes: 105 | Win count: 21 | Win rate: 0.583 | time: 1.52 hours\n",
      "Epoch: 034/14999 | Loss: 0.0018 | Episodes: 32 | Win count: 22 | Win rate: 0.625 | time: 1.55 hours\n",
      "Epoch: 035/14999 | Loss: 0.0033 | Episodes: 13 | Win count: 23 | Win rate: 0.625 | time: 1.56 hours\n",
      "Epoch: 036/14999 | Loss: 0.0036 | Episodes: 8 | Win count: 24 | Win rate: 0.625 | time: 1.57 hours\n",
      "Epoch: 037/14999 | Loss: 0.0016 | Episodes: 13 | Win count: 25 | Win rate: 0.667 | time: 1.58 hours\n",
      "Epoch: 038/14999 | Loss: 0.0023 | Episodes: 4 | Win count: 26 | Win rate: 0.708 | time: 1.58 hours\n",
      "Epoch: 039/14999 | Loss: 0.0019 | Episodes: 20 | Win count: 27 | Win rate: 0.708 | time: 1.60 hours\n",
      "Epoch: 040/14999 | Loss: 0.0021 | Episodes: 1 | Win count: 28 | Win rate: 0.708 | time: 1.60 hours\n",
      "Epoch: 041/14999 | Loss: 0.0010 | Episodes: 5 | Win count: 29 | Win rate: 0.750 | time: 1.61 hours\n",
      "Epoch: 042/14999 | Loss: 0.0042 | Episodes: 29 | Win count: 30 | Win rate: 0.750 | time: 1.63 hours\n",
      "Epoch: 043/14999 | Loss: 0.0019 | Episodes: 12 | Win count: 31 | Win rate: 0.792 | time: 1.64 hours\n",
      "Epoch: 044/14999 | Loss: 0.0009 | Episodes: 12 | Win count: 32 | Win rate: 0.792 | time: 1.65 hours\n",
      "Epoch: 045/14999 | Loss: 0.0040 | Episodes: 9 | Win count: 33 | Win rate: 0.792 | time: 1.66 hours\n",
      "Epoch: 046/14999 | Loss: 0.0011 | Episodes: 6 | Win count: 34 | Win rate: 0.833 | time: 1.67 hours\n",
      "Epoch: 047/14999 | Loss: 0.0202 | Episodes: 110 | Win count: 35 | Win rate: 0.833 | time: 1.77 hours\n",
      "Epoch: 048/14999 | Loss: 0.0023 | Episodes: 13 | Win count: 36 | Win rate: 0.875 | time: 1.78 hours\n",
      "Epoch: 049/14999 | Loss: 0.0273 | Episodes: 17 | Win count: 37 | Win rate: 0.875 | time: 1.79 hours\n",
      "Epoch: 050/14999 | Loss: 0.0007 | Episodes: 22 | Win count: 38 | Win rate: 0.917 | time: 1.81 hours\n",
      "Epoch: 051/14999 | Loss: 0.0022 | Episodes: 19 | Win count: 39 | Win rate: 0.958 | time: 1.83 hours\n",
      "Epoch: 052/14999 | Loss: 0.0006 | Episodes: 24 | Win count: 40 | Win rate: 0.958 | time: 1.85 hours\n",
      "Epoch: 053/14999 | Loss: 0.0015 | Episodes: 21 | Win count: 41 | Win rate: 0.958 | time: 1.87 hours\n",
      "Epoch: 054/14999 | Loss: 0.0010 | Episodes: 11 | Win count: 42 | Win rate: 0.958 | time: 1.88 hours\n",
      "Epoch: 055/14999 | Loss: 0.0003 | Episodes: 3 | Win count: 43 | Win rate: 0.958 | time: 1.88 hours\n",
      "Epoch: 056/14999 | Loss: 0.0002 | Episodes: 18 | Win count: 44 | Win rate: 0.958 | time: 1.90 hours\n",
      "Epoch: 057/14999 | Loss: 0.0005 | Episodes: 17 | Win count: 45 | Win rate: 1.000 | time: 1.92 hours\n",
      "Epoch: 058/14999 | Loss: 0.0007 | Episodes: 3 | Win count: 46 | Win rate: 1.000 | time: 1.92 hours\n",
      "Reached 100% win rate at epoch: 58\n",
      "files: model.h5, model.json\n",
      "n_epoch: 58, max_mem: 392, data: 32, time: 1.93 hours\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6930.312502"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "ac56937a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x16eb5f215b0>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4e381",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca6373c6",
   "metadata": {},
   "source": [
    "# Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "f4eef3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 1, delim=\",\")\n",
    "env = RewardDriller(env_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "2fbf4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDriller(Env):  # type: ignore\n",
    "    \"\"\"Driller environment for horizontal wells with self.rewards based on Q learning\"\"\"\n",
    "    \n",
    "    def __init__(self, env_config: dict[str, Any]) -> None:\n",
    "        \"\"\"Initialize environment with config dictionary.\"\"\"\n",
    "        \n",
    "        self.rewards = np.loadtxt(env_config[\"model_path\"],\n",
    "                             delimiter=env_config[\"delim\"])\n",
    "        \n",
    "        # Normalizing the model\n",
    "        self.rewards = self.rewards*(100/self.rewards.max())\n",
    "        \n",
    "        self.rewards[np.less(self.rewards,0)] = -100\n",
    "        self.rewards[self.rewards == 0] = -1\n",
    "        \n",
    "        self.actions = ['up', 'right', 'down', 'left']\n",
    "        \n",
    "        self.q_values = np.zeros((self.rewards.shape[0], \n",
    "                                  self.rewards.shape[1], \n",
    "                                  len(self.actions)))\n",
    "    \n",
    "    #define a function that determines if the specified location is a terminal state\n",
    "    def is_terminal_state(self, current_row_index, current_column_index):\n",
    "\n",
    "        if ((rewards[current_row_index, current_column_index] <= 0) | \n",
    "            (self.bit_location in self.trajectory[:-1]) | \n",
    "            (self.pipe_used == self.available_pipe)):\n",
    "\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        if ((rewards[current_row_index, current_column_index] <= 0) | \n",
    "            (self.bit_location in self.trajectory[:-1]) | \n",
    "            (self.pipe_used == self.available_pipe)):\n",
    "\n",
    "            return True\n",
    "\n",
    "        elif (self.last_action != None):\n",
    "            if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "                return True\n",
    "\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    #define a function that will choose a random, non-terminal starting location\n",
    "    def get_starting_location(self):        \n",
    "        #get a random row and column index\n",
    "        current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "        current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "\n",
    "        while is_terminal_state(current_row_index, current_column_index):\n",
    "            current_row_index = np.random.randint(self.rewards.shape[0])\n",
    "            current_column_index = np.random.randint(self.rewards.shape[1])\n",
    "\n",
    "        return current_row_index, current_column_index\n",
    "    \n",
    "    #define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "    def get_next_action(self, current_row_index, current_column_index, epsilon):\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.argmax(q_values[current_row_index, current_column_index])\n",
    "\n",
    "        else: \n",
    "            return np.random.randint(len(actions))\n",
    "        \n",
    "    #define a function that will get the next location based on the chosen action\n",
    "    def get_next_location(self, current_row_index, current_column_index, action_index):\n",
    "\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        if actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "\n",
    "        elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
    "            new_column_index += 1\n",
    "\n",
    "        elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
    "            new_row_index += 1\n",
    "\n",
    "        elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "\n",
    "        return new_row_index, new_column_index\n",
    "    \n",
    "    #Define a function that will get the shortest path\n",
    "    def get_shortest_path(self, start_row_index, start_column_index):\n",
    "\n",
    "        if is_terminal_state(start_row_index, start_column_index):\n",
    "            return []\n",
    "\n",
    "        else: \n",
    "            current_row_index, current_column_index = start_row_index, start_column_index\n",
    "\n",
    "        shortest_path = []\n",
    "        shortest_path.append([current_row_index, current_column_index])\n",
    "\n",
    "        while not is_terminal_state(current_row_index, current_column_index):\n",
    "\n",
    "            #get the best action to take\n",
    "            action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "\n",
    "            #move to the next location on the path, and add the new location to the list\n",
    "            current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "\n",
    "            shortest_path.append([current_row_index, current_column_index])\n",
    "\n",
    "        return shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "6ba06e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.loadtxt(env_config[\"model_path\"],\n",
    "                   delimiter=env_config[\"delim\"])\n",
    "\n",
    "# Normalizing the model\n",
    "rewards = rewards*(100/rewards.max())\n",
    "\n",
    "rewards[np.less(rewards,0)] = -100\n",
    "rewards[rewards == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "584b831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 1, delim=\",\")\n",
    "env = QDriller(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "00562cc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-698-acb33f17eeab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_starting_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-696-9d2360459bd5>\u001b[0m in \u001b[0;36mget_starting_location\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mcurrent_column_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[1;32mwhile\u001b[0m \u001b[0mis_terminal_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_row_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_column_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[0mcurrent_row_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mcurrent_column_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-685-61f4eef55dbf>\u001b[0m in \u001b[0;36mis_terminal_state\u001b[1;34m(current_row_index, current_column_index)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     if ((rewards[current_row_index, current_column_index] <= 0) | \n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbit_location\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         (self.pipe_used == self.available_pipe)):\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "env.get_starting_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "4b5ecaaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-691-5269a1558f2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#get the starting location for this episode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mrow_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_starting_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#continue taking actions (i.e., moving) until we reach a terminal state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-689-111fc9813057>\u001b[0m in \u001b[0;36mget_starting_location\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mcurrent_column_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[1;32mwhile\u001b[0m \u001b[0mis_terminal_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_row_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_column_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mcurrent_row_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mcurrent_column_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-685-61f4eef55dbf>\u001b[0m in \u001b[0;36mis_terminal_state\u001b[1;34m(current_row_index, current_column_index)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     if ((rewards[current_row_index, current_column_index] <= 0) | \n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbit_location\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         (self.pipe_used == self.available_pipe)):\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "#run through 1000 training episodes\n",
    "for episode in range(1000):\n",
    "    #get the starting location for this episode\n",
    "    row_index, column_index = env.get_starting_location()\n",
    "    \n",
    "    #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "    while not env.is_terminal_state(row_index, column_index):\n",
    "        \n",
    "        #choose which action to take (i.e., where to move next)\n",
    "        action_index = env.get_next_action(row_index, column_index, epsilon)\n",
    "        \n",
    "        #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "        old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "        row_index, column_index = env.get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "        #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "        reward = env.rewards[row_index, column_index]\n",
    "        old_q_value = env.q_values[old_row_index, old_column_index, action_index]\n",
    "        temporal_difference = reward + (discount_factor * np.max(env.q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "        #update the Q-value for the previous state and action pair\n",
    "        new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "        env.q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab78565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7dc016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7279b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "75a50005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = {\n",
    "#            0: [1, 0],  # down\n",
    "#            1: [0, -1],  # left\n",
    "#            2: [0, 1],  # right\n",
    "#            3: [-1, 0],  # up\n",
    "#           }\n",
    "\n",
    "#define actions\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "1deba847",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = np.zeros((model.shape[0], model.shape[1], len(actions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "c076a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that determines if the specified location is a terminal state\n",
    "\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "\n",
    "    if ((rewards[current_row_index, current_column_index] <= 0) | \n",
    "        (self.bit_location in self.trajectory[:-1]) | \n",
    "        (self.pipe_used == self.available_pipe)):\n",
    "        \n",
    "        return True\n",
    "\n",
    "    elif (self.last_action != None):\n",
    "        if (np.add(actions[action], actions[self.last_action]).tolist() == [0,0]):\n",
    "            return True\n",
    "\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "1e385e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that will choose a random, non-terminal starting location\n",
    "\n",
    "def get_starting_location():        \n",
    "    #get a random row and column index\n",
    "    current_row_index = np.random.randint(model.shape[0])\n",
    "    current_column_index = np.random.randint(model.shape[1])\n",
    "    \n",
    "    while is_terminal_state(current_row_index, current_column_index):\n",
    "        current_row_index = np.random.randint(model.shape[0])\n",
    "        current_column_index = np.random.randint(model.shape[1])\n",
    "        \n",
    "    return current_row_index, current_column_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "8d8105dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.argmax(q_values[current_row_index, current_column_index])\n",
    "    \n",
    "    else: \n",
    "        return np.random.randint(len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "147571d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that will get the next location based on the chosen action\n",
    "\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "    \n",
    "    new_row_index = current_row_index\n",
    "    new_column_index = current_column_index\n",
    "    if actions[action_index] == 'up' and current_row_index > 0:\n",
    "        new_row_index -= 1\n",
    "        \n",
    "    elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
    "        new_column_index += 1\n",
    "        \n",
    "    elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
    "        new_row_index += 1\n",
    "        \n",
    "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "        new_column_index -= 1\n",
    "        \n",
    "    return new_row_index, new_column_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "f8494272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that will get the shortest path\n",
    "\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "    \n",
    "    if is_terminal_state(start_row_index, start_column_index):\n",
    "        return []\n",
    "    \n",
    "    else: \n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "    \n",
    "    shortest_path = []\n",
    "    shortest_path.append([current_row_index, current_column_index])\n",
    "    \n",
    "    while not is_terminal_state(current_row_index, current_column_index):\n",
    "        \n",
    "        #get the best action to take\n",
    "        action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "        \n",
    "        #move to the next location on the path, and add the new location to the list\n",
    "        current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "        \n",
    "        shortest_path.append([current_row_index, current_column_index])\n",
    "        \n",
    "    return shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training parameters\n",
    "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the AI agent should learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "3ac4215d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-686-21d91d509268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#get the starting location for this episode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mrow_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_starting_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#continue taking actions (i.e., moving) until we reach a terminal state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-678-f326b2c448ae>\u001b[0m in \u001b[0;36mget_starting_location\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcurrent_column_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mwhile\u001b[0m \u001b[0mis_terminal_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_row_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_column_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mcurrent_row_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mcurrent_column_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-685-61f4eef55dbf>\u001b[0m in \u001b[0;36mis_terminal_state\u001b[1;34m(current_row_index, current_column_index)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     if ((rewards[current_row_index, current_column_index] <= 0) | \n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbit_location\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         (self.pipe_used == self.available_pipe)):\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "#run through 1000 training episodes\n",
    "for episode in range(1000):\n",
    "    #get the starting location for this episode\n",
    "    row_index, column_index = get_starting_location()\n",
    "    \n",
    "    #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "    while not is_terminal_state(row_index, column_index):\n",
    "        \n",
    "        #choose which action to take (i.e., where to move next)\n",
    "        action_index = get_next_action(row_index, column_index, epsilon)\n",
    "        \n",
    "        #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "        old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "        row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "        #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "        reward = rewards[row_index, column_index]\n",
    "        old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "        temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "        #update the Q-value for the previous state and action pair\n",
    "        new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "        q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c5807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb07f7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7947408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affaf29d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02bd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76534fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180a63d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abcfcf4f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "429bef83",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Drill Campaign: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 1\n",
    "\n",
    "actions = {\n",
    "           0: [1, 0],  # down\n",
    "           1: [0, -1],  # left\n",
    "           2: [0, 1],  # right\n",
    "           3: [-1, 0],  # up\n",
    "          }\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    \n",
    "    print(\"Beginning Drill Campaign:\", episode)\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "#         print(f\"    Action: {actions[action]}\")\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Total Reward: {reward}\")\n",
    "#         print(f\"    done: {done}\\n\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273352b",
   "metadata": {},
   "source": [
    "# Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb3de7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "09a4e748",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 652           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 31            |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8612773e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.08e+06      |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -1.84e-05     |\n",
      "|    value_loss           | 2.08e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 626           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 65            |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9257208e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.03e+06      |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -3.45e-05     |\n",
      "|    value_loss           | 2.04e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 623           |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 98            |\n",
      "|    total_timesteps      | 61440         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6074337e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.53e+05      |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -1.64e-05     |\n",
      "|    value_loss           | 2e+06         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 623           |\n",
      "|    iterations           | 40            |\n",
      "|    time_elapsed         | 131           |\n",
      "|    total_timesteps      | 81920         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.5297354e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.89e+05      |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -8.18e-05     |\n",
      "|    value_loss           | 1.96e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 101          |\n",
      "|    ep_rew_mean          | -1.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 622          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 164          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.577734e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.28e+05     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -2.91e-07    |\n",
      "|    value_loss           | 1.92e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 622           |\n",
      "|    iterations           | 60            |\n",
      "|    time_elapsed         | 197           |\n",
      "|    total_timesteps      | 122880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.1386014e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.42e+05      |\n",
      "|    n_updates            | 590           |\n",
      "|    policy_gradient_loss | -5.3e-05      |\n",
      "|    value_loss           | 1.88e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 622           |\n",
      "|    iterations           | 70            |\n",
      "|    time_elapsed         | 230           |\n",
      "|    total_timesteps      | 143360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1993765e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.84e+05      |\n",
      "|    n_updates            | 690           |\n",
      "|    policy_gradient_loss | 1.49e-06      |\n",
      "|    value_loss           | 1.84e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 622           |\n",
      "|    iterations           | 80            |\n",
      "|    time_elapsed         | 263           |\n",
      "|    total_timesteps      | 163840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0689517e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.85e+05      |\n",
      "|    n_updates            | 790           |\n",
      "|    policy_gradient_loss | -0.000118     |\n",
      "|    value_loss           | 1.81e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 101          |\n",
      "|    ep_rew_mean          | -1.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 621          |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 296          |\n",
      "|    total_timesteps      | 184320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.053851e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.45e+05     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -5.64e-05    |\n",
      "|    value_loss           | 1.77e+06     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 101          |\n",
      "|    ep_rew_mean          | -1.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 620          |\n",
      "|    iterations           | 100          |\n",
      "|    time_elapsed         | 330          |\n",
      "|    total_timesteps      | 204800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.065413e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.71e+05     |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -6.55e-05    |\n",
      "|    value_loss           | 1.73e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 617           |\n",
      "|    iterations           | 110           |\n",
      "|    time_elapsed         | 364           |\n",
      "|    total_timesteps      | 225280        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0167132e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.93e+05      |\n",
      "|    n_updates            | 1090          |\n",
      "|    policy_gradient_loss | -7.96e-06     |\n",
      "|    value_loss           | 1.71e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 614           |\n",
      "|    iterations           | 120           |\n",
      "|    time_elapsed         | 399           |\n",
      "|    total_timesteps      | 245760        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.0522947e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.16e+05      |\n",
      "|    n_updates            | 1190          |\n",
      "|    policy_gradient_loss | -7.71e-05     |\n",
      "|    value_loss           | 1.67e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 612           |\n",
      "|    iterations           | 130           |\n",
      "|    time_elapsed         | 434           |\n",
      "|    total_timesteps      | 266240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5033078e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.63e+05      |\n",
      "|    n_updates            | 1290          |\n",
      "|    policy_gradient_loss | -2.03e-05     |\n",
      "|    value_loss           | 1.64e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 611           |\n",
      "|    iterations           | 140           |\n",
      "|    time_elapsed         | 469           |\n",
      "|    total_timesteps      | 286720        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.8521906e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.74e+05      |\n",
      "|    n_updates            | 1390          |\n",
      "|    policy_gradient_loss | -2.2e-06      |\n",
      "|    value_loss           | 1.61e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 609           |\n",
      "|    iterations           | 150           |\n",
      "|    time_elapsed         | 503           |\n",
      "|    total_timesteps      | 307200        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0137446e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.04e+05      |\n",
      "|    n_updates            | 1490          |\n",
      "|    policy_gradient_loss | -0.000103     |\n",
      "|    value_loss           | 1.58e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 608           |\n",
      "|    iterations           | 160           |\n",
      "|    time_elapsed         | 538           |\n",
      "|    total_timesteps      | 327680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.6517096e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.22e+05      |\n",
      "|    n_updates            | 1590          |\n",
      "|    policy_gradient_loss | -7.2e-05      |\n",
      "|    value_loss           | 1.55e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 607           |\n",
      "|    iterations           | 170           |\n",
      "|    time_elapsed         | 573           |\n",
      "|    total_timesteps      | 348160        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1694148e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.89e+05      |\n",
      "|    n_updates            | 1690          |\n",
      "|    policy_gradient_loss | 4.07e-07      |\n",
      "|    value_loss           | 1.53e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 606           |\n",
      "|    iterations           | 180           |\n",
      "|    time_elapsed         | 608           |\n",
      "|    total_timesteps      | 368640        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5524565e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.8e+05       |\n",
      "|    n_updates            | 1790          |\n",
      "|    policy_gradient_loss | -0.000155     |\n",
      "|    value_loss           | 1.5e+06       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 604           |\n",
      "|    iterations           | 190           |\n",
      "|    time_elapsed         | 643           |\n",
      "|    total_timesteps      | 389120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1621097e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.52e+05      |\n",
      "|    n_updates            | 1890          |\n",
      "|    policy_gradient_loss | -3.79e-05     |\n",
      "|    value_loss           | 1.47e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 603           |\n",
      "|    iterations           | 200           |\n",
      "|    time_elapsed         | 678           |\n",
      "|    total_timesteps      | 409600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.1651455e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.82e+05      |\n",
      "|    n_updates            | 1990          |\n",
      "|    policy_gradient_loss | -8.57e-05     |\n",
      "|    value_loss           | 1.45e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 603           |\n",
      "|    iterations           | 210           |\n",
      "|    time_elapsed         | 712           |\n",
      "|    total_timesteps      | 430080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8096256e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.02e+05      |\n",
      "|    n_updates            | 2090          |\n",
      "|    policy_gradient_loss | -2.34e-05     |\n",
      "|    value_loss           | 1.43e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 604           |\n",
      "|    iterations           | 220           |\n",
      "|    time_elapsed         | 745           |\n",
      "|    total_timesteps      | 450560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.1854156e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.06e+05      |\n",
      "|    n_updates            | 2190          |\n",
      "|    policy_gradient_loss | -6.32e-05     |\n",
      "|    value_loss           | 1.41e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 101          |\n",
      "|    ep_rew_mean          | -1.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 604          |\n",
      "|    iterations           | 230          |\n",
      "|    time_elapsed         | 779          |\n",
      "|    total_timesteps      | 471040       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.025388e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.95e+05     |\n",
      "|    n_updates            | 2290         |\n",
      "|    policy_gradient_loss | -0.000183    |\n",
      "|    value_loss           | 1.39e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 604           |\n",
      "|    iterations           | 240           |\n",
      "|    time_elapsed         | 812           |\n",
      "|    total_timesteps      | 491520        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4555251e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.51e+05      |\n",
      "|    n_updates            | 2390          |\n",
      "|    policy_gradient_loss | -0.000218     |\n",
      "|    value_loss           | 1.37e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 101          |\n",
      "|    ep_rew_mean          | -1.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 604          |\n",
      "|    iterations           | 250          |\n",
      "|    time_elapsed         | 846          |\n",
      "|    total_timesteps      | 512000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.896159e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.58e+05     |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | -2.8e-05     |\n",
      "|    value_loss           | 1.35e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 101          |\n",
      "|    ep_rew_mean          | -1.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 260          |\n",
      "|    time_elapsed         | 880          |\n",
      "|    total_timesteps      | 532480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.055772e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.35e+05     |\n",
      "|    n_updates            | 2590         |\n",
      "|    policy_gradient_loss | -3.58e-05    |\n",
      "|    value_loss           | 1.33e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 605           |\n",
      "|    iterations           | 270           |\n",
      "|    time_elapsed         | 913           |\n",
      "|    total_timesteps      | 552960        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4359743e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.52e+05      |\n",
      "|    n_updates            | 2690          |\n",
      "|    policy_gradient_loss | -0.000132     |\n",
      "|    value_loss           | 1.32e+06      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 605           |\n",
      "|    iterations           | 280           |\n",
      "|    time_elapsed         | 947           |\n",
      "|    total_timesteps      | 573440        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0311854e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.91e+05      |\n",
      "|    n_updates            | 2790          |\n",
      "|    policy_gradient_loss | -0.000166     |\n",
      "|    value_loss           | 1.3e+06       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 605           |\n",
      "|    iterations           | 290           |\n",
      "|    time_elapsed         | 980           |\n",
      "|    total_timesteps      | 593920        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7209968e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.76e+05      |\n",
      "|    n_updates            | 2890          |\n",
      "|    policy_gradient_loss | -7.58e-06     |\n",
      "|    value_loss           | 1.29e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 605           |\n",
      "|    iterations           | 300           |\n",
      "|    time_elapsed         | 1014          |\n",
      "|    total_timesteps      | 614400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.1234965e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.68e+05      |\n",
      "|    n_updates            | 2990          |\n",
      "|    policy_gradient_loss | -3.7e-05      |\n",
      "|    value_loss           | 1.27e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 605           |\n",
      "|    iterations           | 310           |\n",
      "|    time_elapsed         | 1047          |\n",
      "|    total_timesteps      | 634880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9248768e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.69e+05      |\n",
      "|    n_updates            | 3090          |\n",
      "|    policy_gradient_loss | -0.000251     |\n",
      "|    value_loss           | 1.26e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 605           |\n",
      "|    iterations           | 320           |\n",
      "|    time_elapsed         | 1081          |\n",
      "|    total_timesteps      | 655360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3163633e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.41e+05      |\n",
      "|    n_updates            | 3190          |\n",
      "|    policy_gradient_loss | -0.0001       |\n",
      "|    value_loss           | 1.25e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 606           |\n",
      "|    iterations           | 330           |\n",
      "|    time_elapsed         | 1115          |\n",
      "|    total_timesteps      | 675840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0488515e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.12e+05      |\n",
      "|    n_updates            | 3290          |\n",
      "|    policy_gradient_loss | -9.21e-06     |\n",
      "|    value_loss           | 1.24e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 606           |\n",
      "|    iterations           | 340           |\n",
      "|    time_elapsed         | 1148          |\n",
      "|    total_timesteps      | 696320        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1968717e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.7e+05       |\n",
      "|    n_updates            | 3390          |\n",
      "|    policy_gradient_loss | -7.56e-05     |\n",
      "|    value_loss           | 1.23e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 605           |\n",
      "|    iterations           | 350           |\n",
      "|    time_elapsed         | 1182          |\n",
      "|    total_timesteps      | 716800        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8102932e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.55e+05      |\n",
      "|    n_updates            | 3490          |\n",
      "|    policy_gradient_loss | -0.000134     |\n",
      "|    value_loss           | 1.22e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 605           |\n",
      "|    iterations           | 360           |\n",
      "|    time_elapsed         | 1218          |\n",
      "|    total_timesteps      | 737280        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8552779e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.8e+05       |\n",
      "|    n_updates            | 3590          |\n",
      "|    policy_gradient_loss | -0.000202     |\n",
      "|    value_loss           | 1.22e+06      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 101          |\n",
      "|    ep_rew_mean          | -1.01e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 604          |\n",
      "|    iterations           | 370          |\n",
      "|    time_elapsed         | 1254         |\n",
      "|    total_timesteps      | 757760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.644535e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.47e+05     |\n",
      "|    n_updates            | 3690         |\n",
      "|    policy_gradient_loss | -2.78e-05    |\n",
      "|    value_loss           | 1.21e+06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 101         |\n",
      "|    ep_rew_mean          | -1.01e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 603         |\n",
      "|    iterations           | 380         |\n",
      "|    time_elapsed         | 1289        |\n",
      "|    total_timesteps      | 778240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 8.29334e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.25e+05    |\n",
      "|    n_updates            | 3790        |\n",
      "|    policy_gradient_loss | -4.93e-05   |\n",
      "|    value_loss           | 1.2e+06     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 101           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 602           |\n",
      "|    iterations           | 390           |\n",
      "|    time_elapsed         | 1325          |\n",
      "|    total_timesteps      | 798720        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6201305e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.9e+05       |\n",
      "|    n_updates            | 3890          |\n",
      "|    policy_gradient_loss | -1.01e-05     |\n",
      "|    value_loss           | 1.19e+06      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x16e834bd610>"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "env = RewardDriller(env_config)\n",
    "# env = MultiDriller(env_config)\n",
    "\n",
    "\n",
    "ppo = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "ppo.learn(total_timesteps = 800_000, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "f76ac943",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABH4AAAJNCAYAAABHi7IgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdfbSlV10n+O8vlaICJFhUSHhJQgdZkMFGSOBSg2ZQDG8ZtEGw6SYzpqOyLGWhEhctIvRIM65ebaNi4zgLLU0I9sS0aBJfEIU0A2bFBSluYkESK0K3jXSRNEUoQ4JIparymz/qMFNTVuWel+fcl5PPZ6277nP2s5+zv7fq1jknv+xn7+ruAAAAALB4TlrrAAAAAADMh8IPAAAAwIJS+AEAAABYUAo/AAAAAAtK4QcAAABgQSn8AAAAACyok1dzsEfUlj4lj17NIQEAAAAW2v3523u6+4zjnVvVws8peXT+x3rRag4JAAAAsND+U//e35zonFu9AAAAABbUTIWfqrq4qv6qqv5zVb1lqFAAAAAAzG7qW72qalOS/zPJS5LsTfLJqvrD7v7LcZ/js2/+geO2P+2dV00bCwDm7oaf/fBx21/yv790lZPMx/fueP1x239/53tWOQkbwX9/753HbX/CD/4Pq5wEYHrvffNPHLf9B9/5K6ucZHi/8vTbj9v+E5955ionmY+LX/P3x23/09995ConWb9mmfGzPcl/7u6/7u4HkvzHJK8cJhYAAAAAs5ql8HNWkv921OO9ozYAAAAA1oFZdvWq47T1P+hUtSPJjiQ5JY+aYTgAAAAAJjHLjJ+9Sc456vHZSe46tlN37+zupe5e2pwtMwwHAAAAwCRmKfx8MsnTquopVfWIJK9N8ofDxAIAAABgVlPf6tXdh6rqx5J8KMmmJFd29x2DJQMAAABgJrOs8ZPu/mCSDw6UBQAAAIABzXKrFwAAAADrmMIPAAAAwIKa6VavSZ31rX+Xn/ujT/6/j1/7nh84br+f+6+fPG47AKwL7z1+86K8f93yb19/3PZF+fkY1g9/7LTjtvt9ATaU9xy/eSFey172yOM2L8TPluTP3/zM47Yvys83rv907onPmfEDAAAAsKAUfgAAAAAW1EyFn6q6sqr2VdXtQwUCAAAAYBizrvFzVZJfTfJbs0cBgI3t+e/927H7fv3Uk3LPUx+RvRecklTNMdVwbvm3V651hBU956dflzqp1zoGOfHaP8fzuFMezPMffzD/5NwHctLG+OcAzFtXHvPJV+VRn7kwJ99/5lqnYQM60do/Q9qy7YGcccG9efJL9qXW8f1UM0Xr7huT7B8oCwA8bJzy1Qdz9qe+nrP/4utrHWWh3PrvrljrCEzhnq+flA/8zZb80ecesdZRgHXiMZ98VR5zy6sUfVjXDux/RPZ+5Mx8/ob1/Xu6jmtSALD4HvdfHljrCLBufOKLm9c6ArBOPOozF651BBjbl/5i61pHeEhzL/xU1Y6qWq6q5Xv3H573cAAwd18/dbi3z1O++uBgzzWUR2z90lpHYAN53CnD/Q7f83X/TxI4Yj3M9Dl02r61jsAGcWD/+p6xOvd31+7e2d1L3b20ddumeQ8HAHN3z1PX95v7rE7/xx9f6whsIM9//MG1jgAwF197+p+vdQQYxKyLOwPAw87eC05JcuQ2rfU4Y2dWT3zBHyRJvnzHt+WBe89Y4zSTec5Pv26tIzzs/JNzj9yu+IkvbjZjB1gIh07bl689/c9z3/OuX+soMIiZCj9VdU2SFyZ5XFXtTfL27raqIgCLrSp7n/PI7H3OIye6bJJdv9ZSVedJ3/H7edJ3/P5aR2EDOKmSVz7lgbzyKZOtVzXJrl8A37D39Zc+5Pmz3/MfprqOxXPhO2+fy/Ouxm5hQ5up8NPdlwwVBAAAAIBhmY8LAAAAsKAUfgAAAAAWlMIPAAAAwIJS+AEAAABYUAo/AAAAAAtq6sJPVZ1TVR+tqj1VdUdVvXHIYAAAAADMZpbt3A8leVN331pVpyW5papu6O6/HCgbAMzXg53nv+/etU4BG9qDnfzIn5221jGAh4mz3/Mf1jrCwvimlz1yquu+8qG/HzjJfPz5m5+51hHWjaln/HT33d196+j4/iR7kpw1VDAAmDdFH5idog8ArG+DrPFTVecmuSDJzUM8HwAAAACzm7nwU1WnJrk2yeXdfd9xzu+oquWqWr53/+FZhwOAhfL1U+2zAADHOnTavnX5XBvRg094cK0jzNWWbQ+sdYR1keGhzPRps6o250jR5+ruvu54fbp7Z3cvdffS1m2bZhkOABbOPU99xFpHAIB152tP//N1+Vwb0QMXLfYEjDMuWPtb99dDhocyy65eleSKJHu6+13DRQKA1fGJy7au2dhfP/Wk7H32Kdl7wSlrlgGG8Ovfef9aRwAW0H3Puz73Pff6mWbrHDptX+577vW573nXD5hs4zlw6aF8/X85uLAzf578kn05+0X71mTWzZZtD+TsF+3Lk1+yvmeVzbKr14VJLk1yW1XtHrW9tbs/OHssAFgFJ1U+8YOPXesUsKGdVMlvvFDxBxhYde7bfl3u237cG0uYxEnJgcsO5cBlhya6bNpdv1ZbnZT8o5ftyz962fouvqylqQs/3X1TkhowCwAAAAADsqIkAAAAwIJS+AEAAABYUAo/AAAAAAtK4QcAAABgQSn8AAAAACyoqQs/VXVKVe2qqk9V1R1V9Y4hgwEAAAAwm6m3c09yIMlF3f3Vqtqc5Kaq+pPu/sRA2QAAAACYwdSFn+7uJF8dPdw8+uohQgEAAAAwu5nW+KmqTVW1O8m+JDd0983DxAIAAABgVjMVfrr7cHefn+TsJNur6pnH9qmqHVW1XFXL9+4/PMtwAAAAAExgkF29uvveJB9LcvFxzu3s7qXuXtq6bdMQwwEAAAAwhll29TqjqraOjh+Z5MVJ7hwqGAAAALA2vvLBvx+rjfVvll29npjkfVW1KUcKSO/v7g8MEwsAAABYM5uSr3xIoWcRzLKr16eTXDBgFgAAAAAGNMgaPwAAAACsPwo/AAAAAAtK4QcAAABgQSn8AAAAACwohR8AAACABTVz4aeqNlXVX1SVrdwBAAAA1pGpt3M/yhuT7EnymJU6/vWXnpzXvuffDzAkAAAAACuZacZPVZ2d5LuT/OYwcQAAAAAYyqy3ev37JG9O8uAAWQAAAAAY0NSFn6r6niT7uvuWFfrtqKrlqloe53nPPO2eaSMBAAAAcJRZZvxcmOQVVfW5JP8xyUVV9X8d26m7d3b3UncvjfOk/9PTd80QCQAAAIBvmLrw090/091nd/e5SV6b5P/u7u+f9vnOPO2evPq5H8w/fd4Hp30KAAAAAI4yxK5eY/vmMz6f977+Das5JAAAAMDD1iCFn+7+WJKPDfFcAAAAAAxj1l29AAAAAFinFH4AAAAAFpTCDwAAAMCCUvgBAAAAWFAKPwAAAAALaqZdvarqc0nuT3I4yaHuXhoiFAAAAACzG2I79+/q7nsGeB4AAAAABuRWLwAAAIAFNWvhp5N8uKpuqaodQwQCAAAAYBiz3up1YXffVVVnJrmhqu7s7huP7jAqCO1IkiectWnG4QAAAAAY10wzfrr7rtH3fUmuT7L9OH12dvdSdy9t3abwAwAAALBapi78VNWjq+q0bxwneWmS24cKBgAAAMBsZrnV6/FJrq+qbzzPb3f3nw6SCgAAAICZTV346e6/TvLsAbMAAAAAMCDbuQMAAAAsKIUfAAAAgAWl8AMAAACwoBR+AAAAABaUwg8AAADAgpqp8FNVW6vq96rqzqraU1XfNlQwAAAAAGYz9XbuI+9O8qfd/U+r6hFJHjVAJgAAAAAGMHXhp6oek+Q7kvxAknT3A0keGCYWAAAAALOa5Vavb07ypSTvraq/qKrfrKpHH9upqnZU1XJVLd+7//AMwwEAAAAwiVkKPycneU6S93T3BUn+Lslbju3U3Tu7e6m7l7Zu2zTDcAAAAABMYpbCz94ke7v75tHj38uRQhAAAAAA68DUhZ/u/u9J/ltVnTdqelGSvxwkFQAAAAAzm3VXrx9PcvVoR6+/TvKDs0cCAAAAYAgzFX66e3eSpYGyAAAAADCgWdb4AQAAAGAdU/gBAAAAWFCzrvED8A9s37J5rSOMZdeBg2sdAQAAYK7M+AEAAABYUAo/AAAAAAtq6sJPVZ1XVbuP+rqvqi4fMhwAAAAA05t6jZ/u/qsk5ydJVW1K8oUk1w+UCwAAAIAZDXWr14uS/Jfu/puBng8AAACAGQ1V+HltkmuOd6KqdlTVclUt37v/8EDDAQAAALCSmQs/VfWIJK9I8rvHO9/dO7t7qbuXtm7bNOtwAAAAAIxpiBk//3OSW7v7iwM8FwAAAAADGaLwc0lOcJsXAAAAAGtnpsJPVT0qyUuSXDdMHAAAAACGMvV27knS3V9LcvpAWQAAAAAY0FC7egEAAACwzij8AAAAACyomW71Ahbb9i2b1zrCXG2Un2/XgYNrHQEAVs1GeX9mY/P5iocTM34AAAAAFpTCDwAAAMCCmnU795+sqjuq6vaquqaqThkqGAAAAACzmbrwU1VnJfmJJEvd/cwkm5K8dqhgAAAAAMxm1lu9Tk7yyKo6Ocmjktw1eyQAAAAAhjB14ae7v5DkF5N8PsndSb7S3R8+tl9V7aiq5apavnf/4emTAgAAADCRWW71emySVyZ5SpInJXl0VX3/sf26e2d3L3X30tZtm6ZPCgAAAMBEZrnV68VJ/mt3f6m7Dya5Lsm3DxMLAAAAgFnNUvj5fJLnV9WjqqqSvCjJnmFiAQAAADCrWdb4uTnJ7yW5Nclto+faOVAuAAAAAGZ08iwXd/fbk7x9oCwAAAAADGjW7dwBAAAAWKcUfgAAAAAW1Ey3egEbw/Ytm9c6AjNY7b+/XQcOrup4ACwmnz9YzzbC76fPZAzFjB8AAACABaXwAwAAALCgZir8VNUbq+r2qrqjqi4fKhQAAAAAs5u68FNVz0zyw0m2J3l2ku+pqqcNFQwAAACA2cwy4+cZST7R3V/r7kNJ/izJq4aJBQAAAMCsZin83J7kO6rq9Kp6VJKXJznn2E5VtaOqlqtq+d79h2cYDgAAAIBJTL2de3fvqap/l+SGJF9N8qkkh47Tb2eSnUnyjGdt6WnHAwAAAGAyMy3u3N1XdPdzuvs7kuxP8tlhYgEAAAAwq6ln/CRJVZ3Z3fuq6slJXp3k24aJBQAAAMCsZir8JLm2qk5PcjDJG7r7bwfIBAAAAMAAZir8dPcLhgoCAAAAwLBmWuMHAAAAgPVr1lu9AFgw27dsnuq6XQcODpwEgCFN+/oOrA2fyRiKGT8AAAAAC0rhBwAAAGBBKfwAAAAALKgVCz9VdWVV7auq249q21ZVN1TVZ0ffHzvfmAAAAABMapwZP1clufiYtrck+Uh3Py3JR0aPAQAAAFhHViz8dPeNSfYf0/zKJO8bHb8vyfcOnAsAAACAGU27xs/ju/vuJBl9P/NEHatqR1UtV9XyvfsPTzkcAAAAAJOa++LO3b2zu5e6e2nrtk3zHg4AAACAkWkLP1+sqicmyej7vuEiAQAAADCEaQs/f5jkstHxZUn+YJg4AAAAAAxlnO3cr0ny8STnVdXeqnpdkp9P8pKq+mySl4weAwAAALCOnLxSh+6+5ASnXjRwFgAAAAAGNPfFnQEAAABYGyvO+AGGt33L5rWOAINb7d/rXQcOrup4AOuFzxHAQ5n2NcJnq8Vlxg8AAADAglL4AQAAAFhQCj8AAAAAC2qc7dyvrKp9VXX7UW2vqao7qurBqlqab0QAAAAApjHOjJ+rklx8TNvtSV6d5MahAwEAAAAwjBV39eruG6vq3GPa9iRJVc0nFQAAAAAzm/saP1W1o6qWq2r53v2H5z0cAAAAACNzL/x0987uXurupa3bNs17OAAAAABG7OoFAAAAsKAUfgAAAAAW1DjbuV+T5ONJzquqvVX1uqp6VVXtTfJtSf64qj4076AAAAAATGacXb0uOcGp6wfOAgAAAMCA3OoFAAAAsKBWnPEDG8n2LZvXOgKwSjbCv/ddBw6udQRgFWyE1yOAlUz7Wubzzvpnxg8AAADAglL4AQAAAFhQCj8AAAAAC2qc7dyvrKp9VXX7UW2/UFV3VtWnq+r6qto635gAAAAATGqcGT9XJbn4mLYbkjyzu5+V5DNJfmbgXAAAAADMaMXCT3ffmGT/MW0f7u5Do4efSHL2HLIBAAAAMIMh1vj5oSR/cqKTVbWjqparavne/YcHGA4AAACAccxU+KmqtyU5lOTqE/Xp7p3dvdTdS1u3bZplOAAAAAAmcPK0F1bVZUm+J8mLuruHiwQAAADAEKYq/FTVxUl+Osl3dvfXho0EAAAAwBDG2c79miQfT3JeVe2tqtcl+dUkpyW5oap2V9WvzTknAAAAABNaccZPd19ynOYr5pAFAAAAgAENsasXAAAAAOvQ1Is7w7i2b9m81hEA1sRqv/7tOnBwVceD9cznD4DVMe3rrc8tq8eMHwAAAIAFpfADAAAAsKAUfgAAAAAW1DjbuV9ZVfuq6vaj2n6uqj492sr9w1X1pPnGBAAAAGBS48z4uSrJxce0/UJ3P6u7z0/ygSQ/O3QwAAAAAGazYuGnu29Msv+YtvuOevjoJD1wLgAAAABmNPV27lX1b5L8iyRfSfJdD9FvR5IdSfKEszZNOxwAAAAAE5p6cefuflt3n5Pk6iQ/9hD9dnb3Uncvbd2m8AMAAACwWobY1eu3k3zfAM8DAAAAwICmKvxU1dOOeviKJHcOEwcAAACAoay4xk9VXZPkhUkeV1V7k7w9ycur6rwkDyb5myQ/Os+QAAAAAExuxcJPd19ynOYr5pAFAAAAgAENscYPAAAAAOvQ1Nu5s3Ft37J5rSMAMAer/fq+68DBVR2PhyefWwAW07Sv79N+/ng4f04y4wcAAABgQSn8AAAAACyoFQs/VXVlVe2rqtuPc+5fVlVX1ePmEw8AAACAaY0z4+eqJBcf21hV5yR5SZLPD5wJAAAAgAGsWPjp7huT7D/OqV9O8uYkPXQoAAAAAGY31Ro/VfWKJF/o7k8NnAcAAACAgUy8nXtVPSrJ25K8dMz+O5LsSJInnLVp0uEAAAAAmNI0M36emuQpST5VVZ9LcnaSW6vqCcfr3N07u3upu5e2blP4AQAAAFgtE8/46e7bkpz5jcej4s9Sd98zYC4AAAAAZjTOdu7XJPl4kvOqam9VvW7+sQAAAACY1Yozfrr7khXOnztYGgAAAAAGM9WuXgAAAACsfwo/AAAAAAtq4sWdObHtWzavdQQAWDXe9wCA1ebzx+TM+AEAAABYUAo/AAAAAAtqnO3cr6yqfVV1+1Ft/7qqvlBVu0dfL59vTAAAAAAmNc6Mn6uSXHyc9l/u7vNHXx8cNhYAAAAAs1qx8NPdNybZvwpZAAAAABjQLGv8/FhVfXp0K9hjB0sEAAAAwCCmLfy8J8lTk5yf5O4kv3SijlW1o6qWq2r53v2HpxwOAAAAgElNVfjp7i929+HufjDJbyTZ/hB9d3b3Uncvbd22adqcAAAAAExoqsJPVT3xqIevSnL7ifoCAAAAsDZOXqlDVV2T5IVJHldVe5O8PckLq+r8JJ3kc0l+ZI4ZAQAAAJjCioWf7r7kOM1XzCELAAAAAAOaZVcvAAAAANYxhR8AAACABbXirV4b2fYtm9c6Amxouw4cnOo6//YAAADWBzN+AAAAABaUwg8AAADAglqx8FNVV1bVvqq6/Zj2H6+qv6qqO6rqnfOLCAAAAMA0xpnxc1WSi49uqKrvSvLKJM/q7n+c5BeHjwYAAADALFYs/HT3jUn2H9P8+iQ/390HRn32zSEbAAAAADOYdo2fpyd5QVXdXFV/VlXPGzIUAAAAALObdjv3k5M8Nsnzkzwvyfur6pu7u4/tWFU7kuxIkiectWnanAAAAABMaNoZP3uTXNdH7EryYJLHHa9jd+/s7qXuXtq6TeEHAAAAYLVMW/j5/SQXJUlVPT3JI5LcM1QoAAAAAGa34q1eVXVNkhcmeVxV7U3y9iRXJrlytMX7A0kuO95tXgAAAACsnRULP919yQlOff/AWQAAAAAY0LS3egEAAACwzin8AAAAACyoabdzn8qjq7J9y+bVHBLmateBg2sdYV1a7T+XaV9XNsrfn9dNAADYWNbTf6OY8QMAAACwoBR+AAAAABbUioWfqrqyqvaNtm7/RtvvVNXu0dfnqmr3fGMCAAAAMKlx1vi5KsmvJvmtbzR09z//xnFV/VKSrwyeDAAAAICZrFj46e4bq+rc452rqkryz5JcNGwsAAAAAGY16xo/L0jyxe7+7BBhAAAAABjOrIWfS5Jc81AdqmpHVS1X1fKXvnx4xuEAAAAAGNfUhZ+qOjnJq5P8zkP16+6d3b3U3UtnnL5p2uEAAAAAmNAsM35enOTO7t47VBgAAAAAhjPOdu7XJPl4kvOqam9VvW506rVZ4TYvAAAAANbOOLt6XXKC9h8YPA0AAAAAg5l1cWcAAAAA1imFHwAAAIAFteKtXrAWdh04uNYR2EAW/fdl2p9v+5bNqzretKbNCQDMz2p//gCOmMe/ITN+AAAAABaUwg8AAADAghpnO/crq2pfVd1+VNv5VfWJqtpdVctVtX2+MQEAAACY1Dgzfq5KcvExbe9M8o7uPj/Jz44eAwAAALCOrFj46e4bk+w/tjnJY0bH35TkroFzAQAAADCjaXf1ujzJh6rqF3OkePTtw0UCAAAAYAjTLu78+iQ/2d3nJPnJJFecqGNV7RitA7T8pS8fnnI4AAAAACY1beHnsiTXjY5/N8kJF3fu7p3dvdTdS2ecvmnK4QAAAACY1LSFn7uSfOfo+KIknx0mDgAAAABDWXGNn6q6JskLkzyuqvYmeXuSH07y7qo6OcnXk+yYZ0gAAAAAJrdi4ae7LznBqecOnAUAAACAAU17qxcAAAAA65zCDwAAAMCCWvFWr4ejXQcOrnUEgJl5LQNgLXkfenia9u99+5bNAycBvsGMHwAAAIAFpfADAAAAsKBWLPxU1ZVVta+qbj+q7dlV9fGquq2q/qiqHjPfmAAAAABMapwZP1clufiYtt9M8pbu/tYk1yf5qYFzAQAAADCjFQs/3X1jkv3HNJ+X5MbR8Q1Jvm/gXAAAAADMaNo1fm5P8orR8WuSnDNMHAAAAACGMm3h54eSvKGqbklyWpIHTtSxqnZU1XJVLX/py4enHA4AAACASZ08zUXdfWeSlyZJVT09yXc/RN+dSXYmydKzT+lpxgMAAABgclPN+KmqM0ffT0ryr5L82pChAAAAAJjdONu5X5Pk40nOq6q9VfW6JJdU1WeS3JnkriTvnW9MAAAAACa14q1e3X3JCU69e+AsAAAAAAxo2sWdAQAAAFjnFH4AAAAAFtRUu3pN6++6s+vAwdUcEgBmNu171/YtmwdOAjD9axKsZ6v9e+09mocTM34AAAAAFpTCDwAAAMCCGmc793Oq6qNVtaeq7qiqN47at1XVDVX12dH3x84/LgAAAADjGmfGz6Ekb+ruZyR5fpI3VNW3JHlLko9099OSfGT0GAAAAIB1YsXCT3ff3d23jo7vT7InyVlJXpnkfaNu70vyvfMKCQAAAMDkJlrjp6rOTXJBkpuTPL67706OFIeSnDl0OAAAAACmN3bhp6pOTXJtksu7+74JrttRVctVtXzv/sPTZAQAAABgCmMVfqpqc44Ufa7u7utGzV+sqieOzj8xyb7jXdvdO7t7qbuXtm7bNERmAAAAAMYwzq5eleSKJHu6+11HnfrDJJeNji9L8gfDxwMAAABgWieP0efCJJcmua2qdo/a3prk55O8v6pel+TzSV4zn4gAAAAATGPFwk9335SkTnD6RcPGAQAAAGAoE+3qBQAAAMDGofADAAAAsKDGWeMHAOZm14GDax1hbhb5Z0uS7Vs2T3XdtH8uqz3eapv252PtbZTfMeD/s9r/br3Gs5bM+AEAAABYUAo/AAAAAAtqxcJPVZ1TVR+tqj1VdUdVvXHU/prR4weramn+UQEAAACYxDhr/BxK8qbuvrWqTktyS1XdkOT2JK9O8uvzDAgAAADAdFYs/HT33UnuHh3fX1V7kpzV3TckSVXNNyEAAAAAU5lojZ+qOjfJBUlunkcYAAAAAIYzduGnqk5Ncm2Sy7v7vgmu21FVy1W1fO/+w9NkBAAAAGAKYxV+qmpzjhR9ru7u6yYZoLt3dvdSdy9t3bZpmowAAAAATGGcXb0qyRVJ9nT3u+YfCQAAAIAhjLOr14VJLk1yW1XtHrW9NcmWJP9HkjOS/HFV7e7ul80nJgAAAACTGmdXr5uSnGjrruuHjQMAAADAUCba1QsAAACAjUPhBwAAAGBBjbPGDwDAP7DrwMGFHm+1LfrPBwCsDTN+AAAAABaUwg8AAADAglqx8FNV51TVR6tqT1XdUVVvHLX/QlXdWVWfrqrrq2rr/OMCAAAAMK5xZvwcSvKm7n5GkucneUNVfUuSG5I8s7ufleQzSX5mfjEBAAAAmNSKhZ/uvru7bx0d359kT5KzuvvD3X1o1O0TSc6eX0wAAAAAJjXRGj9VdW6SC5LcfMypH0ryJ8NEAgAAAGAIYxd+qurUJNcmuby77zuq/W05cjvY1Se4bkdVLVfV8r37D8+aFwAAAIAxjVX4qarNOVL0ubq7rzuq/bIk35Pkf+3uPt613b2zu5e6e2nrtk1DZAYAAABgDCev1KGqKskVSfZ097uOar84yU8n+c7u/tr8IgIAAAAwjRULP0kuTHJpktuqaveo7a1JfiXJliQ3HKkN5RPd/aNzSQkAAADAxFYs/HT3TUnqOKc+OHwcAAAAAIYy0a5eAAAAAGwcCj8AAAAAC2qcNX4AFtL2LZtXdbxdBw6u6ngAAKwPq/k5cLU/47L+mfEDAAAAsKAUfgAAAAAW1IqFn6o6p6o+WlV7quqOqnrjqP3nqurTVbW7qj5cVU+af1wAAAAAxjXOjJ9DSd7U3c9I8vwkb6iqb0nyC939rO4+P8kHkvzsHHMCAAAAMKEVCz/dfXd33zo6vj/JniRndfd9R3V7dJKeT0QAAAAApqOh+ooAACAASURBVDHRrl5VdW6SC5LcPHr8b5L8iyRfSfJdA2cDAAAAYAZjL+5cVacmuTbJ5d+Y7dPdb+vuc5JcneTHTnDdjqparqrle/cfHiIzAAAAAGMYq/BTVZtzpOhzdXdfd5wuv53k+453bXfv7O6l7l7aum3T9EkBAAAAmMg4u3pVkiuS7Onudx3V/rSjur0iyZ3DxwMAAABgWuOs8XNhkkuT3FZVu0dtb03yuqo6L8mDSf4myY/OJyIAAAAA01ix8NPdNyWp45z64PBxAAAAABjK2Is7AwAAALCxKPwAAAAALKhx1vgZzKOrsn3L5tUcEmDd2Civf7sOHFzrCAAATGnaz3Ib5bMqkzPjBwAAAGBBKfwAAAAALKgVCz9VdU5VfbSq9lTVHVX1xmPO/8uq6qp63PxiAgAAADCpcdb4OZTkTd19a1WdluSWqrqhu/+yqs5J8pIkn59rSgAAAAAmtuKMn+6+u7tvHR3fn2RPkrNGp385yZuT9NwSAgAAADCVidb4qapzk1yQ5OaqekWSL3T3p+aQCwAAAIAZjb2de1WdmuTaJJfnyO1fb0vy0jGu25FkR5I8+axV3T0eAAAA4GFtrBk/VbU5R4o+V3f3dUmemuQpST5VVZ9LcnaSW6vqCcde2907u3upu5fOOH3TcMkBAAAAeEgrTsGpqkpyRZI93f2uJOnu25KceVSfzyVZ6u575pQTAAAAgAmNM+PnwiSXJrmoqnaPvl4+51wAAAAAzGjFGT/dfVOSWqHPuUMFAgAAAGAYE+3qBQAAAMDGofADAAAAsKDsrw7A/8/2LZvXOsLc7DpwcK0jAMBUpn1/Xu33vo2Sk39o2r+DRf7suCjM+AEAAABYUAo/AAAAAAtqxcJPVZ1TVR+tqj1VdUdVvXHU/q+r6gu2eAcAAABYn8ZZ4+dQkjd1961VdVqSW6rqhtG5X+7uX5xfPAAAAACmtWLhp7vvTnL36Pj+qtqT5Kx5BwMAAABgNhOt8VNV5ya5IMnNo6Yfq6pPV9WVVfXYgbMBAAAAMIOxCz9VdWqSa5Nc3t33JXlPkqcmOT9HZgT90gmu21FVy1W1/KUvHx4gMgAAAADjGKvwU1Wbc6Toc3V3X5ck3f3F7j7c3Q8m+Y0k2493bXfv7O6l7l464/RNQ+UGAAAAYAXj7OpVSa5Isqe733VU+xOP6vaqJLcPHw8AAACAaY2zq9eFSS5NcltV7R61vTXJJVV1fpJO8rkkPzKXhAAAAABMZZxdvW5KUsc59cHh4wAAAAAwlIl29QIAAABg41D4AQAAAFhQ46zxA8AaetmTzl/V8T501+6VO21Q27dsXusIALCqNsp730bJuevAwbWOsO6s9p/JRvldWU/M+AEAAABYUAo/AAAAAAtqxcJPVZ1TVR+tqj1VdUdVvfGocz9eVX81an/nfKMCAAAAMIlx1vg5lORN3X1rVZ2W5JaquiHJ45O8MsmzuvtAVZ05z6AAAAAATGbFwk93353k7tHx/VW1J8lZSX44yc9394HRuX3zDAoAAADAZCZa46eqzk1yQZKbkzw9yQuq6uaq+rOqet4JrtlRVctVtfylLx+eNS8AAAAAYxq78FNVpya5Nsnl3X1fjswWemyS5yf5qSTvr6o69rru3tndS929dMbpmwaKDQAAAMBKxir8VNXmHCn6XN3d142a9ya5ro/YleTBJI+bT0wAAAAAJjXOrl6V5Ioke7r7XUed+v0kF436PD3JI5LcM4+QAAAAAExunF29LkxyaZLbqmr3qO2tSa5McmVV3Z7kgSSXdXfPJyYAAAAAkxpnV6+bkvyDtXtGvn/YOAAAAAAMZaJdvQAAAADYOBR+AAAAABbUOGv8ALABfeiu3St3AgCADWTXgYNTXbd9y+aBk2wcZvwAAAAALCiFHwAAAIAFteKtXlV1TpLfSvKEJA8m2dnd766q30ly3qjb1iT3dvf5c0sKAAAAwETGWePnUJI3dfetVXVakluq6obu/uff6FBVv5TkK/MKCQAAAMDkViz8dPfdSe4eHd9fVXuSnJXkL5OkqirJP0ty0RxzAgAAADChidb4qapzk1yQ5Oajml+Q5Ivd/dkTXLOjqparavlLXz48bU4AAAAAJjR24aeqTk1ybZLLu/u+o05dkuSaE13X3Tu7e6m7l844fdP0SQEAAACYyDhr/KSqNudI0efq7r7uqPaTk7w6yXPnEw8AAACAaa0442e0hs8VSfZ097uOOf3iJHd29955hAMAAABgeuPc6nVhkkuTXFRVu0dfLx+de20e4jYvAAAAANbOOLt63ZSkTnDuB4YOBAAAAMAwJtrVCwAAAICNQ+EHAAAAYEGNtasXAAAAPNxt37J5rSOsaNeBg2sdYV16OP+5mPEDAAAAsKAUfgAAAAAW1IqFn6o6p6o+WlV7quqOqnrjqP38qvrEaHv35araPv+4AAAAAIxrnDV+DiV5U3ffWlWnJbmlqm5I8s4k7+juP6mql48ev3B+UQEAAACYxIqFn+6+O8ndo+P7q2pPkrOSdJLHjLp9U5K75hUSAAAAgMlNtKtXVZ2b5IIkNye5PMmHquoXc+SWsW8/wTU7kuxIkiefZRMxAAAAgNUy9uLOVXVqkmuTXN7d9yV5fZKf7O5zkvxkkiuOd1137+zupe5eOuP0TUNkBgAAAGAMYxV+qmpzjhR9ru7u60bNlyX5xvHvJrG4MwAAAMA6Ms6uXpUjs3n2dPe7jjp1V5LvHB1flOSzw8cDAAAAYFrjLLpzYZJLk9xWVbtHbW9N8sNJ3l1VJyf5ekbr+AAAAACwPoyzq9dNSeoEp587bBwAAAAAhjL24s4AAAAAbCwKPwAAAAALapw1fgbzd93ZdeDgag7Jw8z2LZvXOgKc0MuedP5aRwAAAB5mzPgBAAAAWFAKPwAAAAALasXCT1WdU1Ufrao9VXVHVb1x1P7sqvp4Vd1WVX9UVY+Zf1wAAAAAxjXOjJ9DSd7U3c9I8vwkb6iqb0nym0ne0t3fmuT6JD81v5gAAAAATGrFwk93393dt46O70+yJ8lZSc5LcuOo2w1Jvm9eIQEAAACY3ERr/FTVuUkuSHJzktuTvGJ06jVJzjnBNTuqarmqlu/df3j6pAAAAABMZOzCT1WdmuTaJJd3931JfihHbvu6JclpSR443nXdvbO7l7p7aeu2TUNkBgAAAGAMJ4/Tqao250jR5+ruvi5JuvvOJC8dnX96ku+eV0gAAAAAJjfOrl6V5Ioke7r7XUe1nzn6flKSf5Xk1+YVEgAAAIDJjXOr14VJLk1yUVXtHn29PMklVfWZJHcmuSvJe+eYEwAAAIAJrXirV3fflKROcPrdw8YBAAAAYCgT7eoFAAAAwMYx1uLOsFHsOnBwrSMslO1bNq91BJJ86K7dax0BAIANYrU/w/tvsPXPjB8AAACABaXwAwAAALCgFH4AAAAAFtSKhZ+qOqWqdlXVp6rqjqp6x6h9W1XdUFWfHX1/7PzjAgAAADCucWb8HEhyUXc/O8n5SS6uqucneUuSj3T305J8ZPQYAAAAgHVixcJPH/HV0cPNo69O8sok7xu1vy/J984lIQAAAABTGWuNn6raVFW7k+xLckN335zk8d19d5KMvp95gmt3VNVyVS3fu//wULkBAAAAWMFYhZ/uPtzd5yc5O8n2qnrmuAN0987uXurupa3bNk2bEwAAAIAJTbSrV3ffm+RjSS5O8sWqemKSjL7vGzwdAAAAAFMbZ1evM6pq6+j4kUlenOTOJH+Y5LJRt8uS/MG8QgIAAAAwuZPH6PPEJO+rqk05Uih6f3d/oKo+nuT9VfW6JJ9P8po55gQAAABgQisWfrr700kuOE77l5O8aB6hAAAAAJjdRGv8AAAAALBxjHOr12AeXZXtWzav5pCrateBg2sdAQa1UX6nF/l1BQAA1jOfxYc1j/8GM+MHAAAAYEEp/AAAAAAsKIUfAAAAgAW1YuGnqk6pql1V9amquqOq3jFqf83o8YNVtTT/qAAAAABMYpzFnQ8kuai7v1pVm5PcVFV/kuT2JK9O8uvzDAgAAADAdFYs/HR3J/nq6OHm0Vd3954kqar5pQMAAABgamOt8VNVm6pqd5J9SW7o7pvHHaCqdlTVclUtf+nLh6fNCQAAAMCExir8dPfh7j4/ydlJtlfVM8cdoLt3dvdSdy+dcfqmaXMCAAAAMKGJdvXq7nuTfCzJxXNJAwAAAMBgxtnV64yq2jo6fmSSFye5c97BAAAAAJjNODN+npjko1X16SSfzJE1fj5QVa+qqr1Jvi3JH1fVh+YZFAAAAIDJjLOr16eTXHCc9uuTXD+PUAAAAADMbqI1fgAAAADYOFac8cP4tm/ZvNYRxrLrwMFVHW+aP5fVzsjGttq/L//bU563quMBAAAPD/OoK5jxAwAAALCgFH4AAAAAFpTCDwAAAMCCWrHwU1WnVNWuqvpUVd1RVe8Ytf9CVd1ZVZ+uquurauv84wIA/D/t3X2QXXd93/H3l7VsC4IRBjkWthtDATcdx8ggVLcTHiJUotKMKWGgYUJChnRUaEmBlCYwBLDroSU8hJJOGsYFpx6aUNwQaOKOaytNDPVMLNkGSdhFPBUD5sk8VFCVqTDi2z/ukVmvzu/hrPZK2vX7NbPju7v3o99vz372d849vvdcSZIk9ep5xs9hYFtmPgnYDOyIiMuAXcDFmXkJ8GngdfObpiRJkiRJkqZqnvjJmUPDp+uGj8zMmzLzB8PXbwXOn9McJUmSJEmStAxd1/iJiIWI2AvcC+zKzN1L7vJS4IZCdmdE3B4Rt3/jW0eOb7aSJEmSJEnq1nXiJzOPZOZmZs/q2RoRFx/9XkS8HvgB8IeF7NWZuSUzt2x81MJKzFmSJEmSJEkdJr2rV2YeBG4GdgBExEuAnwN+MTNzxWcnSZIkSZKkZet5V6+NR9+xKyLWA9uBAxGxA/hN4PLM/N58pylJkiRJkqSpTuu4zybg2ohYYHai6LrMvD4iPgucAeyKCIBbM/Nl85uqJEmSJEmSpmie+MnM/cClI19//FxmJEmSJEmSpBUx6Ro/kiRJkiRJWj16Xuq1Yv5vJnsO3zc5t/WMdXOYzYPXatieJ3qOy+mlVr83PPapJ3S8qz5/27Jyew6v8EQ02WpYNyVJkqQxPuNHkiRJkiRpjfLEjyRJkiRJ0hrliR9JkiRJkqQ1qnniJyLOjIg9EbEvIu6KiCuHr18VEfsjYm9E3BQRj5n/dCVJkiRJktSr5xk/h4FtmfkkYDOwIyIuA96WmZdk5mbgeuCNc5ynJEmSJEmSJmq+q1dmJnBo+HTd8JGZ+d1Fd3sYkCs/PUmSJEmSJC1X19u5R8QCcAfweOD3MnP38PU3A78MfAf4mUJ2J7AT4NzzFlZgypIkSZIkSerRdXHnzDwyvKTrfGBrRFw8fP31mXkB8IfAKwrZqzNzS2Zu2XC2J34kSZIkSZJOlEnv6pWZB4GbgR1LvvVHwPNXaE6SJEmSJElaAT3v6rUxIjYMt9cD24EDEfGERXe7HDgwnylKkiRJkiRpOXqu8bMJuHa4zs9DgOsy8/qI+GBEXAT8EPgC8LI5zlOSJEmSJEkT9byr137g0pGv+9IuSZIkSZKkU9ika/xIkiRJkiRp9eh6O/eTbc/h+072FE5JW89Yd7KnsGa4LVe3n33M5hM63lWfv+2EjidJkiRJy+UzfiRJkiRJktYoT/xIkiRJkiStUT1v535mROyJiH0RcVdEXLnk+6+JiIyIR89vmpIkSZIkSZqq5xo/h4FtmXkoItYBt0TEDZl5a0RcAPxd4ItznaUkSZIkSZImaz7jJ2cODZ+uGz5y+PydwG8s+lySJEmSJEmniK5r/ETEQkTsBe4FdmXm7oi4HPhyZu6b6wwlSZIkSZK0LF1v556ZR4DNEbEB+FBEXAK8Hnh2KxsRO4GdAOeet3AcU5UkSZIkSdIUk97VKzMPAjcDzwUeC+yLiLuB84GPRcS5I5mrM3NLZm7ZcLYnfiRJkiRJkk6Unnf12jg804eIWA9sBz6emedk5oWZeSFwD/DkzPzaXGcrSZIkSZKkbj0v9doEXBsRC8xOFF2XmdfPd1qSJEmSJEk6Xs0TP5m5H7i0cZ8LV2pCkiRJkiRJWhmTrvEjSZIkSZKk1cMTP5IkSZIkSWtU19u569S05/B9J3sK0oraesa6EzrejV/Zu6zcnsMrPBEty4nuix583M9KM663p4blrkn+/iT5jB9JkiRJkqQ1yhM/kiRJkiRJa1TzxE9EnBkReyJiX0TcFRFXDl+/IiK+HBF7h4/nzH+6kiRJkiRJ6tVzjZ/DwLbMPBQR64BbIuKG4XvvzMy3z296kiRJkiRJWq7miZ/MTODQ8Om64SPnOSlJkiRJkiQdv65r/ETEQkTsBe4FdmXm7uFbr4iI/RFxTUQ8cm6zlCRJkiRJ0mRdJ34y80hmbgbOB7ZGxMXA7wN/HdgMfBV4x1g2InZGxO0RcfvBbx9ZoWlLkiRJkiSpZdK7emXmQeBmYEdmfn04IfRD4N8DWwuZqzNzS2Zu2XD2wnFPWJIkSZIkSX163tVrY0RsGG6vB7YDByJi06K7PQ+4cz5TlCRJkiRJ0nL0vKvXJuDaiFhgdqLousy8PiLeFxGbmV3o+W7gH89vmpIkSZIkSZqq51299gOXjnz9l+YyI0mSJEmSJK2ISdf4kSRJkiRJ0urhiR9JkiRJkqQ1qucaP5J0QvzsYzaf7Cl02XrGupM9BemUsOfwfSd7CpJOYa4Rp4YT/XvwOEk69fiMH0mSJEmSpDXKEz+SJEmSJElrVPPET0ScGRF7ImJfRNwVEVcu+t6vRcSnhq+/db5TlSRJkiRJ0hQ91/g5DGzLzEMRsQ64JSJuANYDzwUuyczDEXHOPCcqSZIkSZKkaZonfjIzgUPDp+uGjwReDrwlMw8P97t3XpOUJEmSJEnSdF3X+ImIhYjYC9wL7MrM3cATgadFxO6I+EhEPHWeE5UkSZIkSdI0XW/nnplHgM0RsQH4UERcPGQfCVwGPBW4LiIeNzxD6H4RsRPYCXDueQsrOXdJkiRJkiRVTHpXr8w8CNwM7ADuAf4kZ/YAPwQePZK5OjO3ZOaWDWd74keSJEmSJOlE6XlXr43DM32IiPXAduAA8GFg2/D1JwKnA9+c31QlSZIkSZI0Rc9LvTYB10bEArMTRddl5vURcTpwTUTcCXwfeMnSl3lJkiRJkiTp5Ol5V6/9wKUjX/8+8OJ5TEqSJEmSJEnHb9I1fiRJkiRJkrR6eOJHkiRJkiRpjep6O3dJOpVd9fnblpXbc3iFJyJJ0hq05/B9J3sKWkXsy7itZ6w72VPQg5jP+JEkSZIkSVqjPPEjSZIkSZK0RjVf6hURZwIfBc4Y7v/HmfmmiPgAcNFwtw3AwczcPLeZSpIkSZIkaZKea/wcBrZl5qGIWAfcEhE3ZOY/PHqHiHgH8J15TVKSJEmSJEnTNU/8ZGYCh4ZP1w0fefT7ERHAC4Ft85igJEmSJEmSlqfrGj8RsRARe4F7gV2ZuXvRt58GfD0zPzOPCUqSJEmSJGl5uk78ZOaR4fo95wNbI+LiRd9+EfD+UjYidkbE7RFx+8FvHzm+2UqSJEmSJKnbpHf1ysyDwM3ADoCIOA34eeADlczVmbklM7dsOHvhOKYqSZIkSZKkKZonfiJiY0RsGG6vB7YDB4ZvbwcOZOY985uiJEmSJEmSlqPnXb02AddGxAKzE0XXZeb1w/d+gcrLvCRJkiRJknTy9Lyr137g0sL3fmWlJyRJkiRJkqSVMekaP5IkSZIkSVo9PPEjSZIkSZK0RvVc40eSJnnDY596sqcgSZIknTL2HL7vZE9BD2I+40eSJEmSJGmN8sSPJEmSJEnSGtU88RMRZ0bEnojYFxF3RcSVw9c3R8StEbE3Im6PiK3zn64kSZIkSZJ69Vzj5zCwLTMPRcQ64JaIuAH4l8CVmXlDRDwHeCvwzPlNVZIkSZIkSVM0T/xkZgKHhk/XDR85fJw1fP0RwFfmMUFJkiRJkiQtT9e7ekXEAnAH8Hjg9zJzd0S8CrgxIt7O7CVjf2d+05QkSZIkSdJUXRd3zswjmbkZOB/YGhEXAy8HXp2ZFwCvBt47lo2IncM1gG4/+O0jKzVvSZIkSZIkNUx6V6/MPAjcDOwAXgL8yfCt/wyMXtw5M6/OzC2ZuWXD2QvHMVVJkiRJkiRN0fOuXhsjYsNwez2wHTjA7Jo+zxjutg34zLwmKUmSJEmSpOl6rvGzCbh2uM7PQ4DrMvP6iDgIvCsiTgP+H7BzjvOUJEmSJEnSRD3v6rUfuHTk67cAT5nHpCRJkiRJknT8Jl3jR5IkSZIkSauHJ34kSZIkSZLWqJ5r/EjSCXHV52872VOQJEmSpDXFZ/xIkiRJkiStUZ74kSRJkiRJWqOaJ34i4syI2BMR+yLiroi4cvj6kyLiryLiExHxZxFx1vynK0mSJEmSpF49z/g5DGzLzCcBm4EdEXEZ8B7gtZn5U8CHgH8xv2lKkiRJkiRpquaJn5w5NHy6bvhI4CLgo8PXdwHPn8sMJUmSJEmStCxd1/iJiIWI2AvcC+zKzN3AncDlw11eAFwwnylKkiRJkiRpObpO/GTmkczcDJwPbI2Ii4GXAv80Iu4AHg58fywbETsj4vaIuP3gt4+s1LwlSZIkSZLUMOldvTLzIHAzsCMzD2TmszPzKcD7gc8VMldn5pbM3LLh7IXjnrAkSZIkSZL69Lyr18aI2DDcXg9sBw5ExDnD1x4C/Bbw7nlOVJIkSZIkSdP0PONnE/CXEbEfuI3ZNX6uB14UEZ8GDgBfAf5gftOUJEmSJEnSVKe17pCZ+4FLR77+LuBd85iUJEmSJEmSjt+ka/xIkiRJkiRp9fDEjyRJkiRJ0hoVmXniBov4BvCFwrcfDXxzGf+sudWbWw1zNGfO3OrLrYY5mjNnbvXlVsMczZkzt/pyq2GO5lZH7icyc+PodzLzlPgAbjf34MqthjmaM2du9eVWwxzNmTO3+nKrYY7mzJlbfbnVMEdzqz/nS70kSZIkSZLWKE/8SJIkSZIkrVGn0omfq8096HKrYY7mzJlbfbnVMEdz5sytvtxqmKM5c+ZWX241zNHcKs+d0Is7S5IkSZIk6cQ5lZ7xI0mSJEmSpJW0nCtCr+QHsAP4FPBZ4LUTctcA9wJ3TshcAPwl8EngLuCVnbkzgT3AviF35cSfcQH4OHD9hMzdwCeAvUy4cjewAfhj4MDwc/7tjsxFwzhHP74LvKpzvFcP2+RO4P3AmZ25Vw6Zu2pjjf2egbOBXcBnhv8+sjP3gmG8HwJbJoz3tmF77gc+BGzozF01ZPYCNwGPmdJj4DVAAo/uHO8K4MuLfo/P6R0P+LXh7/Au4K2d431g0Vh3A3s7c5uBW492G9jamXsS8FfD38WfAWctyYz+fbf6UslV+1LJVftSyVX7Usq1+lIZr9qX2ni1vlTGq/alkqv2pZJr9WV0Xe/oSynX6ksp1+pLKdfqS3W/VelLabxiX2pjNbpSGqvVlVKu1ZVSrtqVRfkH7MtbXankmvuiQq65LyrkmvuisVyrK5Xxil1pjVfrS2W85r6okGvuiwq5Zl8YOYbr6Ush13PsMpbrOXYZy/UcuxyT6+lLYbxmX0rjtfpSGK+1voxleo5bxnI9XTnmuL2zK2O5nq6M5Xq6Mpbr6UrxcUmjK2Pj9XRldLyOroyN13OcO5br6ctYrnXcMvp4rdWXSq513FLKtY5bSrnWcUv18WipL5Xxqn2pjVfrS2W8Yl8qmdZxSynXddxyzO+0507z+mC2M/0c8DjgdGYHY3+zM/t04MlMO/GzCXjycPvhwKd7xgMC+LHh9jpgN3DZhHF/Hfgjpp/4GT3IauSuBf7RcPv0pX+Mnb+TrwE/0XHf84DPA+uHz68DfqUjdzGzkz4PBU4D/hx4Qu/vGXgrw0lC4LXAb3fmfnL4A7qZ8g5xLPds4LTh9m9PGO+sRbf/GfDu3h4zezB7I/CFsR4UxrsCeE1j24/lfmb4HZwxfH5O7zwXff8dwBs7x7sJ+HvD7ecAN3fmbgOeMdx+KXDVkszo33erL5VctS+VXLUvlVy1L6Vcqy+V8ap9qeSqfanNs9aXynjVvlRyrb6MrusdfSnlWn0p5Vp9KeVafSnutxp9KY1X7Esl0+pKc99a6EppvFZXSrlqVxblH7Avb3Wlkmvuiwq55r6okGvui8Zyra5Uxit2pZFr7otK86z1pTJec19UyDX7wsgxXE9fCrmeY5exXM+xy1iu59jlmFxPXwrjNftSyPUcu4zOs9aXwlg9xy1juZ6uHHPc3tmVsVxPV8ZyPV0Zy/V0ZfRxSUdXxsbr6cpYrqcr1cdPY12pjNfTl7Fc175o+P79j9d6+lLIde2LRnJd+6KRXNe+aGmupy+F8Zp9KeS69kVj82z1ZWSsrv3QSK67K4s/TvZLvbYCn83M/5WZ3wf+E/DcnmBmfhT49pTBMvOrmfmx4fb/YXaG9byOXGbmoeHTdcNH9owZEecDfx94z5S5LkdEnMXsAfN7ATLz+5l5cOI/8yzgc5n5hc77nwasj4jTmJ3I+UpH5ieBWzPze5n5A+AjwPPG7lj4PT+X2YLJ8N9/0JPLzE9m5qdqEyvkbhrmCbOzsud35r676NOHMdKZSo/fCfzGWKaRqyrkXg68JTMPD/e5d8p4ERHAC5k946snl8BZw+1HMNKZQu4i4KPD7V3A85dkSn/f1b6Ucq2+VHLVvlRy1b401q9iX45j3Svlqn1pjVfqSyVX7Usl1+pLaV1v9WU019GXUq7Vl1Ku1ZfafqvWl8n7u0qm1ZXqWJWulHKtrpRy1a4Mcxnblzf3RWO5nn1RIdfcFxVyzX1R5Vilui9a7jFOIdfcF9XGq+2LCrnmvqiQa/aloNmXMT19KeSafSnkmn2pqPZlhTX7UlPry4hmb0OVewAACoVJREFUVwqqXakct1e7Usq1ulLJVbtSyVW70nhcUuzKch/PVHLVrrTGK3Wlkqv2pZKbsrYsfrw2ZW25PzdxbVmcm7K2LM5NWVuWPh7tXVumPo4dy01ZW44Zr2NtWZyZsrYszi1rP3SyT/ycB3xp0ef30PGAZCVExIXApcz+j1/P/RciYi+zl5/sysyuHPBvmBX1hxOnmMBNEXFHROzszDwO+AbwBxHx8Yh4T0Q8bOK4v0DfTpDM/DLwduCLwFeB72TmTR3RO4GnR8SjIuKhzM5wXjBhjj+emV8d5vBV4JwJ2eP1UuCG3jtHxJsj4kvALwJv7MxcDnw5M/ctY36viIj9EXFNRDyyM/NE4GkRsTsiPhIRT5045tOAr2fmZzrv/yrgbcN2eTvwus7cncDlw+0XUOnMkr/v7r5MXRc6ctW+LM319mVxbkpfRubZ1Zclue6+FLZLsy9Lct19WZJr9qWwrjf7stz9QUdutC+lXKsvY7mevlTmWexLIdPsSmObFLtSyDW7Usj1rC1j+/KetWW5xwCtXGltGc11rC3H5DrXltI8W2vLWK5nbaltl9raMpbrWVvGcj19GTuG6+nLco79enKlvozmOvpyTK6zL6V5tvoyluvpS227lPoylunpyliu1ZXScXurK8s93u/JjXWlmGt0ZTTX0ZXaPGtdKeVaXWltl1JXSrlWX0q57uNcHvh4bcrjou7HeZ251uOiB+Q61pZjcp1rS2mevY+LFuemPC4a2y6t49zFmSmPiRbnpnTlR7LjaUHz+hgm+p5Fn/8S8G8n5C9kwku9FuV+DLgD+PllZDcwu57ExR33/Tng3w23n8m0l3o9ZvjvOcxeAvf0jswW4AfA3xo+fxedT/0a7n868E1mC0jP/R8J/AWwkdn/Of0w8OLO7K8CH2N2tvLdwDt7f8/AwSXf/99T+kH76fWl3OuZvZY1pvaR2R/y6LWhFueYPWtqN/CI4fO7KT+9ful2+XFmTwN8CPBm4JrO3J3A7zJ7GcRWZi/fO+ZnrGyX3wf++YTf3+8Czx9uvxD4887c32D2lMg7gDcB3yrkHvD3PaEvo+tCR19KuVZfiutQoy/35yb2Zel26e3L0lxvX0rbpdWXpeP19mVprqsvw33vX9d7+7I019uXSq7al1Ku1ZcluUt6+zKyXXr7sjjT1ZXKNql2ZWS8rq6M5KpdobAvb3WllGt1pSM32pVWrtSVsRwda0tlu1S7UslV+9KxXUb7Uhmv2pdKrrm2MHIM1+pLKdfqS0euuLbUcqW+VH6+5tpSyDXXlkKuub40tkupL2NjNdeWQq61towet7e6Usp1rC2tXGltaT6+GOtKIfe2Vlcq26W1tpRyrbWltV1KXSmN11pbSrne49wHPF5r9aWU61lbGrnWcW7xceVYX8ZyTDvOXbpdeo9bluZ6j3NL26V47DIyVu8x7tJc9zHuA/6dnjvN64PZhaxuXFKC103IX8jEEz/MTlDcCPz6ccz7TfS9fv1fM3sW093MXpP3PeA/LmO8KzrHOxe4e9HnTwP+64RxngvcNOH+LwDeu+jzX2Y4SJr48/0r4J/0/p6ZXWxr03B7E/CpKf1gGSd+gJcwu4jWQ5fTR2avxyx97/4c8FPM/k/03cPHD5g9o+rcieN1fw/4b8AzF33+OWBj53Y5Dfg6cP6E3993GBZQZovqd5fxMzwR2DPy9WP+vnv6Mpbr6Usp1+pLbbxaX5bmevvSMd7oti5sz2ZfKtul2pfCeM2+dPx8o31Zcp83MbtwYNf6sjTX05dSrtWX2ni1vozk3tDTl47xRvsysi271pbCNmmuLSPjda0tjZ/tmK5Q2Je3ulLKtbpSy9W60hqv1JVC7oOtrnSOd0xXKtuz2pfGdin2pTJetS+dP1/P2nIFy1tbrmB5a8v9uVpfWuOV+lLILWdtGRvvmL5UtufU9WXxdulaXxaNNXVtGfvZxtaW0eP2VldKuVZXarlaV1rjlbpSyP33Vlc6xzumK5Xt2VpbatultraUxmutLT0/X3FtYcnjtVZfSrlWX2q5Wl9a45X6MpZj2uOi2njH9KWyPXsfF41tl9Zx7tKxeh8T1X625n7o6MfJfqnXbcATIuKxEXE6s6cw/em8BouIYPZ6yk9m5u9MyG2MiA3D7fXAdmZXM6/KzNdl5vmZeSGzn+0vMvPFHeM9LCIefvQ2s4to3dkx3teAL0XERcOXngX8z1ZukRcx7el/XwQui4iHDtv2Wcyur9EUEecM//1rzJ65MGXcP2W24DD8979MyE4WETuA3wQuz8zvTcg9YdGnl9PXmU9k5jmZeeHQm3uYXbj2ax3jbVr06fPo6Mzgw8C24d94Ij86q9xjO3AgM+/pvD/MXr/6jOH2NmbvQtC0qDMPAX6L2TPFFn+/9Pdd7ctxrAujuVZfKrlqX8ZyPX2pjFftS2W7VPvS2J7FvlRy1b5Ufr5WX0rreqsvy9oflHIdfSnlWn0Zy328oy+l8Yp9qWyTVldq27LWlVKu1ZXSz1btSmVfXu3Kco8BSrlWVyq5alcKuee3ulIZr7q2VLZLtS+N7VnsSyVX7Uvl52utLaVjuNbasqxjv1KuY20p5Vpry1juto61pTRea19U2i6t9aW2PUf7Usm01pbSz9ZaW0rH7a21ZVnH+6Vcx9pSyrXWlrHcxzrWltJ4rbWltF1aa0tte9bWllKutbaUfr5qXxZZ+nit93HR1Md5o7kJj4uW5nofF92fm/i4aOl4vY+Llm6X3sdFY9uz9bhoaab3MdHSn623Kw/Uc3Zonh/Mru/yaWZn014/Ifd+ZteVuY9ZCX61I/PTzF6De/St5I55a7dC7hJmb+W5n1lpRq/S3fg3nknnS72YvfZzHz96y9kp22Uzs7eD28+suKNvLzuSeyjwLYan0k0Y70pmf7h3Au9juAJ6R+5/MFsc9wHPmvJ7Bh7F7P8YfGb479mduecNtw8zOxt7Y2fus8yuRXW0M2PvWjCW++CwXfYze6u986b2mPLTpcfGex+zt/Xbz2wnsKkzdzqz//t5J7OX323rnSfwH4CXTfz9/TSzpybuY/b0zad05l7JbK34NPAWjn0q8ujfd6svlVy1L5VctS+VXLUvpVyrL5Xxqn2p5Kp9qc2z1pfKeNW+VHKtvoyu6x19KeVafSnlWn0p5Vp9ae63Cn0pjVfsSyXT6kpxjo2ulMZrdaWUq3Zlyb/xTH70kp/mvqiQa+6LCrnmvqiQa+6LxnKtrlTGa+6LCrnmvqg0z1pfKuM190WFXGttGT2Ga/WlkmutLaVca20p5VprS/MYdawvlfFa+6JSrrW+FOdZ6ktlrNbaUso11xZGjttbXankeo5zx3I9x7ljuZ7j3OrjkrGuVMbrOc4dy/Uc547Os9SVxng9x7ljuZ6+HPN4rbMvY7mevozlevoyluvpS/XxaKUvY+P19GUs19OX0XnW+lIYq6crY7nu45bFH0efWiRJkiRJkqQ15mS/1EuSJEmSJElz4okfSZIkSZKkNcoTP5IkSZIkSWuUJ34kSZIkSZLWKE/8SJIkSZIkrVGe+JEkSZIkSVqjPPEjSZIkSZK0RnniR5IkSZIkaY36/2eVykVqkaqFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = MultiDriller(env_config)\n",
    "env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = ppo.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#         print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "e1c9f74a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x171621666d0>"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad433c1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "001c4fc7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 101       |\n",
      "|    ep_rew_mean      | -1.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 1000      |\n",
      "|    fps              | 1338      |\n",
      "|    time_elapsed     | 75        |\n",
      "|    total_timesteps  | 101000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 32.9      |\n",
      "|    n_updates        | 12749     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 101       |\n",
      "|    ep_rew_mean      | -1.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 2000      |\n",
      "|    fps              | 937       |\n",
      "|    time_elapsed     | 215       |\n",
      "|    total_timesteps  | 202000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.000366  |\n",
      "|    n_updates        | 37999     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 101       |\n",
      "|    ep_rew_mean      | -1.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 3000      |\n",
      "|    fps              | 851       |\n",
      "|    time_elapsed     | 355       |\n",
      "|    total_timesteps  | 303000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.00147   |\n",
      "|    n_updates        | 63249     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 101       |\n",
      "|    ep_rew_mean      | -1.01e+04 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 4000      |\n",
      "|    fps              | 817       |\n",
      "|    time_elapsed     | 494       |\n",
      "|    total_timesteps  | 404000    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 90.4      |\n",
      "|    n_updates        | 88499     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x16e82e34640>"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "dqn = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "dqn.learn(total_timesteps=500_000, log_interval=1_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "4e9b03e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=70, num_wells = 3, delim=\",\")\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = dqn.predict(state, deterministic=True)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a182590",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31eaef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc33143",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "1b68430c-913a-422d-a19b-8a284d7bc5f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 620       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.36     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -358      |\n",
      "|    value_loss         | 1.05e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 623       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.32     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -366      |\n",
      "|    value_loss         | 1.04e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 624       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.36     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | -376      |\n",
      "|    value_loss         | 1.03e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 624       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -355      |\n",
      "|    value_loss         | 1.02e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 625       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -375      |\n",
      "|    value_loss         | 1.02e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 626       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -406      |\n",
      "|    value_loss         | 1.01e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 627       |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.36     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -382      |\n",
      "|    value_loss         | 9.97e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 626       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | -256      |\n",
      "|    value_loss         | 9.88e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 625       |\n",
      "|    iterations         | 9000      |\n",
      "|    time_elapsed       | 71        |\n",
      "|    total_timesteps    | 45000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.33     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8999      |\n",
      "|    policy_loss        | -324      |\n",
      "|    value_loss         | 9.79e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 624       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | 225       |\n",
      "|    value_loss         | 4.6e+04   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 623       |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 88        |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -383      |\n",
      "|    value_loss         | 9.62e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 622       |\n",
      "|    iterations         | 12000     |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 60000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11999     |\n",
      "|    policy_loss        | 307       |\n",
      "|    value_loss         | 8.19e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 621       |\n",
      "|    iterations         | 13000     |\n",
      "|    time_elapsed       | 104       |\n",
      "|    total_timesteps    | 65000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12999     |\n",
      "|    policy_loss        | -385      |\n",
      "|    value_loss         | 9.45e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 620       |\n",
      "|    iterations         | 14000     |\n",
      "|    time_elapsed       | 112       |\n",
      "|    total_timesteps    | 70000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.31     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13999     |\n",
      "|    policy_loss        | 188       |\n",
      "|    value_loss         | 1.23e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 617       |\n",
      "|    iterations         | 15000     |\n",
      "|    time_elapsed       | 121       |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | -417      |\n",
      "|    value_loss         | 9.28e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 614       |\n",
      "|    iterations         | 16000     |\n",
      "|    time_elapsed       | 130       |\n",
      "|    total_timesteps    | 80000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15999     |\n",
      "|    policy_loss        | 185       |\n",
      "|    value_loss         | 1.51e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 612       |\n",
      "|    iterations         | 17000     |\n",
      "|    time_elapsed       | 138       |\n",
      "|    total_timesteps    | 85000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16999     |\n",
      "|    policy_loss        | -239      |\n",
      "|    value_loss         | 9.12e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 610       |\n",
      "|    iterations         | 18000     |\n",
      "|    time_elapsed       | 147       |\n",
      "|    total_timesteps    | 90000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.27     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17999     |\n",
      "|    policy_loss        | -110      |\n",
      "|    value_loss         | 1.47e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 609       |\n",
      "|    iterations         | 19000     |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 95000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.3      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | -426      |\n",
      "|    value_loss         | 8.95e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 607       |\n",
      "|    iterations         | 20000     |\n",
      "|    time_elapsed       | 164       |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.886    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | -207      |\n",
      "|    value_loss         | 8.87e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 606       |\n",
      "|    iterations         | 21000     |\n",
      "|    time_elapsed       | 173       |\n",
      "|    total_timesteps    | 105000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.84     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 20999     |\n",
      "|    policy_loss        | -400      |\n",
      "|    value_loss         | 8.79e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 605       |\n",
      "|    iterations         | 22000     |\n",
      "|    time_elapsed       | 181       |\n",
      "|    total_timesteps    | 110000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.764    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 21999     |\n",
      "|    policy_loss        | -316      |\n",
      "|    value_loss         | 8.71e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 603       |\n",
      "|    iterations         | 23000     |\n",
      "|    time_elapsed       | 190       |\n",
      "|    total_timesteps    | 115000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.945    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 22999     |\n",
      "|    policy_loss        | -419      |\n",
      "|    value_loss         | 8.63e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 602       |\n",
      "|    iterations         | 24000     |\n",
      "|    time_elapsed       | 199       |\n",
      "|    total_timesteps    | 120000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.166    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 23999     |\n",
      "|    policy_loss        | -8.14     |\n",
      "|    value_loss         | 8.55e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 602       |\n",
      "|    iterations         | 25000     |\n",
      "|    time_elapsed       | 207       |\n",
      "|    total_timesteps    | 125000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.26     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 24999     |\n",
      "|    policy_loss        | -296      |\n",
      "|    value_loss         | 8.47e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 601       |\n",
      "|    iterations         | 26000     |\n",
      "|    time_elapsed       | 216       |\n",
      "|    total_timesteps    | 130000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.453    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 25999     |\n",
      "|    policy_loss        | -30.9     |\n",
      "|    value_loss         | 8.39e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 600       |\n",
      "|    iterations         | 27000     |\n",
      "|    time_elapsed       | 224       |\n",
      "|    total_timesteps    | 135000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.25     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 26999     |\n",
      "|    policy_loss        | -13.6     |\n",
      "|    value_loss         | 8.32e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 28000     |\n",
      "|    time_elapsed       | 233       |\n",
      "|    total_timesteps    | 140000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.9      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 27999     |\n",
      "|    policy_loss        | -202      |\n",
      "|    value_loss         | 8.24e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 29000     |\n",
      "|    time_elapsed       | 241       |\n",
      "|    total_timesteps    | 145000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 28999     |\n",
      "|    policy_loss        | -271      |\n",
      "|    value_loss         | 8.16e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 598       |\n",
      "|    iterations         | 30000     |\n",
      "|    time_elapsed       | 250       |\n",
      "|    total_timesteps    | 150000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.173    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 29999     |\n",
      "|    policy_loss        | -9.45     |\n",
      "|    value_loss         | 8.08e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 598       |\n",
      "|    iterations         | 31000     |\n",
      "|    time_elapsed       | 259       |\n",
      "|    total_timesteps    | 155000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.399    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 30999     |\n",
      "|    policy_loss        | -27.8     |\n",
      "|    value_loss         | 8.01e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 597       |\n",
      "|    iterations         | 32000     |\n",
      "|    time_elapsed       | 267       |\n",
      "|    total_timesteps    | 160000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.928    |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 31999     |\n",
      "|    policy_loss        | -455      |\n",
      "|    value_loss         | 7.93e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 597       |\n",
      "|    iterations         | 33000     |\n",
      "|    time_elapsed       | 276       |\n",
      "|    total_timesteps    | 165000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 32999     |\n",
      "|    policy_loss        | -503      |\n",
      "|    value_loss         | 7.86e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 596       |\n",
      "|    iterations         | 34000     |\n",
      "|    time_elapsed       | 284       |\n",
      "|    total_timesteps    | 170000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.779    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 33999     |\n",
      "|    policy_loss        | -182      |\n",
      "|    value_loss         | 7.78e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 596       |\n",
      "|    iterations         | 35000     |\n",
      "|    time_elapsed       | 293       |\n",
      "|    total_timesteps    | 175000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.809    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 34999     |\n",
      "|    policy_loss        | -77.1     |\n",
      "|    value_loss         | 7.71e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 596       |\n",
      "|    iterations         | 36000     |\n",
      "|    time_elapsed       | 302       |\n",
      "|    total_timesteps    | 180000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.624    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 35999     |\n",
      "|    policy_loss        | -55       |\n",
      "|    value_loss         | 7.63e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 595       |\n",
      "|    iterations         | 37000     |\n",
      "|    time_elapsed       | 310       |\n",
      "|    total_timesteps    | 185000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 36999     |\n",
      "|    policy_loss        | -280      |\n",
      "|    value_loss         | 7.56e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 595       |\n",
      "|    iterations         | 38000     |\n",
      "|    time_elapsed       | 319       |\n",
      "|    total_timesteps    | 190000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 37999     |\n",
      "|    policy_loss        | -281      |\n",
      "|    value_loss         | 7.48e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 594       |\n",
      "|    iterations         | 39000     |\n",
      "|    time_elapsed       | 327       |\n",
      "|    total_timesteps    | 195000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 38999     |\n",
      "|    policy_loss        | -347      |\n",
      "|    value_loss         | 7.41e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 594       |\n",
      "|    iterations         | 40000     |\n",
      "|    time_elapsed       | 336       |\n",
      "|    total_timesteps    | 200000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 39999     |\n",
      "|    policy_loss        | -450      |\n",
      "|    value_loss         | 7.34e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 594       |\n",
      "|    iterations         | 41000     |\n",
      "|    time_elapsed       | 345       |\n",
      "|    total_timesteps    | 205000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.376    |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 40999     |\n",
      "|    policy_loss        | -21.8     |\n",
      "|    value_loss         | 7.27e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 593       |\n",
      "|    iterations         | 42000     |\n",
      "|    time_elapsed       | 353       |\n",
      "|    total_timesteps    | 210000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.36     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 41999     |\n",
      "|    policy_loss        | -316      |\n",
      "|    value_loss         | 7.19e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 593       |\n",
      "|    iterations         | 43000     |\n",
      "|    time_elapsed       | 362       |\n",
      "|    total_timesteps    | 215000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.31     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 42999     |\n",
      "|    policy_loss        | -311      |\n",
      "|    value_loss         | 7.12e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 592       |\n",
      "|    iterations         | 44000     |\n",
      "|    time_elapsed       | 371       |\n",
      "|    total_timesteps    | 220000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.04     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 43999     |\n",
      "|    policy_loss        | -340      |\n",
      "|    value_loss         | 7.05e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 591       |\n",
      "|    iterations         | 45000     |\n",
      "|    time_elapsed       | 380       |\n",
      "|    total_timesteps    | 225000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.23     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 44999     |\n",
      "|    policy_loss        | -399      |\n",
      "|    value_loss         | 6.98e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 591       |\n",
      "|    iterations         | 46000     |\n",
      "|    time_elapsed       | 388       |\n",
      "|    total_timesteps    | 230000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 45999     |\n",
      "|    policy_loss        | -350      |\n",
      "|    value_loss         | 6.91e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 590       |\n",
      "|    iterations         | 47000     |\n",
      "|    time_elapsed       | 397       |\n",
      "|    total_timesteps    | 235000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 46999     |\n",
      "|    policy_loss        | -214      |\n",
      "|    value_loss         | 6.84e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 591       |\n",
      "|    iterations         | 48000     |\n",
      "|    time_elapsed       | 405       |\n",
      "|    total_timesteps    | 240000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 47999     |\n",
      "|    policy_loss        | -345      |\n",
      "|    value_loss         | 6.77e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 591       |\n",
      "|    iterations         | 49000     |\n",
      "|    time_elapsed       | 414       |\n",
      "|    total_timesteps    | 245000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 48999     |\n",
      "|    policy_loss        | -336      |\n",
      "|    value_loss         | 6.7e+04   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 591       |\n",
      "|    iterations         | 50000     |\n",
      "|    time_elapsed       | 422       |\n",
      "|    total_timesteps    | 250000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.27     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 49999     |\n",
      "|    policy_loss        | -222      |\n",
      "|    value_loss         | 6.63e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 592       |\n",
      "|    iterations         | 51000     |\n",
      "|    time_elapsed       | 430       |\n",
      "|    total_timesteps    | 255000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 50999     |\n",
      "|    policy_loss        | -313      |\n",
      "|    value_loss         | 6.56e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 592       |\n",
      "|    iterations         | 52000     |\n",
      "|    time_elapsed       | 438       |\n",
      "|    total_timesteps    | 260000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 51999     |\n",
      "|    policy_loss        | -284      |\n",
      "|    value_loss         | 6.49e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 592       |\n",
      "|    iterations         | 53000     |\n",
      "|    time_elapsed       | 447       |\n",
      "|    total_timesteps    | 265000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.26     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 52999     |\n",
      "|    policy_loss        | -293      |\n",
      "|    value_loss         | 6.42e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 593       |\n",
      "|    iterations         | 54000     |\n",
      "|    time_elapsed       | 455       |\n",
      "|    total_timesteps    | 270000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 53999     |\n",
      "|    policy_loss        | -345      |\n",
      "|    value_loss         | 6.36e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 593       |\n",
      "|    iterations         | 55000     |\n",
      "|    time_elapsed       | 463       |\n",
      "|    total_timesteps    | 275000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.02     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 54999     |\n",
      "|    policy_loss        | -127      |\n",
      "|    value_loss         | 6.29e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 593       |\n",
      "|    iterations         | 56000     |\n",
      "|    time_elapsed       | 471       |\n",
      "|    total_timesteps    | 280000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.26     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 55999     |\n",
      "|    policy_loss        | -264      |\n",
      "|    value_loss         | 6.22e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 593       |\n",
      "|    iterations         | 57000     |\n",
      "|    time_elapsed       | 480       |\n",
      "|    total_timesteps    | 285000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 56999     |\n",
      "|    policy_loss        | -240      |\n",
      "|    value_loss         | 6.16e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 593       |\n",
      "|    iterations         | 58000     |\n",
      "|    time_elapsed       | 488       |\n",
      "|    total_timesteps    | 290000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.25     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 57999     |\n",
      "|    policy_loss        | -384      |\n",
      "|    value_loss         | 6.09e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 594       |\n",
      "|    iterations         | 59000     |\n",
      "|    time_elapsed       | 496       |\n",
      "|    total_timesteps    | 295000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 58999     |\n",
      "|    policy_loss        | -129      |\n",
      "|    value_loss         | 6.02e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 594       |\n",
      "|    iterations         | 60000     |\n",
      "|    time_elapsed       | 504       |\n",
      "|    total_timesteps    | 300000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 59999     |\n",
      "|    policy_loss        | -234      |\n",
      "|    value_loss         | 5.96e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 594       |\n",
      "|    iterations         | 61000     |\n",
      "|    time_elapsed       | 512       |\n",
      "|    total_timesteps    | 305000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.666    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 60999     |\n",
      "|    policy_loss        | -487      |\n",
      "|    value_loss         | 5.89e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 594       |\n",
      "|    iterations         | 62000     |\n",
      "|    time_elapsed       | 521       |\n",
      "|    total_timesteps    | 310000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.5      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 61999     |\n",
      "|    policy_loss        | -89.9     |\n",
      "|    value_loss         | 5.83e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 595       |\n",
      "|    iterations         | 63000     |\n",
      "|    time_elapsed       | 529       |\n",
      "|    total_timesteps    | 315000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.635    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 62999     |\n",
      "|    policy_loss        | -198      |\n",
      "|    value_loss         | 5.76e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 595       |\n",
      "|    iterations         | 64000     |\n",
      "|    time_elapsed       | 537       |\n",
      "|    total_timesteps    | 320000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.807    |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 63999     |\n",
      "|    policy_loss        | -148      |\n",
      "|    value_loss         | 5.7e+04   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 595       |\n",
      "|    iterations         | 65000     |\n",
      "|    time_elapsed       | 545       |\n",
      "|    total_timesteps    | 325000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.794    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 64999     |\n",
      "|    policy_loss        | -92.1     |\n",
      "|    value_loss         | 5.64e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 595       |\n",
      "|    iterations         | 66000     |\n",
      "|    time_elapsed       | 553       |\n",
      "|    total_timesteps    | 330000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.648    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 65999     |\n",
      "|    policy_loss        | -168      |\n",
      "|    value_loss         | 5.57e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 596       |\n",
      "|    iterations         | 67000     |\n",
      "|    time_elapsed       | 561       |\n",
      "|    total_timesteps    | 335000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.697    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 66999     |\n",
      "|    policy_loss        | -140      |\n",
      "|    value_loss         | 5.51e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 596       |\n",
      "|    iterations         | 68000     |\n",
      "|    time_elapsed       | 570       |\n",
      "|    total_timesteps    | 340000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.542    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 67999     |\n",
      "|    policy_loss        | -35.1     |\n",
      "|    value_loss         | 5.45e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 596       |\n",
      "|    iterations         | 69000     |\n",
      "|    time_elapsed       | 578       |\n",
      "|    total_timesteps    | 345000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.565    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 68999     |\n",
      "|    policy_loss        | -264      |\n",
      "|    value_loss         | 5.38e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 596       |\n",
      "|    iterations         | 70000     |\n",
      "|    time_elapsed       | 586       |\n",
      "|    total_timesteps    | 350000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.215    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 69999     |\n",
      "|    policy_loss        | -9.28     |\n",
      "|    value_loss         | 5.32e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 597       |\n",
      "|    iterations         | 71000     |\n",
      "|    time_elapsed       | 594       |\n",
      "|    total_timesteps    | 355000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.761    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 70999     |\n",
      "|    policy_loss        | -144      |\n",
      "|    value_loss         | 5.26e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 597       |\n",
      "|    iterations         | 72000     |\n",
      "|    time_elapsed       | 602       |\n",
      "|    total_timesteps    | 360000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.453    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 71999     |\n",
      "|    policy_loss        | -83.5     |\n",
      "|    value_loss         | 5.2e+04   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 597       |\n",
      "|    iterations         | 73000     |\n",
      "|    time_elapsed       | 610       |\n",
      "|    total_timesteps    | 365000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.679    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 72999     |\n",
      "|    policy_loss        | -93.8     |\n",
      "|    value_loss         | 5.14e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 597       |\n",
      "|    iterations         | 74000     |\n",
      "|    time_elapsed       | 618       |\n",
      "|    total_timesteps    | 370000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 73999     |\n",
      "|    policy_loss        | -206      |\n",
      "|    value_loss         | 5.08e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 597       |\n",
      "|    iterations         | 75000     |\n",
      "|    time_elapsed       | 627       |\n",
      "|    total_timesteps    | 375000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 74999     |\n",
      "|    policy_loss        | -284      |\n",
      "|    value_loss         | 5.02e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 598       |\n",
      "|    iterations         | 76000     |\n",
      "|    time_elapsed       | 635       |\n",
      "|    total_timesteps    | 380000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.37     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 75999     |\n",
      "|    policy_loss        | -283      |\n",
      "|    value_loss         | 4.96e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 598       |\n",
      "|    iterations         | 77000     |\n",
      "|    time_elapsed       | 643       |\n",
      "|    total_timesteps    | 385000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 76999     |\n",
      "|    policy_loss        | -276      |\n",
      "|    value_loss         | 4.9e+04   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 598       |\n",
      "|    iterations         | 78000     |\n",
      "|    time_elapsed       | 651       |\n",
      "|    total_timesteps    | 390000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.33     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 77999     |\n",
      "|    policy_loss        | -233      |\n",
      "|    value_loss         | 4.84e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 598       |\n",
      "|    iterations         | 79000     |\n",
      "|    time_elapsed       | 659       |\n",
      "|    total_timesteps    | 395000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 78999     |\n",
      "|    policy_loss        | -247      |\n",
      "|    value_loss         | 4.78e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 598       |\n",
      "|    iterations         | 80000     |\n",
      "|    time_elapsed       | 668       |\n",
      "|    total_timesteps    | 400000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.11     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 79999     |\n",
      "|    policy_loss        | -381      |\n",
      "|    value_loss         | 4.72e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 598       |\n",
      "|    iterations         | 81000     |\n",
      "|    time_elapsed       | 676       |\n",
      "|    total_timesteps    | 405000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 80999     |\n",
      "|    policy_loss        | -192      |\n",
      "|    value_loss         | 4.66e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 82000     |\n",
      "|    time_elapsed       | 684       |\n",
      "|    total_timesteps    | 410000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.863    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 81999     |\n",
      "|    policy_loss        | -150      |\n",
      "|    value_loss         | 4.61e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 83000     |\n",
      "|    time_elapsed       | 692       |\n",
      "|    total_timesteps    | 415000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.771    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 82999     |\n",
      "|    policy_loss        | -193      |\n",
      "|    value_loss         | 4.55e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 84000     |\n",
      "|    time_elapsed       | 700       |\n",
      "|    total_timesteps    | 420000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.746    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 83999     |\n",
      "|    policy_loss        | -157      |\n",
      "|    value_loss         | 4.49e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 85000     |\n",
      "|    time_elapsed       | 708       |\n",
      "|    total_timesteps    | 425000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.5      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 84999     |\n",
      "|    policy_loss        | -220      |\n",
      "|    value_loss         | 4.43e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 86000     |\n",
      "|    time_elapsed       | 717       |\n",
      "|    total_timesteps    | 430000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.812    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 85999     |\n",
      "|    policy_loss        | -169      |\n",
      "|    value_loss         | 4.38e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 87000     |\n",
      "|    time_elapsed       | 725       |\n",
      "|    total_timesteps    | 435000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.541    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 86999     |\n",
      "|    policy_loss        | -123      |\n",
      "|    value_loss         | 4.32e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 88000     |\n",
      "|    time_elapsed       | 733       |\n",
      "|    total_timesteps    | 440000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.984    |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 87999     |\n",
      "|    policy_loss        | -185      |\n",
      "|    value_loss         | 4.27e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 89000     |\n",
      "|    time_elapsed       | 741       |\n",
      "|    total_timesteps    | 445000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.59     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 88999     |\n",
      "|    policy_loss        | -83.3     |\n",
      "|    value_loss         | 4.21e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 599       |\n",
      "|    iterations         | 90000     |\n",
      "|    time_elapsed       | 750       |\n",
      "|    total_timesteps    | 450000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.01     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 89999     |\n",
      "|    policy_loss        | -124      |\n",
      "|    value_loss         | 4.16e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 600       |\n",
      "|    iterations         | 91000     |\n",
      "|    time_elapsed       | 758       |\n",
      "|    total_timesteps    | 455000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.309    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 90999     |\n",
      "|    policy_loss        | -12.7     |\n",
      "|    value_loss         | 4.1e+04   |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 600       |\n",
      "|    iterations         | 92000     |\n",
      "|    time_elapsed       | 766       |\n",
      "|    total_timesteps    | 460000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.552    |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 91999     |\n",
      "|    policy_loss        | -27.1     |\n",
      "|    value_loss         | 4.05e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 600       |\n",
      "|    iterations         | 93000     |\n",
      "|    time_elapsed       | 774       |\n",
      "|    total_timesteps    | 465000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.485    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 92999     |\n",
      "|    policy_loss        | -64.3     |\n",
      "|    value_loss         | 3.99e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 600       |\n",
      "|    iterations         | 94000     |\n",
      "|    time_elapsed       | 782       |\n",
      "|    total_timesteps    | 470000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.619    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 93999     |\n",
      "|    policy_loss        | -186      |\n",
      "|    value_loss         | 3.94e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 600       |\n",
      "|    iterations         | 95000     |\n",
      "|    time_elapsed       | 790       |\n",
      "|    total_timesteps    | 475000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.31     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 94999     |\n",
      "|    policy_loss        | -235      |\n",
      "|    value_loss         | 3.89e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 600       |\n",
      "|    iterations         | 96000     |\n",
      "|    time_elapsed       | 799       |\n",
      "|    total_timesteps    | 480000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.896    |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 95999     |\n",
      "|    policy_loss        | -130      |\n",
      "|    value_loss         | 3.84e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 600       |\n",
      "|    iterations         | 97000     |\n",
      "|    time_elapsed       | 807       |\n",
      "|    total_timesteps    | 485000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 96999     |\n",
      "|    policy_loss        | -257      |\n",
      "|    value_loss         | 3.78e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 600       |\n",
      "|    iterations         | 98000     |\n",
      "|    time_elapsed       | 815       |\n",
      "|    total_timesteps    | 490000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 97999     |\n",
      "|    policy_loss        | -233      |\n",
      "|    value_loss         | 3.73e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 601       |\n",
      "|    iterations         | 99000     |\n",
      "|    time_elapsed       | 823       |\n",
      "|    total_timesteps    | 495000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.685    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 98999     |\n",
      "|    policy_loss        | -63.1     |\n",
      "|    value_loss         | 3.68e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 101       |\n",
      "|    ep_rew_mean        | -1.01e+04 |\n",
      "| time/                 |           |\n",
      "|    fps                | 601       |\n",
      "|    iterations         | 100000    |\n",
      "|    time_elapsed       | 831       |\n",
      "|    total_timesteps    | 500000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.565    |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99999     |\n",
      "|    policy_loss        | -27.2     |\n",
      "|    value_loss         | 3.63e+04  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x16e807cdfa0>"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "# More the number of wells, more time to train \n",
    "# env_config = dict(model_path=r\"data/2d_stacked.csv\", available_pipe=100, num_wells = 3, delim=\",\")\n",
    "\n",
    "# env = RewardDriller(env_config)\n",
    "\n",
    "a2c = A2C(\"MlpPolicy\", env, verbose=3)\n",
    "a2c.learn(total_timesteps=500_000, log_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "263efa6f",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = RewardDriller(env_config)\n",
    "\n",
    "episodes = 100\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(env.model, vmin=-10, vmax=2)\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "#     print(\"Beginning Drill Campaign:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "#     reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = a2c.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "#     print(f\"    Reward: {reward}\")\n",
    "\n",
    "    for i in range(0,len(env.multi_trajectory)):\n",
    "        traj_z, traj_x = np.asarray(env.multi_trajectory[i]).T\n",
    "        plt.plot(traj_x, traj_z, \"-\", linewidth=6)\n",
    "\n",
    "    plt.xticks(np.arange(0, 80, 1.0))\n",
    "    plt.yticks(np.arange(0, 40, 1.0))\n",
    "    plt.xlim([-0.5, 79.5])\n",
    "    plt.ylim([39.5, -0.5])\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4d162",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
